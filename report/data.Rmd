
```{r setup, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

# 1. Data Gathering

Here are the book's characteristics:

| Title | Author | Date | Parts | Chapters | Pages |
|:---|:---|:---|:---|:---|:---|
| Invisible Women | Caroline Perez Criado | 2019 | 6 | 16 | 399 |

The book *Invisible Women* under study is directly downloaded from an online source https://yes-pdf.com/book/113#google_vignette in its PDF version. To upload it in *Rstudio*, we use the `pdf_text` utility from the **pdftools** package that extracts texts from PDF files. The advantage of uploading the book from a website link is that it is easy to obtain but on the other side, it requires a tedious preparation in order to be able to get started with the cleaning. Here, we first need to manipulate the PDF version to keep only the chapters of the book and remove other useless parts for our analysis such as blank pages, the title page etc.. 

```{r load data, echo = FALSE, message = FALSE}
#load the data
text <- pdf_text("https://yes-pdf.com/electronic-book/33")
```

# 2. Data Structuring and Cleaning

## Corpus

First of all, we need to gather the parts of the book useful to our analysis (i.e. chapters) in a *corpus* object. Since the book contains extra information such as a title page, Preface, Content page and others, we first need to proceed to some cleaning of the PDF version in order to remain only with the text of each chapter associated with their titles. Therefore, after indicating exactly where the text under study starts and ends and, after extracting the chapter titles and organizing the text by chapters, we obtain the usable data in a corpus to further analyze *Invisible Women*'s content. 

The below output shows the beginning of the first five chapters of the book. Each chapter is referred as a **document** and their content as a **text**. We also notice that chapters are organized in six parts.

```{r start and stop, message=FALSE, warning=FALSE, include=FALSE}

# The text starts after the title page of part I
start <- grep("CHAPTER 1", text)
start < -start[1]

# The text ends just before the "Afterword" section
stop <- grep("Afterword", text) - 1
stop<-stop[2]

# extract lines from the book
lines <- text[start:stop]

```

```{r chapters, echo=FALSE, message=FALSE, warning=FALSE}

# chapters start with "CHAPTER 1", "CHAPTER 2", etc...
chapter <- grep("CHAPTER", lines)

# Gets the section texts (including the front matter)
start <- c(1, chapter) # + 1 to skip title
end <- c(chapter - 1, length(lines))
text <- mapply(function(s, e) paste(lines[s:e], collapse = "\n"), 
               start, 
               end)

# Trims leading and trailing white space
text <- trimws(text)

# Discards the front matter
text <- text[-1]

# Gets the section titles
document <- sub("^[[:space:]]*[[:digit:]]+[.][[:space:]]*", "", lines[chapter])
document <- trimws(document)

```

```{r corpus, echo = FALSE, message = FALSE}
# Creates a corpus object
data <- corpus_frame(document, text) 
```

```{r title wrangling, echo = FALSE, message = FALSE}

# Cleans title to keep only CHAPTER x, and clean text to remove Chapter X

for (i in 1:9){
  data[i,1] <- substr(data[i,1],
                      1,
                      9)
  
  data$text[[i]] <- substring(data$text[[i]],
                              10)
}

for (i in 10:16){
  data[i,1] <- substr(data[i,1],
                      1,
                      10)
  
  data$text[[i]]<-substring(data$text[[i]],
                            11)
}
# add the parts of the book
data <-  data %>% mutate("part" = c(1,1,2,2,2,2,3,3,3,4,4,5,5,5,6,6))

# Checks type of "data"
#typeof(data) # list

kable(data[1:5,],
      caption = "Head of Invisible Women's corpus") %>% 
      kable_styling(bootstrap_options = c("striped",
                                          "hover", 
                                          "condensed", 
                                          "responsive"),
                    full_width = T, 
                    position = "left", 
                    font_size = 16, 
                    fixed_thead = T)

```

## Tokenization

Tokenization is the method used to split a text into tokens. Here, we tokenize the chapters (i.e. document) by space. To do so, we proceed to remove numbers, punctuation, symbols and separators because we believe that it will not affect our analysis. Note that our unit of analysis is word.

### Quanteda

The **Quanteda** package uses a corpus object.

The below summary shows that *Invisible Women* consists of 16 documents (i.e. chapters) and for each document, three columns indicate the number of tokens, the number of sentences as well as the number of token types per document. 

```{r corpus summary, echo = FALSE, message = FALSE}
# Loads into a corpus
data.cp <- corpus(text)
summary(data.cp)

```

```{r tokenization, echo = FALSE, message = FALSE}

# Tokenize = create a tokens object
data.tk1 <- data.cp %>% 
                   tokens(remove_numbers = TRUE,
                          remove_punct = TRUE,
                          remove_symbols = TRUE,
                          remove_separators = TRUE) %>%
                   tokens_split(separator = "'") %>%
                   tokens_split(separator = "-") 
```

## Stop Words

To continue the cleaning process, we remove useless words that bring very to no information using the *stop_words* dictionary from the **quanteda** package and we map letters to lower cases since names (such as first or last names) are not of a specific importance in this book. 

The advantage of removing stop words is that it reduces the dimension of the number of features/terms to analyze so that the focus of the analysis is on terms that bring relevant information. In this aim, we remove the word "chapter" which does not provide any value. 


```{r cleaning, echo = FALSE, message = FALSE}

# Removes stop words and converts into lower cases 
data.tk2 <- data.tk1 %>%
            tokens_remove(stop_words$word) %>% 
            tokens_remove("chapter") %>%
            tokens_tolower()

```

## Lemmatization
                                                    
Lemmatization simplifies tokens by generating tokens from a dictionary and reduces the vocabulary to its simplest and meaningful essence. Consequently, the set of token types in a corpus is reduced. For example, "started" and "starts" are reduced to "start" and have thus "start" as a lemma. 

The below output displays for each chapter the lemmas of the first tokens as well as the total number of different lemmas by chapter. For example, chapter one contains 2,413 different lemmas. 

```{r lemmatization, echo = FALSE, message = FALSE}

# Lemmatization
data.tk2 <- data.tk2  %>%  tokens_replace(pattern = hash_lemmas$token, 
                                          replacement = hash_lemmas$lemma)
head(data.tk2,3)
```

## Stemming 

Stemming also simplifies tokens by reducing a word to its stem with simple rule-based algorithm usig the *token_wordstem()* function. As lemmatization, stemming reduces the size of a vocabulary but in an inconsistent way. Indeed, reducing a word to its stem does not guarantee meaningful tokens (e.g official is reduced to offici). This is why, since the interpretation of the tokens matter, we decide not to use the stemming in the rest of our analysis and only apply it here to demonstrate its purpose.

The below output displays the first twelve tokens reduced to their steam for each document. For example, "snow-clearing" was reduced to "snow-clear". 

```{r stemming, echo = FALSE, message = FALSE}
# Stemming
data_stem <- data.tk2 %>% tokens_wordstem()
head(data_stem, 3)
```

## Document-Term Matrix (DTM)

Now, without considering the stemming, we compute the **Document-Term-Matrix** that will be useful throughout the analysis.

The below snapshot of the matrix indicates that after cleaning and lemmatizing, there are 5,742 features to be analyzed and that the DTM is sparse at 83.76% (i.e. contains mostly zeros). The matrix displays the frequency of features (i.e. terms or words here) by documents (i.e. texts or chapters here). For example, the first row indicates that the word *sexist* is found twice in chapter 1 and the first column indicates that the same word is found in chapter 1, chapter 4 and chapter 6. 

```{r DTM, echo = FALSE, message = FALSE}
# Creates a document-feature matrix
data.dfm <- dfm(data.tk2)  
data.dfm
```












