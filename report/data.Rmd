```{r setup, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

# 1. Data Gathering

The book *Invisible Women* under study is directly downloaded from [source: https://yes-pdf.com/book/113#google_vignette] in its PDF version. To upload it in *Rstudio*, we use the `pdf_text` utility from the **pdftools** package that extracts texts from pdf files.
One of its advantages, is that is it easy to upload. On the cons side, the cleaning can sometimes be quite long and tedious.

```{r load data, echo = FALSE, message = FALSE}
#load the data
text <- pdf_text("https://yes-pdf.com/electronic-book/33")
```

Here are the book's characteristics:

| Title | Author | Date | Parts | Chapters | Pages |
|:---|:---|:---|:---|:---|:---|
| Invisible Women | Caroline Perez Criado | 2019 | 6 | 16 | 399 |

# 2. Data Structuring and Cleaning

## Corpus

*The preface* of the book starts on page 11 and *the introduction* on page 15.  

```{r start and stop, message=FALSE, warning=FALSE, include=FALSE}

# The text starts after the title page of part I
start <- grep("CHAPTER 1", text)
start < -start[1]

# The text ends just before the "Afterword" section
stop <- grep("Afterword", text) - 1
stop<-stop[2]

# extract lines from the book
lines <- text[start:stop]

```

```{r chapters, echo=FALSE, message=FALSE, warning=FALSE}

# chapters start with "CHAPTER 1", "CHAPTER 2", etc...
chapter <- grep("CHAPTER", lines)

# Gets the section texts (including the front matter)
start <- c(1, chapter) # + 1 to skip title
end <- c(chapter - 1, length(lines))
text <- mapply(function(s, e) paste(lines[s:e], collapse = "\n"), 
               start, 
               end)

# Trims leading and trailing white space
text <- trimws(text)

# Discards the front matter
text <- text[-1]

# Gets the section titles
document <- sub("^[[:space:]]*[[:digit:]]+[.][[:space:]]*", "", lines[chapter])
document <- trimws(document)

```

After indicating where exactly the text under study starts and ends, and after extracting the chapter titles and organizing the text by chapters, we obtain the usable data to further analyze *Invisible Women*'s content. 

```{r corpus, echo = FALSE, message = FALSE}
# Creates a corpus object
data <- corpus_frame(document, text) 
```

The below output shows the beginning of the first five chapters of the book.

```{r title wrangling, echo = FALSE, message = FALSE}

# Cleans title to keep only CHAPTER x, and clean text to remove Chapter X

for (i in 1:9){
  data[i,1] <- substr(data[i,1],
                      1,
                      9)
  
  data$text[[i]] <- substring(data$text[[i]],
                              10)
}

for (i in 10:16){
  data[i,1] <- substr(data[i,1],
                      1,
                      10)
  
  data$text[[i]]<-substring(data$text[[i]],
                            11)
}
# add the parts of the book
data <-  data %>% mutate("part" = c(1,1,2,2,2,2,3,3,3,4,4,5,5,5,6,6))

# Checks type of "data"
#typeof(data) # list

kable(data[1:5,],
      caption = "Head of Invisible Women's corpus") %>% 
      kable_styling(bootstrap_options = c("striped",
                                          "hover", 
                                          "condensed", 
                                          "responsive"),
                    full_width = T, 
                    position = "left", 
                    font_size = 16, 
                    fixed_thead = T)

```

```{r}
#supervised learning data cleaning


# count the total number of pages additioning all the pages
totalpages <- 0

for ( i in 1 :nrow(data)){
  # split the paragraph by pages
  paragraphdata <- strsplit(data$text[[i]],"\n\n")
  # count the number of pages for the chapter
  NbPages <- length(paragraphdata[[1]])
  # add the number of pages of the chapter to the total number of pages 
  totalpages <- totalpages + NbPages
  
  
}



# remove change the nb of pages because the chapter number and the name of the chapter are counted as a two pages
totalpages_nochap <- totalpages - (16*2)

# create emptydata frame with all the pages
supervised_data <- data.frame(matrix(NA, ncol = 4, nrow = totalpages_nochap))
colnames(supervised_data)<-c("document","part","page_nb","pagetext")

rowindex <- 1

for (i in 1:nrow(data)){
  paragraphdata <- strsplit(data$text[[i]],"\n\n") # split the text to pages
  paragraphdata <- paragraphdata[[1]][-c(1,2)] # remove first two elements ( chapter and chapter name )
  
  NbPages <- length(paragraphdata)# compute the number of pages
  
  for (j in 1:NbPages){
    supervised_data$document[[rowindex]] <- data$document[[i]]
    supervised_data$part[[rowindex]] <- data$part[[i]]
    supervised_data$page_nb[[rowindex]] <- paste(i,j,sep=".")
    supervised_data$pagetext[[rowindex]] <- paragraphdata[j]
    rowindex <- rowindex+1
  }
}

# Cleaning 

# transform data frame into copus object
supervised_cp <- corpus(supervised_data,text_field = "pagetext")

# data cleaning ans lemmatization
supervised_tk <- supervised_cp %>% 
                   tokens(remove_numbers = TRUE,
                          remove_punct = TRUE,
                          remove_symbols = TRUE,
                          remove_separators = TRUE) %>%
                   tokens_split(separator = "'") %>%
                   tokens_split(separator = "-") %>%
                   tokens_remove(stop_words$word) %>% 
                   tokens_tolower() %>%  
                   tokens_replace(pattern = hash_lemmas$token, 
                                          replacement = hash_lemmas$lemma) %>%
                   tokens_remove(c("chapter","woman")) 


```


## Tokenization

Tokenization is the method used to split a text into tokens. Our unit of analysis are words. Here, we tokenize the chapters (i.e. document) by space. To do so, we proceed to remove numbers, punctuation, symbols and separators because we believe that it will not affect our analysis and keeping them [KEEPING WHAT?] will not bring more insight.

### Quanteda

The **Quanteda** package uses a corpus object.

The below summary shows that *Invisible Women* consists of 16 documents (i.e. chapters) and for each document, three columns indicate the number of tokens, the number of sentences as well as the number of token types per document. 

```{r corpus summary, echo = FALSE, message = FALSE}
# Loads into a corpus
data.cp <- corpus(text)
summary(data.cp)

```

```{r tokenization, echo = FALSE, message = FALSE}

# Tokenize = create a tokens object
data.tk1 <- data.cp %>% 
                   tokens(remove_numbers = TRUE,
                          remove_punct = TRUE,
                          remove_symbols = TRUE,
                          remove_separators = TRUE) %>%
                   tokens_split(separator = "'") %>%
                   tokens_split(separator = "-") 
```

## Stop Words

To continue the cleaning process, we remove useless words that bring very to no information using the *stop_words* dictionary from the **quanteda** package and we map letters to lower cases since names (such as first or last names) are not of a specific importance in this book. 

The advantage of removing stop words is that it reduces the dimension of the number of features/terms to analyze so that the focus of the analysis is on terms that bring relevant information. In this aim, we remove the word "chapter" which does not provide any value. 

### Quanteda

```{r cleaning, echo = FALSE, message = FALSE}

# Removes stop words and converts into lower cases 
data.tk2 <- data.tk1 %>%
            tokens_remove(stop_words$word) %>% 
            tokens_remove("chapter") %>%
            tokens_tolower()

```

## Lemmatization
                                                    
Lemmatization simplifies tokens by generating tokens from a dictionary and reduces the vocabulary to its simplest and meaningful essence. Consequently, the set of types in a corpus is shortened [THE NUMBER OF TOKEN TYPES IS REDUCED?]. For example, "started" and "starts" are reduced to "start" and have thus "start" as a lemma. 

The below output displays for each chapter the lemmas of the first tokens as well as the total number of different lemmas by chapter. For example, chapter (i.e. text) one contains 2,413 different lemmas. 

```{r lemmatization, echo = FALSE, message = FALSE}

# Lemmatization
data.tk2 <- data.tk2  %>%  tokens_replace(pattern = hash_lemmas$token, 
                                          replacement = hash_lemmas$lemma)
head(data.tk2)
```

## Stemming 

Stemming also simplifies tokens by reducing a word to its stem with simple rule based algorithm usig the *token_wordstem()* function. As lemmatization, stemming reduces the size of a vocabulary but in an inconsistent way. [EXPLAIN WHY IN AN INCONSISTENT WAY]

Since the interpretation of the tokens matter, we decide not to use the stemming in the rest of our analysis and only apply it here to demonstrate its purpose, since reducing a word to its stem does not guarantee meaningful tokens (e.g official is reduced to offici).

The below output displays the first twelve tokens reduced to their steam for each document. For example, "snow-clearing" was reduced to "snow-clear". 

```{r stemming, echo = FALSE, message = FALSE}
# Stemming
data_stem <- data.tk2 %>% tokens_wordstem()
head(data_stem)
```

## Document-Term Matrix (DTM)

Now, without considering the stemming, we compute the **Document-Term-Matrix** that will be useful throughout the analysis.

The below snapshot of the matrix indicates that after cleaning and lemmatizing, there are 5,742 features to be analyzed and that the DTM is sparse at 83.76% (i.e. contains mostly zeros). The matrix displays the frequency of features (i.e. terms or words here) by documents (i.e. texts or chapters here). For example, the first row indicates that the word *sexist* is found twice and the word *town* is found six times and, the first column indicates that the word *sexist* is found in chapter one, chapter four and chapter 6. 

```{r DTM, echo = FALSE, message = FALSE}
# Creates a document-feature matrix
data.dfm <- dfm(data.tk2)  
data.dfm
```












