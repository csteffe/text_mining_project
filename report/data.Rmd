```{r setup, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

# 1. Data Gathering
The book *Invisible Women* under study was downloaded from [ADD WEBSITE SOURCE] in its PDF version. To upload it in *Rstudio*, we use the `pdf_text` utility from the **pdftools** package that extracts texts from pdf files.
One of its advantage, is that is it easy to upload. On the cons side, the cleaning can sometimes be quite difficult.

```{r load data, echo = FALSE, message = FALSE}
#load the data
text <- pdf_text(here::here("data/Invisible Women.pdf"))
```
Here are the book's characteristics:

| Title | Author | Date | Parts | Chapters | Pages |
|:---|:---|:---|:---|:---|:---|
| Invisible Women | Caroline Perez Criado | 2019 | 6 | 16 | 399 |


# 2. Data Structuring and Cleaning

## Corpus
*The preface* of the book starts on page 11 and *the introduction* on page 15.  

```{r start and stop, message=FALSE, warning=FALSE, include=FALSE}

# The text starts after the title page of part I
start <- grep("CHAPTER 1", text)
start < -start[1]

# The text ends just before the "Afterword" section
stop <- grep("Afterword", text) - 1
stop<-stop[2]

# extract lines from the book
lines <- text[start:stop]

```

```{r chapters, echo=FALSE, message=FALSE, warning=FALSE}

# chapters start with "CHAPTER 1", "CHAPTER 2", etc...
chapter <- grep("CHAPTER", lines)

# Gets the section texts (including the front matter)
start <- c(1, chapter) # + 1 to skip title
end <- c(chapter - 1, length(lines))
text <- mapply(function(s, e) paste(lines[s:e], collapse = "\n"), 
               start, 
               end)

# Trims leading and trailing white space
text <- trimws(text)

# Discards the front matter
text <- text[-1]

# Gets the section titles
document <- sub("^[[:space:]]*[[:digit:]]+[.][[:space:]]*", "", lines[chapter])
document <- trimws(document)

```

After indicating where exactly the text under study starts and ends and, after extracting the chapter titles and organizing the text by chapters, we obtain the usable data to further analyze *Invisible Women*'s text. 

```{r corpus, echo = FALSE, message = FALSE}
# Creates a corpus object
data <- corpus_frame(document, text) 
```

```{r title wrangling, echo = FALSE, message = FALSE}

# Cleans title to keep only CHAPTER x, and clean text to remove Chapter X

for (i in 1:9){
  data[i,1] <- substr(data[i,1],
                      1,
                      9)
  
  data$text[[i]] <- substring(data$text[[i]],
                              10)
}

for (i in 10:16){
  data[i,1] <- substr(data[i,1],
                      1,
                      10)
  
  data$text[[i]]<-substring(data$text[[i]],
                            11)
}


# Checks type of "data"
#typeof(data) # list

kable(data[1:5,],
      caption = "Head of Invisible Women's corpus") %>% 
      kable_styling(bootstrap_options = c("striped",
                                          "hover", 
                                          "condensed", 
                                          "responsive"),
                    full_width = T, 
                    position = "left", 
                    font_size = 16, 
                    fixed_thead = T)

```

## Tokenization

Tokenization is the method used to split a text into tokens. Our unit of analysis are words. Here, we tokenize the chapters (i.e. document) by space. To do so, we proceed to remove numbers, punctuation, symbols and separators because we believe that it will not affect our analysis and keeping them will not bring more insight.

### Quanteda

The **Quanteda** package uses a corpus object.

The below summary shows that *Invisible Women* consists of 16 documents. And for each document, three columns indicates the number of tokens, the number of sentences as well as the number of token types per document. 

```{r corpus summary, echo = FALSE, message = FALSE}
# Loads into a corpus
data.cp <- corpus(text)
summary(data.cp)

```

```{r tokenization, echo = FALSE, message = FALSE}

# Tokenize = create a tokens object
data.tk1 <- data.cp %>% 
                   tokens(remove_numbers = TRUE,
                          remove_punct = TRUE,
                          remove_symbols = TRUE,
                          remove_separators = TRUE) %>%
                   tokens_split(separator = "'")

```

## Stop Words

To continue the cleaning process, we remove useless words that bring very to no information using the *stop_words* dictionary from the **quanteda** package and we map letters to lower cases since names (such as first or last names) are not of a specific importance in this book. 

The advantage of removing stop words is that it reduces the dimension of the number of features/terms to analyze so that the focus of the analysis is on terms that bring relevant information. In this aim, we remove the word "chapter" which does not provide any value. 

### Quanteda

```{r cleaning, echo = FALSE, message = FALSE}

# Removes stop words and converts into lower cases 
data.tk2 <- data.tk1 %>%
            tokens_remove(stop_words$word) %>% 
            tokens_remove("chapter") %>%
            tokens_tolower()

```

## Lemmatization
                                                    
Lemmatization simplifies tokens by generating tokens from a dictionary and reduces the vocabulary to its simplest and meaningful essence. Therefore the set of types in a corpus is shorten. For example, "started" and "starts" were reduced to "start" and have thus "start" as a lemma. 

### Quanteda

```{r lemmatization, echo = FALSE, message = FALSE}

# Lemmatization
data.tk2 <- data.tk2  %>%  tokens_replace(pattern = hash_lemmas$token, 
                                          replacement = hash_lemmas$lemma)
```

## Stemming 

Stemming also simplifies tokens by reducing a word to its stem with simple rule based algorithm *token_wordstem()* function. Just as lemmatization, stemming reduces the size of a vocabulary but in an inconsistent way. 

For interpretation matter, since the interpretation of the terms are important here, we decide not to use the stemming in the rest of the analysis.

### Quanteda

The below output displays the first twelve tokens reduced to their steam for each document (i.e. chapter). For example, "snow-clearing" was reduced to "snow-clear". 

```{r stemming, echo = FALSE, message = FALSE}

# Stemming
data_stem <- data.tk2 %>% tokens_wordstem()

```

### Tidytext "remove"

[ADD SAME CODE FOR THIS PACKAGE]

## Document-Term Matrix (DTM)

### Quanteda

```{r DTM, echo = FALSE, message = FALSE}

# Creates a document-feature matrix
data.dfm <- dfm(data.tk2)  
data.dfm

```

### Tidytext "remove"

[Add code for this package]

## Global Frequencies

### Quanteda

The below table displays the term frequencies. The *feature* column indicates a lemmatized "and stemmatized" token, *frequency* returns the number of time the term is found in the corpus, the *rank* sorts the terms by frequencies, the *docfreq* indicates the document frequency which is the number of documents in which the token is found and *group* indicates ?.

```{r term frequency, echo = FALSE, message = FALSE}

# Generates counts and document frequencies summaries of the feature of the document-feature matrix "data.dfm"
data.freq <- textstat_frequency(data.dfm) 

library(kableExtra)
kbl(head(data.freq, 10), align = c('c', 'c', 'c', 'c', 'c'), 
    caption = "Feature/term frequencies") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  
```

### Tidytext

[ADD CODES]

## Term Frequency and Inverse Document Matrix (TF-IDF)

The TF-IDF matrix is a weighted document-feature matrix by term frequency-inverse document frequency (TF-IDF). In other words, it is used to detect which words are specific to one document. 

### Quanteda

The below output shows, for the document-feature matrix of 16 documents (i.e. chapters), 8,640 features (i.e. tokens) and 0 docvars (i.e. ??), the weighted frequency for each token in each document (i.e. chapter).

```{r term frequency and inverse document matrix, echo = FALSE, message = FALSE}

# Weighted document-feature matrix by tf-idf
data.tfidf <- dfm_tfidf(data.dfm)
data.tfidf
```

The following output shows the twenty terms that have the largest TF-IDF. 

```{r largest tf-idf per word, echo = FALSE, message = FALSE}

# S20 largest TF-IDFs
sort(apply(data.tfidf,
           2,
           max),
     decreasing = TRUE)[1:20]

```

### Tidytext

[ADD CODES]

## Zipf's Law

The Zipf's law shows the distribution of words used in a corpus. It says that most frequent terms are not specific to one document since they are frequent in all documents. 

### Quanteda

The below plot is on a log-log scale, the frequency versus rank gives a linear relation [EXPLAIN WHY]. 

For example, the $log(f_women) \approx a + b*log(r_women) \approx ?$ 

```{r fit, echo = FALSE, message = FALSE}

library(stats)
#fit_women <- lm() # [FINISH!!]

```

[IMPROVE PLOT MAYBE ?]

```{r Zipf law plot log-log, echo = FALSE, message = FALSE}

# Illustration of the Zipf's law 
ggplot(data.freq, aes(x = log(rank), y = log(frequency)))+ 
   geom_point()+
   geom_label(data = data.freq[c(1:5,6627:6631)], aes(label=feature))+
   ggtitle("The Zipf's law on a log-log scale")
```

Both plots show that *women*, *female*, *data* and *found* are the most frequent terms of the corpus indicating that they are not specific to one chapter (i.e. document) but are frequent in all chapters. 

### Tidytext

[ADD codes]











