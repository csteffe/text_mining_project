---
output:
  pdf_document: default
  html_document: default
---
# 1. Data Gathering

[we will explain the method: identify your own source of text data and perform any necessary scraping or
parsing tasks to construct the dataset -> read from pdf, why it's easy -> just load the data and when it can be more difficult -> encoding issues ]

```{r setup, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

The book *Invisible Women* under study was downloaded from [ADD WEBSITE SOURCE] in its PDF version. To upload it in *Rstudio*, we use the *pdf_text* utility from the **pdftools** package that extracts texts from pdf files.

```{r load data, echo = FALSE, message = FALSE}
text <- pdf_text(here::here("data/Invisible Women.pdf"))
```

# 2. Data Structuring and Cleaning

## Corpus

The preface of the book starts on page 11 and the introduction on page 15.  

(To create corpus with chapter : https://cran.r-project.org/web/packages/corpus/vignettes/corpus.html, and cleaning.)  

```{r start and stop, echo = FALSE, message = FALSE, warning = FALSE}

# The text starts after the Project Gutenberg header...
start <- grep("CHAPTER 1", text)
start < -start[1]
#print(book[34])

# The text ends at the Project Gutenberg footer.
stop <- grep("Afterword", text) - 1
stop<-stop[2]
#print(text[259])

lines <- text[start:stop]

```

```{r, chapters, echo = FALSE, message = FALSE}

# chapters start with "1.", "2.", etc...
chapter <- grep("CHAPTER", lines)

# Gets the section texts (including the front matter)
start <- c(1, chapter) # + 1 to skip title
end <- c(chapter - 1, length(lines))
text <- mapply(function(s, e) paste(lines[s:e], collapse = "\n"), 
               start, 
               end)

# Trims leading and trailing white space
text <- trimws(text)

# Discards the front matter
text <- text[-1]

# Gets the section titles, removing the prefix ("1.", "2.", etc.)
document <- sub("^[[:space:]]*[[:digit:]]+[.][[:space:]]*", "", lines[chapter])
document <- trimws(document)

```

After indicating where exactly the text under study starts and ends and, after extracting the chapter titles and organizing the text by chapters, we obtain the usable data to further analyze *Invisible Women*'s text. 

```{r corpus, echo = FALSE, message = FALSE}

library(corpus)

# Creates a corpus object
data <- corpus_frame(document, text) 

# set the row names; not necessary but makes results easier to read
#rownames(data) <- sprintf("ch%02d", seq_along(chapter))

```

```{r title wrangling, echo = FALSE, message = FALSE}

# Cleans title to keep only CHAPTER x, and clean text to remove Chapter X

for (i in 1:9){
  data[i,1] <- substr(data[i,1],
                      1,
                      9)
  
  data$text[[i]] <- substring(data$text[[i]],
                              10)
}

for (i in 10:16){
  data[i,1] <- substr(data[i,1],
                      1,
                      10)
  
  data$text[[i]]<-substring(data$text[[i]],
                            11)
}


# Checks type of "data"
#typeof(data) # list

print(data)

```

## Quanteda

### Tokenization

The **Quanteda** package uses a corpus object (unlike **tidytext** package that uses a data frame?).

The below summary shows that *Invisible Women* consists of 16 documents. And for each document, three columns indicates the number of tokens (i.e. terms), the number of sentences as well as the number of token types per document. 

```{r corpus summary, echo = FALSE, message = FALSE}

library(quanteda)

# Loads into a corpus
data.cp <- corpus(text)
summary(data.cp)

```

Tokenization is the method used to split a text into tokens (i.e. unit of analysis, here: words?). Here, we tokenize (i.e. split) the chapters (i.e. document) by space. To tokenize, we proceed to remove punctuation, symbols and separators because we believe that removing them will not affect our analysis and keeping them will not bring more insight to our analysis. We keep numbers since the book focuses on statistics. Therefore, numbers will be a key element of the analysis?. 

```{r tokenization, echo = FALSE, message = FALSE}

# Tokenize = create a tokens object
data.tk1 <- tokens(data.cp,
                   remove_numbers = FALSE,
                   remove_punct = TRUE,
                   remove_symbols = TRUE,
                   remove_separators = TRUE) 

```

### Stop Words

To continue the cleaning process, we remove useless words that bring very to no information using the *stop_words* dictionary from the **quanteda** package and we proceed to map letters to lower cases since names (such as first or last names) are not of a specific importance in this book. 

```{r cleaning, echo = FALSE, message = FALSE}

# Removes stop words and converts into lower cases 
data.tk2 <- data.tk1 %>%
            tokens_remove(stop_words$word) %>%
            tokens_tolower()

```

### Lemmatization

Lemmatization also simplifies tokens by generating a token dictionary for each token to reduce a vocabulary (i.e. set of types in a corpus) to its useful essence. [ADD AN EXAMPLE LIKE ABOVE] For example, "?" was reduced to "" and has thus "?" as a lemma. 

```{r lemmatization, echo = FALSE, message = FALSE}

# Lemmatization: replaces tokens in a tokens object (i.e. text)
library(lexicon)
data.tk2  %>%  tokens_replace(pattern = hash_lemmas$token, 
                              replacement = hash_lemmas$lemma)

```

### Stemming 

Steaming simplifies tokens by reducing a word to its stem with simple rule based algorithm *token_wordstem()* function. Stemming reduces the size of a corpus'? vocabulary. 

The below output displays the first twelve tokens reduced to their steam for each document (i.e. chapter). For example, "snow-clearing" was reduced to "snow-clear". 

```{r stemming, echo = FALSE, message = FALSE}

# Stemming
data.tk2 %>% tokens_wordstem()

```

### Dcoument-Term Matrix (DTM)

```{r term frequency, echo = FALSE, message = FALSE}

# Creates a document-feature matrix
data.dfm <- dfm(data.tk2)  # EST-CE QUE LE OUTPUT EST NORMAL ? CA ME SEMBLE CHELOU ??
View(data.dfm)

```

# Global Frequencies

The below table displays the term frequencies. The *feature* column indicates a lemmatized and stemmatized token, *frequency* returns the number of time the lemma? is found in the corpus?, the *rank* indicates ?, the *docfreq* indicates the document frequency which is the number of documents in which the token is found and *group* indicates ?.

```{r term frequency, echo = FALSE, message = FALSE}

# Generates counts and document frequencies summaries of the feature of the document-feature matrix "data.dfm"
data.freq <- textstat_frequency(data.dfm) 

library(kableExtra)
kbl(head(data.freq, 10), align = c('c', 'c', 'c', 'c', 'c'), 
    caption = "Feature/term frequencies") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  
```

### Term Frequency and Inverse Document Matrix (TF-IDF)

The TF-IDF matrix is a weighted document-feature matrix by term frequency-inverse document frequency (TF-IDF). In other words, it is used to detect which words are specific to one document. The below output shows, for the document-feature matrix of 16 documents (i.e. chapters), 8,640 features (i.e. tokens) and 0 docvars (i.e. ??), the weighted frequency for each token in each document (i.e. chapter).

```{r term frequency and inverse document matrix, echo = FALSE, message = FALSE}

# Weighted document-feature matrix by tf-idf
data.tfidf <- dfm_tfidf(data.dfm)
data.tfidf
```

The following output shows the twenty terms that have the largest TF-IDF. 

```{r largest tf-idf per word, echo = FALSE, message = FALSE}

# S20 largest TF-IDFs
sort(apply(data.tfidf,
           2,
           max),
     decreasing = TRUE)[1:20]

```

### Zipf's Law

The Zipf law shows the distribution of words used in a corpus. It says that most frequent terms are not specific to one document since they are frequent in all documents. 

The below plot displays the term frequencies against their ranks on a non log-log scale. 

[IMPROVE PLOT ? MAYBE GGPLOT ?]

```{r Zipf law plot, echo = FALSE, message = FALSE}

# Illustration of the Zipf's law 
plot(frequency~rank, 
     data = data.freq, 
     pch = 20)

# Adds text on plot
text(frequency~rank, 
     data = data.freq[1:4,], 
     label = feature, 
     pos=4)

```

And the second plot is on a log-log scale, the frequency versus rank gives a linear relation [EXPLAIN WHY]. 

For example, the $log(f_women) \approx a + b*log(r_women) \approx ?$ 

```{r fit, echo = FALSE, message = FALSE}

library(stats)
#fit_women <- lm() # [FINISH!!]

```

[IMPROVE PLOT MAYBE ?]

```{r Zipf law plot log-log, echo = FALSE, message = FALSE}

# Illustration of the Zipf's law 
plot(log(frequency)~log(rank), 
     data = data.freq, 
     pch = 20)

# Adds text on plot
text(log(frequency)~log(rank), 
     data = data.freq[1:4,], 
     label = feature, 
     pos = 4)

```

Both plots show that *women*, *female*, *data* and *found* are the most frequent terms of the corpus indicating that they are not specific to one chapter (i.e. document) but are frequent in all chapters. 













