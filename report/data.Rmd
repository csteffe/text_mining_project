```{r setup, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

# 1. Data Gathering

The book *Invisible Women* under study was downloaded from [source: https://yes-pdf.com/book/113#google_vignette] in its PDF version. To upload it in *Rstudio*, we use the `pdf_text` utility from the **pdftools** package that extracts texts from pdf files.
One of its advantages, is that is it easy to upload. On the cons side, the cleaning can sometimes be quite long and tedious.

```{r load data, echo = FALSE, message = FALSE}
#load the data
text <- pdf_text(here::here("data/Invisible Women.pdf"))
```

Here are the book's characteristics:

| Title | Author | Date | Parts | Chapters | Pages |
|:---|:---|:---|:---|:---|:---|
| Invisible Women | Caroline Perez Criado | 2019 | 6 | 16 | 399 |

# 2. Data Structuring and Cleaning

## Corpus

*The preface* of the book starts on page 11 and *the introduction* on page 15.  

```{r start and stop, message=FALSE, warning=FALSE, include=FALSE}

# The text starts after the title page of part I
start <- grep("CHAPTER 1", text)
start < -start[1]

# The text ends just before the "Afterword" section
stop <- grep("Afterword", text) - 1
stop<-stop[2]

# extract lines from the book
lines <- text[start:stop]

```

```{r chapters, echo=FALSE, message=FALSE, warning=FALSE}

# chapters start with "CHAPTER 1", "CHAPTER 2", etc...
chapter <- grep("CHAPTER", lines)

# Gets the section texts (including the front matter)
start <- c(1, chapter) # + 1 to skip title
end <- c(chapter - 1, length(lines))
text <- mapply(function(s, e) paste(lines[s:e], collapse = "\n"), 
               start, 
               end)

# Trims leading and trailing white space
text <- trimws(text)

# Discards the front matter
text <- text[-1]

# Gets the section titles
document <- sub("^[[:space:]]*[[:digit:]]+[.][[:space:]]*", "", lines[chapter])
document <- trimws(document)

```

After indicating where exactly the text under study starts and ends, and after extracting the chapter titles and organizing the text by chapters, we obtain the usable data to further analyze *Invisible Women*'s content. 

```{r corpus, echo = FALSE, message = FALSE}
# Creates a corpus object
data <- corpus_frame(document, text) 
```

The below output shows the beginning of the first five chapters of the book.

```{r title wrangling, echo = FALSE, message = FALSE}

# Cleans title to keep only CHAPTER x, and clean text to remove Chapter X

for (i in 1:9){
  data[i,1] <- substr(data[i,1],
                      1,
                      9)
  
  data$text[[i]] <- substring(data$text[[i]],
                              10)
}

for (i in 10:16){
  data[i,1] <- substr(data[i,1],
                      1,
                      10)
  
  data$text[[i]]<-substring(data$text[[i]],
                            11)
}
# add the parts of the book
data <-  data %>% mutate("part" = c(1,1,2,2,2,2,3,3,3,4,4,5,5,5,6,6))

# Checks type of "data"
#typeof(data) # list

kable(data[1:5,],
      caption = "Head of Invisible Women's corpus") %>% 
      kable_styling(bootstrap_options = c("striped",
                                          "hover", 
                                          "condensed", 
                                          "responsive"),
                    full_width = T, 
                    position = "left", 
                    font_size = 16, 
                    fixed_thead = T)

```

## Tokenization

Tokenization is the method used to split a text into tokens. Our unit of analysis are words. Here, we tokenize the chapters (i.e. document) by space. To do so, we proceed to remove numbers, punctuation, symbols and separators because we believe that it will not affect our analysis and keeping them [KEEPING WHAT?] will not bring more insight.

### Quanteda

The **Quanteda** package uses a corpus object.

The below summary shows that *Invisible Women* consists of 16 documents (i.e. chapters) and for each document, three columns indicate the number of tokens, the number of sentences as well as the number of token types per document. 

```{r corpus summary, echo = FALSE, message = FALSE}
# Loads into a corpus
data.cp <- corpus(text)
summary(data.cp)

```

```{r tokenization, echo = FALSE, message = FALSE}

# Tokenize = create a tokens object
data.tk1 <- data.cp %>% 
                   tokens(remove_numbers = TRUE,
                          remove_punct = TRUE,
                          remove_symbols = TRUE,
                          remove_separators = TRUE) %>%
                   tokens_split(separator = "'") %>%
                   tokens_split(separator = "-") 
```

## Stop Words

To continue the cleaning process, we remove useless words that bring very to no information using the *stop_words* dictionary from the **quanteda** package and we map letters to lower cases since names (such as first or last names) are not of a specific importance in this book. 

The advantage of removing stop words is that it reduces the dimension of the number of features/terms to analyze so that the focus of the analysis is on terms that bring relevant information. In this aim, we remove the word "chapter" which does not provide any value. 

### Quanteda

```{r cleaning, echo = FALSE, message = FALSE}

# Removes stop words and converts into lower cases 
data.tk2 <- data.tk1 %>%
            tokens_remove(stop_words$word) %>% 
            tokens_remove("chapter") %>%
            tokens_tolower()

```

## Lemmatization
                                                    
Lemmatization simplifies tokens by generating tokens from a dictionary and reduces the vocabulary to its simplest and meaningful essence. Consequently, the set of types in a corpus is shortened [THE NUMBER OF TOKEN TYPES IS REDUCED?]. For example, "started" and "starts" are reduced to "start" and have thus "start" as a lemma. 

The below output displays for each chapter the lemmas of the first tokens as well as the total number of different lemmas by chapter. For example, chapter (i.e. text) one contains 2,413 different lemmas. 

```{r lemmatization, echo = FALSE, message = FALSE}

# Lemmatization
data.tk2 <- data.tk2  %>%  tokens_replace(pattern = hash_lemmas$token, 
                                          replacement = hash_lemmas$lemma)
head(data.tk2)
```

## Stemming 

Stemming also simplifies tokens by reducing a word to its stem with simple rule based algorithm usig the *token_wordstem()* function. As lemmatization, stemming reduces the size of a vocabulary but in an inconsistent way. [EXPLAIN WHY IN AN INCONSISTENT WAY]

Since the interpretation of the tokens matter, we decide not to use the stemming in the rest of our analysis and only apply it here to demonstrate its purpose, since reducing a word to its stem does not guarantee meaningful tokens (e.g official is reduced to offici).

The below output displays the first twelve tokens reduced to their steam for each document. For example, "snow-clearing" was reduced to "snow-clear". 

```{r stemming, echo = FALSE, message = FALSE}
# Stemming
data_stem <- data.tk2 %>% tokens_wordstem()
head(data_stem)
```

## Document-Term Matrix (DTM)

Now, without considering the stemming, we compute the **Document-Term-Matrix** that will be useful throughout the analysis.

The below snapshot of the matrix indicates that after cleaning and lemmatizing, there are 5,742 features to be analyzed and that the DTM is sparse at 83.76% (i.e. contains mostly zeros). The matrix displays the frequency of features (i.e. terms or words here) by documents (i.e. texts or chapters here). For example, the first row indicates that the word *sexist* is found twice and the word *town* is found six times and, the first column indicates that the word *sexist* is found in chapter one, chapter four and chapter 6. 

```{r DTM, echo = FALSE, message = FALSE}
# Creates a document-feature matrix
data.dfm <- dfm(data.tk2)  
data.dfm
```

## Global Frequencies

The below table displays the term frequencies (i.e. frequency by term). The *feature* column shows lemmatized tokens, *frequency* returns the number of times the term is found in the corpus, the *rank* sorts the terms by decreasing frequencies, the *docfreq* indicates the document frequency which is the number of documents in which the token is found. [WHAT ABOUT GROUP?]

This table indicates that for example, the term *woman* has the highest frequency (i.e. appears the most in the corpus) and is found in all the chapters (i.e. documents) of the book. 

```{r term frequency, echo = FALSE, message = FALSE}

# Generates counts and document frequencies summaries of the feature of the document-feature matrix "data.dfm"
data.freq <- textstat_frequency(data.dfm) 

kbl(head(data.freq, 10), align = c('c', 'c', 'c', 'c', 'c'), 
    caption = "Feature/term frequencies") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  
```

## Term Frequency and Inverse Document Matrix (TF-IDF)

The TF-IDF matrix is a weighted document-feature matrix by term frequency-inverse document frequency (TF-IDF) [IT SEEMS WRONG]. In other words, it is used to re-balance a word frequency with respect to its document specificity.

The below output shows the weighted frequency for each token, by document. 

```{r term frequency and inverse document matrix, echo = FALSE, message = FALSE}
# Weighted document-feature matrix by tf-idf
data.tfidf <- dfm_tfidf(data.dfm)
data.tfidf
```

From the above TF-IDF matrix, we extract the twenty terms that have the largest TF-IDF in a decreasing fashion. [ADD INTERPRETATION BY COMPARING THE TERMS TAX AND SANCHEZ]

```{r largest tf-idf per word, echo = FALSE, message = FALSE}
# 20 largest TF-IDFs
sort(apply(data.tfidf,
           2,
           max),
     decreasing = TRUE)[1:20]

```

## Zipf's Law

The Zipf's law shows the distribution of words used in a corpus compared to its rank in the frequency table. It says that the frequency of a token is inversely proportional to its rank.

The below plot displays the log-frequency agaist the log-rank of the terms and shows the negative linear relation relationship between the frequency and the rank. 

This plot shows that *women*, *female*, *datum* and *find* are the most frequent terms of the corpus indicating that they are not specific to one chapter but are indeed frequent in all chapters. It means, according to the Zipf's law, that these words are very frequent in the corpus and could hide some meaningful information as they are not stop words.

```{r Zipf law plot log-log, echo = FALSE, message = FALSE}

# Illustration of the Zipf's law 
ggplot(data.freq, aes(x = log(rank), y = log(frequency)))+ 
   geom_point()+
   geom_label(data = data.freq[c(1:5,6627:6631)], aes(label=feature))+
   ggtitle("The Zipf's law on a log-log scale")
```














