```{r, echo=F, message=F}
source(here::here("scripts/setup.R"))

```

# Exploratory Data Analysis (EDA)

To proceed to the Exploratory Data Analysis (EDA), we use the **quanteda package**.

## Cloud of Words

To start the EDA, we proceed to visually assess which words are expected to be found the most regularly in the corpus. Thus, the CoW plot is a visual representation of term frequencies in which the size and position of terms are proportional to their frequencies. Nevertheless, this visualization is more graphic than informative since the only information we can extract from is that the terms with the largest font sizes are the most frequent in the corpus. Indeed, from the below plot, we see that *woman* is the most used (largest font size and centered position) term in the corpus, followed by *female*, *datum*, *male*, *find*, *time*, *gender* and *study*. 

To generate the CoW plot, we use the DTM that was obtained after cleaning and lemmatizing but without considering the stemming, see *Document-Term Matrix* in Section 2 **Data Structuring and Cleaning**.  

```{r cloud of words, echo=F, message=FALSE, warning=FALSE}

# Generate Cloud of Words plot
textplot_wordcloud(data.dfm)

```

## Global Frequency

To assess more accurately the frequency of the terms in the corpus, we compute the global frequencies. The following graphical representation displays the ten most frequent terms in the corpus. As inferred previously from the CoW, we see that *woman* is the most frequent term overall followed by *female*, *datum*, *male*, *find*, *time*, *gender* and *study*. 

Global frequencies indicate that *woman* is by far the most frequent term and that the frequency differences between the following nine terms are much less extreme (i.e. their ranks are less distinguishable). 

In addition, we see that the term *data* was lemmatized into *datum*. 

```{r TF-IDF plot, echo=F, message=F, warning=F}

# Generates counts and document frequencies summaries of the feature of the document-feature matrix "data.dfm"
data.freq <- textstat_frequency(data.dfm) 

#Plot the results 
tf_plot <- data.freq  %>% 
              top_n(10, frequency) %>%                    # selects the top 10 rows by frequency values
              ggplot(aes(x = reorder(feature, frequency), # orders terms by frequency values
                         y = frequency)) + 
              geom_bar(stat = "identity", fill = "dodgerblue3", alpha = 0.5) + # generates a barplot
              coord_flip() +                              # flips the coordinates
              labs(title = "Top 10 global frequencies",
                   subtitle = "",
                   x = "Term",
                   y = "Freqency",
                   caption = "source: Invisible Women.pdf")

tf_plot

```

## Term-Frequency (TF)

To deep dive into these frequencies, we then display a Term-Frequency (TF) table providing information on term frequencies, their rank and document frequencies. Indeed, the *feature* lists the lemmatized tokens, the *frequency* provides the number of times the term is found in the corpus (i.e. global frequency), the *rank* sorts the terms by decreasing frequencies (i.e. rank is negatively proportinoal to the frequency), the *docfreq* indicates the number of documents in which the token is found (i.e. document frequency). 

The table below shows that *woman* appears 1594 times in the corpus and in all 16 documents (docfreq = 16) which means that it is not a document-specific term. Moreover, it appears four times more than the second most frequent term *female*.

```{r term frequency, echo = FALSE, message = FALSE}

kbl(head(data.freq[,1:4], 
         10), 
    align = c('c', 'c', 'c', 'c', 'c'), #originally the data.freq has a column named group but as it is not relevant for us we select only the 3 first columns
    caption = "Term frequencies") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

## Term-Frequency against Document-Frequency 

The below graph gives an overall graphical view of the previous table indicating 1) the term-frequency and 2) whether a word is specific to a document or not (i.e. document-frequency). As observed previously, we see that *woman* is the most frequent term in the corpus and that it is not document-specific since it is the most frequent term over all chapters of the book. On the contrary, the terms *trial* and *tax* are less frequent overall and also less frequent in documents meaning that they are more specific to some chapters (i.e. documents) of the book. 
 
Since we observe that *woman* has a very large term-frequency and is not document-specific, we decide to remove this term for the rest of our analysis because we believe it will hide some important and interesting insights as it will always appear as the top frequency in all documents. 

[Can we use a nicer looking theme ?]
```{r TF vs DF, echo=F, message=F, warning=F}

data.freq %>% 
 ggplot(aes(x = log(docfreq), 
            y = log(frequency))) +
 geom_text(aes(label = feature), 
           position = position_jitter(), 
           size = 3) + 
 labs(title = "TF versus DF (log-log)",
      subtitle = "",
      x = "Document log-frequency",
      y = "Term log-frequency",
      caption = "source: Invisible Women.pdf") +
  theme_minimal()

```

```{r second cleaning, echo = FALSE, message = FALSE}

# second cleaning without "woman"
data.tk3 <- data.tk2 %>% tokens_remove("woman")

# Creates a second document-feature matrix
data.dfm2 <- dfm(data.tk3) 

# generate second second doc frequency
data.freq2 <- textstat_frequency(data.dfm2)  

# Generate a second weighted document-feature matrix by tf-idf
data.tfidf2 <- dfm_tfidf(data.dfm2)

```

## Term-Frequency by Document 

The plot below shows kind of the same information than the frequency plot but with a little twist. Here, we show the top 10 frequencies associated with their chapter. It would have been interesting to have the top frequencies of each documents but there would have been an information overload as there are 16 chapters, so we decided not to display it. 

Chapter 10, namely *The Drugs Don't Work*, is associated with "sex", "drug" and "study". In the previous plot (TF versus DF), we've been able to see that although "sex" and "study" appear in multiple documents, "drug" is a bit less document frequent. Nevertheless, we do not want to over interpret and the specificity will be covered more deeply in the following section. 

Concerning chapter 13, *From Purse to Wallet*, we only have one related word, "tax". Such as "drug", although frequent, "tax" does not appear in a lot of documents. 

chapter 14,*Women's Rights are Human Rights*,has a large frequency of the term *female*.

Chapter 3, *The Long Friday*, is related to the words *pay*, *leave* and *time*.

Chapter 4, *Myth of Meritocracy*, is associated to *female* and *male*. 

Even though some of those words are informative, others are extremely general. Indeed, it is not enlightening to have *female* associated to one document as the whole book is about feminism.  

```{r cloud of words by document plot, echo=F, message=F, warning=F}

nowoman.10 <- data.dfm2 %>% 
           tidy() %>%  
           #group_by(term) %>% 
  # creates a tibble 
           top_n(10, count) %>%       # selects the top 10 rows by count   
           #ungroup() %>% 
           ggplot(aes(x = reorder(term, count), 
                      y = count)) +   
           geom_col(fill = "dodgerblue3", alpha = 0.5) + 
           coord_flip() + 
          labs(title = "Top 10 frequencies along with documents",
           subtitle = "",
           x = "Term",
           y = "Freqency",
           caption = "source: Invisible Women.pdf") +
           facet_wrap(~ document, 
                      ncol = 2) +
           theme_minimal() 

nowoman.10

```

## Zipf's Law

The Zipf's law shows the distribution of words used in a corpus compared to his rank in the frequency table. It says that the frequency of a token is inversely proportional to its rank.

The below plot is on a log-log scale, the frequency versus rank gives a negative linear relation as the original distribution is a negative exponential function. 

```{r Zipf law plot log-log, echo = FALSE, message = FALSE}

# Illustration of the Zipf's law 
ggplot(data.freq2, aes(x = log(rank), y = log(frequency)))+ 
   geom_point(color = "dodgerblue3", alpha = 0.5)+
   geom_label(data = data.freq2[c(1:5,6627:6631)], aes(label=feature)) +
   labs(title = "The Zipf's law on a log-log scale",
    subtitle = "",
    caption = "source: Invisible Women.pdf")+
   theme_minimal()


```

This plot shows that *female*, *datum*, *male*, *find* and *time* are the most frequent terms of the corpus probably indicating that they are not specific to one chapter but are frequent in all chapters. Indeed, although Zipf's Law do not state about the specificity, there is a very low probability that those words are specific to a chapter that is sufficiently long to make it as frequent. It means, according to the Zipf's law, that these words are very frequent in the corpus therefore it could hide some meaningful information as they are not stop words. The Zipf's law now leads us to move to weighted frequencies. 

## Term Frequency and Inverse Document Matrix (TF-IDF)

The TF-IDF matrix is a weighted document-feature matrix by term frequency-inverse document frequency (TF-IDF). In other words, it is used to rebalance a word frequency with respect to its document specificity.

The below output shows, for the document-feature matrix of 16 documents and 5'741 features, the weighted frequency for each token by document. The sparsity has increased a bit in comparison to the DTM matrix as "women", a term occurring in all the document, has been removed.

```{r term frequency and inverse document matrix, echo=FALSE, message=FALSE}
# Weighted document-feature matrix by tf-idf
data.tfidf2 <- dfm_tfidf(data.dfm2)
data.tfidf2
```

The following plot shows the twenty largest TF-IDF with the associated terms.
Note that it is equal to the result of computing for each term the maximum TF-IDF over all chapters of the book.
We can see for example that the words "tax","trial", "drug" and "dummy"  appear quite often in the corpus and in few documents.

```{r max tf-idf, echo=F, message=F, warning=F,fig.width=8}

data.tfidf2 %>% 
    tidy() %>%
    group_by(term) %>%
    summarize(count = max(count)) %>% 
    arrange(desc(count)) %>%
    top_n(20, count) %>%
    ggplot(aes(x = reorder(term, count), y = count)) + 
    geom_bar(stat = "identity", fill = "dodgerblue3", alpha = 0.5) + 
    coord_flip() +
    geom_text(aes(label = sprintf("%0.1f", round(count, digits = 1))),
              hjust=1, vjust=0.5, color="white", size=3.5)+
    labs(title = "Largest TF-IDF in the book",
    subtitle = "",
             x = "Max TF-IDF",
             y = "Term",
             caption = "source: Invisible Women.pdf") + 
    theme_minimal() 
             


```  

## TF-IDF by Document

The following plot shows the 10 highest TF-IDF associated with their chapter. 
To avoid redundancy, we will not comment each of the term-document. But for chapter 10, *The Drugs Don't Work*, we can see it is now associated with *trial* and how we kind of guess *drug*. 
The term *tax* is still really frequent and we can now definitely state that it is specific to chapter 13, *From Purse to Wallet*.   
Chapter 14, *Women's Rights are Human Rights*, is in fact more specifically associated with the term *interrupt* than with the term *female* in TF by document.

```{r tfidf by document plot, echo=F, message=F, warning=F}

tfidf_plot <- data.tfidf2 %>% 
              tidy() %>%  
              top_n(10, count) %>% 
              ggplot(aes(x = term, 
                         y = count)) +
              geom_col(fill = "dodgerblue3", alpha = 0.5) + 
              coord_flip() + 
              facet_wrap(~ document, 
                         ncol = 2) +
              theme_minimal() +
               labs(title = "10 Largest TF-IDF Associated to Chapters",
               subtitle = "",
               x = "Term",
               y = "Count",
               caption = "source: Invisible Women.pdf")

tfidf_plot

```                

## Keyness

The keyness measure is a chi-square test of independence indicating whether some terms are characteristic of a target compared to a reference. To illustrate that we select chapter 14, *Women’s Rights are Human Rights*, which has the specific term *interrupt* which is a rather vague term and compute its Keyness compare to the reference.
```{r keyness, echo=F, message=F, warning=F}
data.keyness <- textstat_keyness(data.dfm, target =paste("text",14,sep="") )
  print(textplot_keyness(data.keyness)) 
```

The plot above allows us to see that this chapter is characterized by the terms *party*, *politician*, *election* or even *candidate* and to conclude at first glance that this chapter is more about political topics than the rest of the corpus.

We compute for each chapter of the book the keyness of terms and we display them in the below gif format. Each chapter is then at some point the target and the reference. 
```{r keyness2, echo=F, message=F, warning=F, animation.hook = "gifski", , interval = 15}
for (i in 1:nrow(data)){ 
  
   data.keyness <- textstat_keyness(data.dfm, target =paste("text",i,sep="") )
  print(textplot_keyness(data.keyness)) 
  
}

```

## Link between words

Here, we look at how words are connected and how inter-connected they are. We first compute the co-occurrences between terms. 

The feature co-occurrence matrix is a 5'741 by 5'741 matrix with 32,959,081 elements. Because of the size of the matrix, we reduce the matrix's size following a condition. Thus, the condition is to keep co-occurrences that occur more than 110 times between two words. After filtering for co-occurrences greater than 110, we get a smaller feature co-occurrence matrix of dimensions 20 by 20 features. 

```{r co-occurrences, echo=F, message=F, warning=F}

# Creates a feature co-occurrence matrix


data.coocc <- fcm(data.tk2,     
                  context = "document", 
                  tri = FALSE)                 # returns upper and lower triangles of the matrix

index <- data.freq[-c(1,2),] %>% 
         filter(frequency > 110) %>%  # filter for co-occurrences greater than 110
         data.frame() %>% 
         select(feature)

data.coocc <- data.coocc[index$feature, index$feature]

data.coocc

```

Using the above feature co-occurrences matrix, we generate a network (object). To generate a readable network of co-occurrences, we add a second condition on feature co-occurrences that is to keep only the strongest co-occurrences, greater than 2100. 

```{r network, echo=F, message=F, warning=F, fig.width=10}
# Creates a feature co-occurrence matrix
# Keep strongest co-occurrences
data.coocc[data.coocc <= 2100] <- 0
data.coocc[data.coocc > 2100] <- 1

# Creates a network from the co-occurrences matrix
network <- graph_from_adjacency_matrix(data.coocc, 
                                       mode = "undirected", 
                                       diag = FALSE)

plot(network, 
     layout = layout_with_kk)
title(main = "Network graph", sub = "source: Invisible Women.pdf")
  

```
The graph above reveals that the terms *datum*, *male* and *find* are central and co-occur a lot with the other terms around. It is surprising that for a book named "Invisible Women", the term "female" is not found in the largest co-occurrences. So we reintroduced the term "woman" and even more surprisingly this term is still not in the center of the co-occurrences despite its enormous frequency.

## Dispersion Plot 

Dispersion or X-Ray plots inspect where a specific token is used in each text by locating a pattern in each text. 

The below plot shows how the words *female* and *male* are used among the chapters. 

```{r pattern location, message=FALSE, warning=FALSE, include=FALSE}

# Checks the result
head(kwic(data.cp, pattern = c("female","male")))

kbl(head(kwic(data.cp, 
              pattern = c("female","male"))), 
    caption = "Pattern Location") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

```{r xray plot, echo=F, message=F, warning=F}
# Plot
textplot_xray(kwic(data.cp, pattern = c("female","male"))) +
 labs(title = "Lexical dispersion plot (female and male)",
                   subtitle = "",
                   caption ="source: Invisible Women.pdf")
```

The above lexical dispersion plot shows that these two terms appear in almost all chapters and seem to appear close together. It also shows that they are present together with higher frequency in chapters 4 (*The Long Friday*) and 14(*Women's Rights are Human Rights*).

The below plot shows that the term *sex* is more specific to chapter 10 (*The Drugs Don't Work*) and that *female* and *sex* are not necessarily associated. Note that here *sex* is reduced to its lemma and it is necessary to keep in mind all its declensions in the analysis of this plot.

```{r xray-plot 2 patterns, echo=F, message=F, warning=F}

# Plot
textplot_xray(kwic(data.cp, pattern = c("female")),
              kwic(data.cp, pattern = c("sex")),
              scale = "absolute") +
               labs(title = "Lexical dispersion plot (female and sex)",
                   subtitle = "",
                   caption ="source: Invisible Women.pdf")

# intéressant de comparer female et male ?? 
```


```{r xray plot 2 patterns, echo=F, message=F, warning=F}

# Plot
textplot_xray(kwic(data.cp, pattern = c("male")),
              kwic(data.cp, pattern = c("sex")),
              scale = "absolute") +
               labs(title = "Lexical dispersion plot (male and sex)",
                   subtitle = "",
                   caption ="source: Invisible Women.pdf")

```
This last plot reveals that the term *male* is frequently present in Chapter 10, which is the most specific to the term *sex*. Furthermore, we can conclude that this chapter(*The Drugs Don't Work*) will focus on gender topics by associating the terms *sex*, *female* and *male*.

## Lexical Diversity

Lexical diversity is a diversity index that measures the richness of the vocabulary in one document. 

### Token-Type Ratio (TTR) 

The TTR is a diversity measure indicating a document's richness in the number of token types. The more types of tokens are found in a document, the richest is the vocabulary of this specific document. The closest to 1 is the TTR the richest is the vocabulary of a document. We need to be careful with the TTR measure because it is dependent on the length of the document. TTR is computed using the document-term matrix.

The below graph sorts chapters of the book by descending TTR. According to TTR, chapter 15 *Who Will Rebuild?* has the richest vocabulary among all chapters of the book with a TTR of 0.592 and that chapter 3 *The Long Friday* has the poorest with a TTR of 0.374. [Overall, the book seems to have a medium vocabulary richness since the TTR does not exceed 0.356[???].] ??? on le garde?


```{r TTR plot, echo=F, message=F, warning=F, fig.width=8}
ttr <- data.dfm2 %>% 
       textstat_lexdiv()  # computes the lexical diversity

ttr <- ttr %>%
       arrange(desc(ttr$TTR))

ttr %>% 
  ggplot(aes(reorder(document, -TTR), TTR)) + 
  geom_bar(stat = "identity", fill = "dodgerblue3", alpha = 0.5) +
  geom_text(aes(label = sprintf("%0.3f", round(TTR, digits = 3))),
                              vjust=1.6, color="white", size=3.5)+
  labs(title = "Token-Type Ration (TTR)",
       subtitle = "Chapter 15 has the richest vocabulary",
       x = "Chapter",
       y = "TTR",
       caption ="source: Invisible Women.pdf") +
       theme_minimal()

```

### Moving-Average Token-Type Ratio (MATTR)

Moving-Average Token-Type Ratio is an average of the Token-Type Ratio. It is an algorithm using windows of the text to compute the TTR and repeating several times over different windows of the same size the TTR computation. The advantage of the MATTR is that it is less dependent on the length of the document than TTR. A too large window can produce an error since no local TTR can be computed and a too small window results in pointless values (always 1). MATTR is computed using ordered tokens.

The below graph plots the the Moving-Average Token-Type Ration and shows that the MATTR among chapters are very similar and it ranges from 0.798 to 0.725 over all chapters of the book. According to the MATTR, chapter 15 *Who Will Rebuild?* still has the richest vocabulary with a MATTR of 0.798. 
```{r MATTR plot, echo=F, message=F, warning=F, fig.width=8}
mattr <- data.tk2 %>% 
         textstat_lexdiv(measure = "MATTR", 
                         MATTR_window = 80)    
mattr <- mattr %>%
         arrange(desc(mattr$MATTR))

mattr %>% 
 ggplot(aes(reorder(document, -MATTR), MATTR)) + 
 geom_bar(stat = "identity", fill = "dodgerblue3", alpha = 0.5) +
 geom_text(aes(label = sprintf("%0.3f", round(MATTR, digits = 3))),
                            vjust=1.6, color="white", size=3.5)+
 labs(title = "Moving-Average Token-Type Ratio (MATTR)",
                   subtitle = "",
                   x = "Chapter",
                   y = "MATTR",
                   caption ="source: Invisible Women.pdf")


```


