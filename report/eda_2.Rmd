```{r, echo=F, message=F}
source(here::here("scripts/setup.R"))
```

# Exploratory Data Analysis (EDA)

To proceed to the Exploratory Data Analysis (EDA), we use the **quanteda package**.

## Cloud of Words plot

The Cloud of Words plot is possible representation used to visualize term frequencies. Nevertheless, this vizualition is more graphic than informative, this is why we decided to start our EDA with. 

The size and the position (centered) are propotional to the frequency.

[Improve plot: use different colors]

```{r cloud of words for corpus plot, echo=F, message=F, warning=F}

CoW_cp <- textplot_wordcloud(data.dfm)

CoW_cp 

```

This plot shows that the most frequent terms in the corpus seems to be women, female, male, datum, etc...


## Global frequencies


To assess a bit more seriously the frequency of the terms, we compute the global frequencies and show the results in the plot below. The following visualization plots the 10 most frequent terms in the whole corpus. As we supposed previously with the cloud of words, the plot indicates that the term *woman* is the most frequent term overall, followed by female, datum, male and, find. The frequency difference between those words are so slight that it was hard to distinguish their rank. In addition we can see that the term "data" has been changed by lemmatization to "datum". 

[Improve plot: layout/color/add title/add subtitle/caption with source]

```{r TF-IDF plot, echo=F, message=F, warning=F}

# Generates counts and document frequencies summaries of the feature of the document-feature matrix "data.dfm"
data.freq <- textstat_frequency(data.dfm) 

#Plot the results 
tf_plot <- data.freq  %>% 
              top_n(10, frequency) %>%                    # selects the top 10 rows by frequency values
              ggplot(aes(x = reorder(feature, frequency), # orders terms by frequency values
                         y = frequency)) + 
              geom_bar(stat = "identity", fill = "dodgerblue3", alpha = 0.5) + # generates a barplot
              coord_flip() +                              # flips the coordinates
              xlab("Frequency") + 
              ylab("Term") +
              theme_minimal() +
              ggtitle("Top 10 global frequencies")

tf_plot

```
To deep dive in those frequencies, we display the terms frequency table below which resumes numerically the information (???).  The *feature* column indicates lemmatized tokens, *frequency* returns the number of time the term is found in the corpus, the *rank* sorts the terms by decreasing frequencies, the *docfreq* indicates the document frequency which is the number of documents in which the token is found. There is an important point to make here, "woman" appears 1594 times in the corpus and it is not a specific term as it appears in all the documents. Moreover *woman* appears four times more than the second most frequent term *female* in the corpus and this huge difference could hide some insights.


```{r term frequency, echo = FALSE, message = FALSE}
kbl(head(data.freq[,1:4], 10), align = c('c', 'c', 'c', 'c', 'c'), #originally the data.freq has a column named group but as it is not relevant for us we select only the 3 first columns
    caption = "Term frequencies") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  
```
## Term Frequency against Document Frequency 

Now let's have a global graphical view on the frequency of a term relative to its document frequency (i.e. number of documents in which a term is found), indicating whether a word is specific to a document or not. As seen previously, we see that *women* is not document-specific since it is the most frequent term over all chapters of the book and that it is found in all of them. On the contrary, the words *trial* and *tax* are less frequent and also less frequent in documents making them more document-specific. 
 
For all the reasons mentionned until here, we decided to remove "woman" for the rest of our analysis.
[Improve plot]

```{r TF vs DF, echo=F, message=F, warning=F}

data.freq %>% 
 ggplot(aes(x = log(docfreq), 
  y = log(frequency))) +
 geom_text(aes(label = feature), 
      position = position_jitter(), 
      size=3) + 
 labs(title = "TF versus DF (log-log)",
   subtitle = "",
   x = "Document log-frequency",
   y = "Term log-frequency",
  caption = "source: Invisible Women.pdf") +
  theme_minimal()




```

```{r second cleaning, echo = FALSE, message = FALSE}
# second cleaning without "woman"
data.tk3 <- data.tk2 %>% tokens_remove("woman")

# Creates a second document-feature matrix
data.dfm2 <- dfm(data.tk3) 

# generate second second doc frequency
data.freq2 <- textstat_frequency(data.dfm2)  

# Generate a second weighted document-feature matrix by tf-idf
data.tfidf2 <- dfm_tfidf(data.dfm2)
```


## Term-Frequency by Document 

The plot below shows kind of the same information than the frequency plot but with a little twist. Here, we show the top 10 frequencies associated with their chapter.  

Chapter 10, namely *The Drugs Don't Work*, is associated with "sex", "drug" and "study". In the previous plot (TF versus DF), we've been able to see that although "sex" and "study" appear in multiple documents, "drug" is a bit less document frequent. Nevertheless, we do not want to over interpret and the specificity will be covered more deeply in the following section. 

Concerning chapter 13, *From Purse to Wallet*, we only have one related word, "tax". Such as "drug", although frequent, "tax" does not appear in a lot of documents. 

chapter 14 *Women's Rights are Human Rights*

Chapter 3, *The Long Friday*

Chapter 4, *Myth of Meritocracy*


We see that the term *woman* is the most associated with chapters 11, 10 and 3 which are respectively, *Yentl Syndrome*, *The Drugs Don't Work* and *The Long Friday*. 

Regarding the term *female*, even though it is the second most frequent term in the book, it seems to be more specific to chapter 14 *Women's Rights are Human Rights* since we only find it in this particular chapter [is this right?]. 

[Improve plot: color, layout, title, caption with source, can we change text1 by the name of the chapter ?]

-> MAYBE REMOVE WOMEN like in the Harry Potter example and explain why we remove it
-> I removed all terms like women, female etc... just for this plot, otherwise it is meaningless so we need to change the whole interpretation.  

-> justifier pq on fait pas l'autre plot avec les freq par doc. 


```{r cloud of words by document plot, echo=F, message=F, warning=F}

nowoman.10 <- data.dfm2 %>% 
           tidy() %>%  
           #group_by(term) %>% 
  # creates a tibble 
           top_n(10, count) %>%       # selects the top 10 rows by count   
           #ungroup() %>% 
           ggplot(aes(x = reorder(term, count), 
                      y = count)) +   
           geom_col(fill = "dodgerblue3", alpha = 0.5) + 
           coord_flip() + 
           xlab("Term") +
           ylab("Frequency") +
           facet_wrap(~ document, 
                      ncol = 2) +
           theme_minimal() +
           ggtitle("Top 10 frequencies along with documents")

nowoman.10


#---> Il y a un soucis ça plot pas les 10 termes les plus fréquents au travers de tous les docs mais juste les 10 fréquences les plus hautes par doc... Pas sûre d'être claire
```




## Zipf's Law

The Zipf's law shows the distribution of words used in a corpus compared to his rank in the frequency table . It says that the frequency of a token is inversely proportional to its rank.

The below plot is on a log-log scale, the frequency versus rank gives a negative linear relation as the original distribution is a negative exponential function. 

For example, the $log(f_women) \approx a + b*log(r_women) \approx ?$ "pas necessaire a mon avis ni le plot en dessous"

```{r fit, echo = FALSE, message = FALSE}
#fit_women <- lm() # [FINISH!!]

```

```{r Zipf law plot log-log, echo = FALSE, message = FALSE}

# Illustration of the Zipf's law 
ggplot(data.freq, aes(x = log(rank), y = log(frequency)))+ 
   geom_point()+
   geom_label(data = data.freq[c(1:5,6627:6631)], aes(label=feature)) +
   ggtitle("The Zipf's law on a log-log scale")
```

This plot shows that *women*, *female*, *datum* and *find* are the most frequent terms of the corpus indicating that they are not specific to one chapter but are frequent in all chapters. It means, according to the Zipf's law, that these words are very frequent in the corpus and could hide some meaningful information as they are not stop words.

## Term Frequency and Inverse Document Matrix (TF-IDF)

The TF-IDF matrix is a weighted document-feature matrix by term frequency-inverse document frequency (TF-IDF). In other words, it is used to rebalance a word frequency with respect to its document specificity.

The below output shows, for the document-feature matrix of 16 documents and 8,640 features, the weighted frequency for each token by document. Obviously, the sparsity stays the same.

```{r term frequency and inverse document matrix, echo = FALSE, message = FALSE}
# Weighted document-feature matrix by tf-idf
data.tfidf <- dfm_tfidf(data.dfm)
data.tfidf
```

The following output shows the twenty terms that have the largest TF-IDF. 

```{r largest tf-idf per word, echo = FALSE, message = FALSE}
# 20 largest TF-IDFs
sort(apply(data.tfidf,
           2,
           max),
     decreasing = TRUE)[1:20]

```

## TF-IDF by Document

The following plots show how some chapters are associated with certain words **using TF-IDF**. [Add interpretation when readable] 
plutot ca non?
The following plot shows the 10 highest TF-IDF compute by chapter. For example the term *tax* is really frequent and specific in chapter 13 whereas chapter 10 is associated with *trial* and *drug*.
```{r tfidf by document plot, echo=F, message=F, warning=F}

tfidf_plot <- data.tfidf2 %>% 
              tidy() %>%  
              top_n(10, count) %>% 
              ggplot(aes(x = term, 
                         y = count)) +
              geom_col() + 
              coord_flip() + 
              facet_wrap(~ document, 
                         ncol = 2)

tfidf_plot

```                

## Largest TF-IDF (over all documents) 

The below plot displays the terms that have at least one large TF-IDF . To do so, we compute for each term the maximum TF-IDF over all chapters of the book. The output shows that the term *tax* has a large TF-IDF (i.e. has the largest weighted frequency) in at least one chapter of the book.

[Improve plot]

-> words that occurr often but not in a lot of documents (i.e. specific to few or one chapter)

```{r max tf-idf, echo=F, message=F, warning=F}

tfidf_max <- data.tfidf %>% 
             tidy() %>%
             group_by(term) %>%
             summarize(count = max(count)) %>% 
             arrange(desc(count)) %>%
             top_n(10, count) %>%
             ggplot(aes(x = reorder(term, count), 
                        y = count)) + 
             geom_bar(stat = "identity") + 
             coord_flip() +
             labs(title = "Largest TF-IDF in the book",
                  subtitle = "",
                  x = "Max TF-IDF",
                  y = "Term",
                  caption = "source: Invisible Women.pdf")
             
tfidf_max

```   



## Keyness

The keyness measure is a chi-square test of independence indicating whether a term is characteristic of a target compared to a reference. 

We compute for each chapter of the book (i.e. text1 to text16) the keyness of terms. Each chapter (i.e. text[i]) is then at some point the target and the reference. 

Add interpretation

```{r keyness, echo=F, message=F, warning=F}

for (i in 1:nrow(data)){ # I don't know what variable to use because dfm is not a data frame ??
  
   data.keyness <- textstat_keyness(data.dfm, target =paste("text",i,sep="") )
   textplot_keyness(data.keyness)
  
}


```

## Link between words

Here, we look at how words are connected and how inter-connected they are (i.e. co-occurrences). We first compute the co-occurrences between terms. 

The feature co-occurrence matrix is a 8,640 by 8,640 matrix with 74,649,600 elements. Because of the size of the matrix, we reduce the matrix' size following a condition. Thus, the condition is to keep co-occurrences that occur more than eight times between two words. After filtering for co-occurrences greater than eight, we get a smaller feature co-occurrence matrix of dimensions 929 by 929 features. 

```{r co-occurrences, echo=F, message=F, warning=F}

# Creates a feature co-occurrence matrix

data.nowoman<-data.tk2%>%tokens_remove(c("women","female","woman"))

data.coocc <- fcm(data.nowoman,     
                  context = "document", 
                  tri = FALSE)                 # returns upper and lower triangles of the matrix

index <- data.freq[-c(1,2),] %>% 
         filter(frequency > 30) %>%  # filter for co-occurrences greater than 8
         data.frame() %>% 
         select(feature)

data.coocc <- data.coocc[index$feature, index$feature]

data.coocc

```

Using the above feature co-occurrences matrix, we generate a network (object). To generate a readable network of co-occurrences, we add a second condition on feature co-occurrences that is to keep only the strongest co-occurrences, greater than 10000. 

Describe what we see on the plot ? The generated network is very interesting. Indeed, it has a circular shape with no term at the center that would indicate that it, for example *women*, is associated with the others. In fact, it seems that even by keeping only feature co-occurrences above 1,000, the network's shape remains circular. 

[Improve plot layout]

```{r network, echo=F, message=F, warning=F}

library(igraph)




# Keep strongest co-occurrences
data.coocc[data.coocc <= 6500] <- 0
data.coocc[data.coocc > 6500] <- 1

# Creates a network from the co-occurrences matrix
network <- graph_from_adjacency_matrix(data.coocc, 
                                       mode = "undirected", 
                                       diag = FALSE)

plot(network, 
     layout = layout_with_kk)

```

## Lexical Diversity

Lexical diversity is a diversity index that measures the richness of the vocabulary in one document. 

### Token-Type Ratio (TTR) [Les chiffres ont changé]

The TTR is a diversity measure indicating a document's richness in the number of token types (i.e. vocabulary). The more types of tokens are found in a document, the richest is the vocabulary of this specific document. The closest to 1 is the TTR the richest is the vocabulary of a document. We need to be careful with the TTR measure because it is dependent on the length of the document. TTR is computed using the document-term matrix.

The below table sorts chapters of the book by descending TTR. According to TTR, chapter 15 *Who Will Rebuild?* has the richest vocabulary among all chapters of the book with a TTR of 0.622 and that chapter 3 *The Long Friday* has the poorest with a TTR of 0.457. Overall, the book seems to have a medium vocabulary richness since the TTR does not exceed 0.622. 

```{r TTR, echo=F, message=F, warning=F}

ttr <- data.dfm %>% 
       textstat_lexdiv()  # computes the lexical diversity

ttr <- ttr %>%
       arrange(desc(ttr$TTR))

library(kableExtra)
kbl(ttr, 
    align = c('c', 'c'), 
    caption = "Token-Type Ratio (TTR)") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

The below graph plots the above information.

[Improve plot]

```{r TTR plot, echo=F, message=F, warning=F}

ttr_plot <- ttr %>% 
            ggplot(aes(reorder(document, -TTR), TTR)) + 
            geom_bar(stat = "identity") +
            labs(title = "Token-Type Ration (TTR)",
                 subtitle = "Chapter 15 has the richest vocabulary",
                 x = "Chapter",
                 y = "TTR")

ttr_plot  # [ADD TTR ESTIMATES ON THE TIP OF THE BARS AND REMOVE PREVIOUS PLOT]

```

### Moving-Average Token-Type Ratio (MATTR)

Moving-Average Token-Type Ratio is an average of the Token-Type Ratio. It is an algorithm using windows of the text to compute the TTR and repeating several times over different windows of the same size the TTR computation. The advantage of the MATTR is that it is less dependent on the length of the document than TTR. A too large window can produce an error since no local TTR can be computed and a too small window results in pointless values (always 1). MATTR is computed using ordered tokens.

The Moving-Average Token-Type Ration ranges from 0.914 to 0.936 over all chapters of the book and according to the MATTR, chapter 6 *Being Worth Less Than a Show* has the richest vocabulary with a MATTR of 0.936. 

-> EXPLAIN WHY ALMOSTALL EQUAL TO 1 

```{r MATTR, echo=F, message=F, warning=F}

mattr <- data.tk2 %>% 
         textstat_lexdiv(measure = "MATTR", 
                         MATTR_window = 20)    # determines the size of the window to use 

mattr <- mattr %>%
         arrange(desc(mattr$MATTR))

kbl(mattr, 
    align = c('c', 'c'), 
    caption = "Moving-Average Token-Type Ratio (MATTR)") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

The below graph plots the above information and shows that the MATTR among chapters are very similar. 

[Improve plot]

```{r MATTR plot, echo=F, message=F, warning=F}

mattr_plot <- mattr %>% 
              ggplot(aes(reorder(document, -MATTR), MATTR)) + 
              geom_bar(stat = "identity") +
              labs(title = "Moving-Average Token-Type Ratio (MATTR)",
                   subtitle = "",
                   x = "Chapter",
                   y = "MATTR")

mattr_plot

```


## Dispersion Plot

Dispersion or X-Ray plots inspect where a specific token is used in each text by locating a pattern in each text. 

The below table shows how in chapter 1 (i.e. text1) the word *women* are used. 

```{r pattern location, echo=F, message=F, warning=F}

# Checks the result
head(kwic(data.cp, pattern = "women")) 

kbl(head(kwic(data.cp, 
              pattern = "women")), 
    caption = "Pattern Location") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

```{r xray plot, echo=F, message=F, warning=F}

# Plot
textplot_xray(kwic(data.cp, 
                   pattern = c("women")))

```

We can extend to two patterns. The below lexical dispersion plot shows that the term *sex* is more specific to chapter 10 and that *women* and *sex* are not necessarily associated. 

-> ADD TITLE OF TEXT 10 AND EXPLAIN WH THESE TWO TOKENS WOULD APPEAR MORE OFTEN TOGETHER
-> comparison female and male terms 

```{r xray plot 2 patterns, echo=F, message=F, warning=F}

# Plot
textplot_xray(kwic(data.cp, pattern = c("women")),
              kwic(data.cp, pattern = c("sex")),
              scale = "absolute")

# intéressant de comparer female et male ?? 
```

