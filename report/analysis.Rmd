# Analysis

```{r, echo=F, message=F, warning=F}
source(here::here("scripts/setup.R"))
```

* Answers to the research questions

1) What does the text mining analysis of this book brings to light ?
a) general vocabulary analysis + chapter vocabulary analysis + part vocabulary analysis
b) overall token analysis + chapter token analysis + part token analysis
...

* Different methods considered

So far only **quanteda** package used, let's see if we can try with another one and explains differences and why we chose them.

* Competing approaches

Explain why we use the **quanteda** method (which is very similar to **tm**, explain how) rather than **tidytext** method 

* Justifications

## Sentiment Analysis

The sentiment analysis qualifies or quantifies the sentiment emerging from one text. To proceed to the sentiment analysis, we use two approaches. The first one uses qualifiers (i.e. dictionary-based) and the second one uses values (i.e. value-based). 

When analyzing the sentiment, we should not use the data in which stop words were removed since they might be in the sentiment dictionary used or created. 

```{r data.tk1, echo=F, message=F, warning=F}

# Not removing the stop words
data.tk3 <- data.tk1 %>%
            tokens_tolower() %>%
            tokens_replace(pattern = hash_lemmas$token, 
                              replacement = hash_lemmas$lemma)

```

### Dictionary-Based 

The dictionary-based sentiment analysis matches tokens from each document to a reference dictionary with token values and then look for word polarity. The sentiment is the average over token values of the document. 

The disadvantage of the dictionary-based sentiment analysis is that the negative forms are not taken into consideration. For example, in the sentence *I don't enjoy the show*, the sentence will be considered positive because it will not consider the contraction *don't* but only the word *enjoy*.

The following table shows the average sentiment over the token values of each chapter of the book. The possible sentiment can be **positive**, **negative**, **neg_positive** or **neg_negative**.

```{r dictionary-based analysis, echo=F, message=F, warning=F}

# Dictionary used
# data_dictionary_LSD2015 # un-comment to see the object

# Matches tokens with dictionary values
data.sent <- tokens_lookup(data.tk3, 
                           dictionary = data_dictionary_LSD2015) %>% 
             dfm() %>%                                                # creates a dtm matrix
             tidy()

kbl(data.sent, 
    caption = "Sentiment Analysis (Dictionary-Based)") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

The graph is a graphical representation of the above table. For each chapter of the book, we see the proportion of each type of sentiments. 

```{r dictionary-based analysis plot, echo=F, message=F, warning=F}

# Plots the above tibble
ggplot(data.sent,
       aes(x = reorder(document, count), 
           y = count, 
           fill = term)) + 
  geom_bar(stat = "identity") + 
  coord_flip() + 
  labs(title = "Dictionary-Based Sentiment Analysis",
       x = "Count",
       y = "Chapter (i.e. document)")

```

### Valence Shifters 

The valence shifters approach uses positive/negative sentiment scores (i.e. value-based) to extract the sentiment of a document. Here, we use two dictionaries, a polarized words dicitonary and a valence-shifters one. 

The next table shows the first ten words of the polarized words dictionary, displaying the words and their numerical value.

```{r polarized words dictionary, echo=F, message=F, warning=F}

library(sentimentr)

# Polarized words dictionary
kbl(head(hash_sentiment_jockers_rinker, 10), 
    caption = "Polarized Words Dictionary") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

The next table shows the first ten words of the valence-shifters dictionary, displaying the words and their numerical value.

```{r valence-shifters dictionary, echo=F, message=F, warning=F}

# Valence-shifters dictionary
kbl(head(hash_valence_shifters, 10), 
    caption = "Valence-Shifters Dictionary") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

To proceed to the value-shifters sentiment analysis, we first extract the sentences from the text and then we compute the sentiment value for each sentence. Because the column *word_count* had NAs we remove rows that show NAs since no sentiment information can be provided (i.e. sentiment = 0).

One possibility is to assign weights to certain types of sentences to 0 which unpolarizes them. For example, we could have decided to unpolarize the questions but here we decided to keep them as we believe they could be insightful. 

```{r sentiment analysis, echo=F, message=F, warning=F}

# Extracts sentences from the text
text_sentences <- get_sentences(text)

# Computes sentiment by sentence
sent_bysent <- sentiment(text_sentences) %>%   # assigns a polarity scores
               filter(word_count != "NA")

kbl(head(sent_bysent, 10), 
    caption = "Sentiment Values by Sentence") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

After taking a look at the sentiment score for each sentence, we zoom out to look into the sentiment score by chapter of the book. The *ave_sentiment* column gives the average sentiment (i.e) score by chapter. 

[Remove text form the document column and keep only the chapter number] The below table displays the sentiment scores from largest to smallest for the chapters of the book. According to the table, chapter 6 *Being Worth Less Than A Shoe* has the greatest average sentiment score with 0.275 which is consider positive and chapter 7 *The Plough Hypothesis* has the smallest score with -0.3953 which is consider negative. 

In total, five chapters have a positive sentiment (i.e. scores above 0.05), four have a negative sentiment (i.e. scores below -0.05) and seven chapters have a neutral sentiment (i.e. scores between -0.05 and 0.05). 

```{r sentiment analysis by document, echo=F, message=F, warning=F}

# Computes the sentiment score for each chapter
sent_bychap <- sentiment_by(text,
                            by = document) %>%
               arrange(desc(ave_sentiment)) %>%
               filter(document != "NA")

kbl(sent_bychap, 
    caption = "Sentiment Values by Chapter") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) 

```

Sentiment changes as sentences changes. Thus, in order to better track the evolution of sentiment we plot the sentiment scores' evolution by chapter of the book (i.e. document). 

[Improve plot: remove text from document column so x is readible/layout]

```{r sentiment analysis by document plot, echo=F, message=F, warning=F}

# Plots the sentiment curve by document
sent_bychap %>% 
  ggplot(aes(x = document, y = ave_sentiment)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  labs(title = "Sentiment Score By Chapter",
       x = "Sentiment Score",
       y = "Chapter")

```

## Similarities and Clustering

Similarity is a proximity measure. Once can measure similarity between terms to see if they are used in the same context or similarity between documents and see whether a document uses the same tokens. 

### Similarities between Documents

#### Jaccard Index

To study similarities between chapters (i.e. documents) of the book, we first compute the Jaccard index matrix displaying the relative number of common words using the TF-IDF matrix. 

```{r Jaccard, echo=F, message=F, warning=F}

# Computes similarities and distances between documents
data.jac <- textstat_simil(data.tfidf,                  # uses the frequencies
                           method = "jaccard", 
                           margin = "documents")        # between documents

library(reshape2)
library(ggplot2)

# Converts the object to matrix then to data frame
data.jac.mat <- melt(as.matrix(data.jac)) 

# Plots
ggplot(data = data.jac.mat, 
       aes(x = Var1, 
           y = Var2, 
           fill = value)) +
  scale_fill_gradient2(low = "blue", 
                       high = "red", 
                       mid = "white",
                       midpoint = 0.5, 
                       limit = c(0,1), 
                       name="Jaccard") +
  geom_tile() +
  labs(title = "Heatmap Representation Of Similarities Between Documents",
       x = "Chapter",
       y = "Chapter")

```

Then, we compute the cosine similarity matrix. 

```{r Cosine, echo=F, message=F, warning=F}

# Cosine
data.cos <- textstat_simil(data.tfidf,                  # uses frequencies
                           method = "cosine",           # cosine
                           margin = "documents")   

# Converts the object to matrix then to data frame
data.cos.mat <- melt(as.matrix(data.cos))

# Plot
ggplot(data = data.cos.mat, 
       aes(x = Var1, 
           y = Var2, 
           fill = value)) +
  scale_fill_gradient2(low = "blue", 
                       high = "red", 
                       mid = "white",
                       midpoint = 0.5, 
                       limit = c(0,1), 
                       name="Cosine") +
  geom_tile() +
  labs(title = "Heatmap Representation Of Similarities Between Documents",
       x = "Chapter",
       y = "Chapter")

```

Finally, we compute the Euclidean-based similarity matrix. Is there a clearer separation between documents ? compare with above heatmaps ! Which documents are similar ? Are their groups ? 

```{r Euclidean, echo=F, message=F, warning=F}

# Euclidean
data.euc <- textstat_dist(data.tfidf,                   # uses the frequencies
                          method = "euclidean",         # Euclidean distances
                          margin = "documents")

# Converts the object to matrix then to data frame
data.euc.mat <- melt(as.matrix(data.euc))

# Maximum distance
M <- max(data.euc.mat$value) 

# Converts from distance to similarity in [0,1]
data.euc.mat$value.std <- (M - data.euc.mat$value)/M 

# Plots
ggplot(data = data.euc.mat, 
       aes(x = Var1, 
           y = Var2, 
           fill = value.std)) +
  scale_fill_gradient2(low = "blue", 
                       high = "red", 
                       mid = "white",
                       midpoint = 0.5, 
                       limit = c(0,1), 
                       name = "Euclidean") +
  geom_tile() +
  labs(title = "Heatmap Representation Of Similarities Between Documents",
       x = "Chapter",
       y = "Chapter")

```

### Clustering of Documents

To procede to clustering documents, we need to build the dissimilarities and/or vector space model (VSM) on which we can apply the clustering methods. There are two approaches to clustering. One is based on distances (i.e. hierarchical clustering) and the other one is based on features co-occurrences? (i.e. partitioning). 

Clusters are difficult to interpret. This is why we can look at the most frequent terms in each cluster to understand better what they are grouping.

#### Hierarchical Clustering

Hierarchical clustering is based on distances and applied on the dissimilarities.

##### Euclidean-based Dissimilarity Matrix

The Euclidean dissimilarity matrix seems to show that there are three clusters. One grouping chapter 10 *The Drugs Don't Work*, one chapter 9 *A Sea Of Dudes* and the other one the rest og the book. 

```{r Euclidean-based, echo=F, message=F, warning=F}

# Hierarchical clustering
data.hc_eu <- hclust(as.dist(data.euc))

plot(data.hc_eu)

```

From the clustering, we extract the ten words that are the most used. 

```{r Euclidean-based cutree, echo=F, message=F, warning=F}

# Creates three clusters
data.clust_eu <- cutree(data.hc_eu, 
                        k = 3)
data.clust_eu 

# Extract most frequent terms from clusters
clust_eu <- data.frame(Clust.1 = names(sort(apply(data.tfidf[data.clust_eu == 1,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.2 = names(sort(apply(data.tfidf[data.clust_eu == 2,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.3 = names(sort(apply(data.tfidf[data.clust_eu == 3,], 2, sum), 
                                            decreasing = TRUE)[1:10]))

kbl(clust_eu, 
    caption = "Ten Most Frequent Terms By Cluster") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

##### Inverted Jaccard Dissimilarity Matrix

The inverted Jaccard dissimilarity matrix seems to show that there are two clusters. One grouping chapter 15 *Who Will Rebuild* and the other one grouping the rest of the book. 

```{r Jaccard inverted, echo=F, message=F, warning=F}

# Hierarchical clustering
data.hc_jac <- hclust(as.dist(1 - data.jac)) # inverted similarity matrix

plot(data.hc_jac)

```

From the clustering, we extract the ten words that are the most used. 

```{r Jaccard cutree, echo=F, message=F, warning=F}

# Creates three clusters
data.clust_jac <- cutree(data.hc_jac, 
                         k = 3)
data.clust_jac

# Extract most frequent terms from clusters
clust_jac <- data.frame(Clust.1 = names(sort(apply(data.tfidf[data.clust_jac == 1,], 2, sum), 
                                             decreasing = TRUE)[1:10]),
                       Clust.2 = names(sort(apply(data.tfidf[data.clust_jac == 2,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.3 = names(sort(apply(data.tfidf[data.clust_jac == 3,], 2, sum), 
                                            decreasing = TRUE)[1:10]))

kbl(clust_jac, 
    caption = "Ten Most Frequent Terms By Cluster") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

##### Inverted Cosine Dissimilarity Matrix

The inverted cosine dissimilarity matrix seems to show that there are two clusters. One grouping chapters 4, 14, 5, 6, 10, 11 and 8, 9 and the other one grouping chapters 1, 2, 15, 16, 7, 13, 3 and 12.

```{r cosine inverted, echo=F, message=F, warning=F}

# Hierarchical clustering
data.hc_cos <- hclust(as.dist(1 - data.cos)) # inverted similarity matrix

plot(data.hc_cos)

```

From the clustering, we extract the ten words that are the most used. 

```{r cosine cutree, echo=F, message=F, warning=F}

# Creates three clusters
data.clust_cos <- cutree(data.hc_cos, 
                         k = 3)
data.clust_cos

# Extract most frequent terms from clusters
clust_cos <- data.frame(Clust.1 = names(sort(apply(data.tfidf[data.clust_cos == 1,], 2, sum), 
                                             decreasing = TRUE)[1:10]),
                       Clust.2 = names(sort(apply(data.tfidf[data.clust_cos == 2,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.3 = names(sort(apply(data.tfidf[data.clust_cos == 3,], 2, sum), 
                                            decreasing = TRUE)[1:10]))

kbl(clust_cos, 
    caption = "Ten Most Frequent Terms By Cluster") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

#### K-Means

K-means is applied on the features (i.e. terms?) using TF-IDF. 

##### Euclidean

```{r Euclidean-based km, echo=F, message=F, warning=F}

# Creates three clusters
data.km_eu <- kmeans(data.tfidf,
                     centers = 3)
data.km_eu$cluster

# Extract most frequent terms from clusters
clust_eu_km <- data.frame(Clust.1 = names(sort(apply(data.tfidf[data.km_eu$cluster == 1,], 2, sum), 
                                               decreasing = TRUE)[1:10]),
                          Clust.2 = names(sort(apply(data.tfidf[data.km_eu$cluster == 2,], 2, sum),                                                    decreasing = TRUE)[1:10]),
                          Clust.3 = names(sort(apply(data.tfidf[data.km_eu$cluster == 3,], 2, sum),  
                                               decreasing = TRUE)[1:10]))

kbl(clust_eu_km, 
    caption = "Ten Most Frequent Terms By Cluster") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

##### Jaccard

```{r Jaccard km, echo=F, message=F, warning=F}

# Creates three clusters
data.km_jac <- kmeans(data.tfidf,
                      centers = 3)
data.km_jac$cluster

# Extract most frequent terms from clusters
clust_jac_km <- data.frame(Clust.1 = names(sort(apply(data.tfidf[data.km_jac$cluster == 1,], 2, sum), 
                                                decreasing = TRUE)[1:10]),
                           Clust.2 = names(sort(apply(data.tfidf[data.km_jac$cluster == 2,], 2, sum),                                                   decreasing = TRUE)[1:10]),
                           Clust.3 = names(sort(apply(data.tfidf[data.km_jac$cluster == 3,], 2, sum),  
                                                decreasing = TRUE)[1:10]))

kbl(clust_jac_km, 
    caption = "Ten Most Frequent Terms By Cluster") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

##### Cosine

```{r cosine km, echo=F, message=F, warning=F}

# Creates three clusters
data.km_cos <- kmeans(data.tfidf,
                      centers = 3)
data.km_cos$cluster

# Extract most frequent terms from clusters
clust_cos_km <- data.frame(Clust.1 = names(sort(apply(data.tfidf[data.km_cos$cluster == 1,], 2, sum), 
                                                decreasing = TRUE)[1:10]),
                           Clust.2 = names(sort(apply(data.tfidf[data.km_cos$cluster == 2,], 2, sum),                                                   decreasing = TRUE)[1:10]),
                           Clust.3 = names(sort(apply(data.tfidf[data.km_cos$cluster == 3,], 2, sum),  
                                                decreasing = TRUE)[1:10]))

kbl(clust_cos_km, 
    caption = "Ten Most Frequent Terms By Cluster") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

### Similarities Between Words

Now we analyze similarities between words through chapters (i.e. documents). Because of the large number of words, we focus on word frequency ranks smaller or equal to 40 (i.e. 40 most frequent words).

#### Cosine

What words are similar ? It means they are used in similar proportion through documents.

```{r euclidean similarities btwn words, echo=F, message=F, warning=F}

# Extracts the features
data.feat <- textstat_frequency(data.tf) %>% 
  filter(rank <= 40)

data.feat$feature

data.cos <- textstat_simil(data.tf[, data.feat$feature], 
                           method = "cosine", 
                           margin = "feature")

# Converts the object to matrix then to data frame
data.cos.mat <- melt(as.matrix(data.cos)) 

# Plots
ggplot(data = data.cos.mat, 
       aes(x = Var1, 
           y = Var2,  
           fill = value)) +
  scale_fill_gradient2(low = "blue", 
                       high = "red", 
                       mid = "white",
                       midpoint = 0.5, 
                       limit = c(0,1), 
                       name = "Cosine") +
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 45, 
                                   hjust = 1))


```

### Clustering Words

To cluster the words, we first have to transform the similarity matrix into a dissimilarity matrix. The following plot is the dendogram of the cosine dissimilarities.

```{r clustering words, echo=F, message=F, warning=F}

data.cos2 <- textstat_simil(data.tfidf, 
                            method = "cosine", 
                            margin = "features")

data.hc <- hclust(as.dist(1 - data.cos2))
plot(data.hc)

#PROBLEM C!!!!

```

### Clustering With Co-occurrences

Using co-occurrences to cluster features requires transforming co-occurrences into dissimilarities. Object needs to be the tokens because co-occurrences needs the token order and thus cannot be computed on a BOW object. 

Are terms co-occurring ? if yes, a lot ? 

```{r clustering with co-occurrences, echo=F, message=F, warning=F}

# Makes a co-occurrence on a (half-)window of 3
data.fcm <- fcm(data.tk, 
                window = 3, 
                tri = FALSE)

# Makes the co-occurrence matrix symmetrical
data.fcm <- (data.fcm + t(data.fcm))/2

# Makes a matrix
data.fcm.mat <- melt(as.matrix(data.fcm[data.feat$feature, 
                                        data.feat$feature]), 
                     varnames = c("Var1","Var2"))

# Makes a heatmap with most frequent features
ggplot(data = data.fcm.mat, 
       aes(x = Var1, 
           y = Var2, 
           fill = value)) +
  scale_fill_gradient2(low = "blue", 
                       high = "red", 
                       mid = "white",
                       midpoint = 140, 
                       limit = c(0,280), 
                       name = "Co-occurrence") +
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r clustering analysis, echo=F, message=F, warning=F}

data.inv_occ <- 280 - as.matrix(data.fcm[data.feat$feature, 
                                         data.feat$feature]) # 280 is the max co-occurrence here

data.hc <- hclust(as.dist(data.inv_occ))

plot(cdata.hc)

```
