# 4. Analysis

This section dives deeper into the analysis of the content of the corpus. Each part of our analysis is supported by well-designed relevant charts and graphs using the **ggplot2**, **sentimentr**, **reshape2**, **quanteda.textmodels**, **seededlda** and **text2vec** packages [VERIFY IF ALL THESE PACKAGES PROVIDE OR HELP PROVIDE A GRAPHICAL REPRESENTATION].

-> talk about the complexity of the book: specify the length of the text before and after the cleaning ? assess the difference (big or small)  and interpret (if big, most words are bringing no information, if small, most terms are insightful) ? [DO YOU HAVE ANY OTHER IDEAS THAT COULD ASSESS THE COMPLEXITY OF THE TEXT ?]

-> talk about the uniqueness of the data : [NOT SURE WHAT TO MENTION HERE BUT MUST BE MENTIONNED]


## Sentiment Analysis 

Now that the content of the corpus is cleaned and that we have explored its content, we proceed to its sentiment analysis (i.e. opinion mining) which qualifies or quantifies the sentiment emerging from one text (i.e. chapter). To proceed to the sentiment analysis, we use two approaches. The first one uses qualifiers (i.e. dictionary-based) and the second one uses values (i.e. value-based).  

When analyzing the sentiment emerging from a document, we do not use the data in which stop words are removed since they might be in the sentiment dictionary and provide useful insight. 

```{r data.tk4, echo=F, message=F, warning=F}

# Tokenize = create a tokens object
#data.tk1 <- tokens(data.cp,
                   #remove_numbers = FALSE,
                   #remove_punct = TRUE,
                   #remove_symbols = TRUE,
                   #remove_separators = TRUE)

# Not removing the stop words
data.tk4 <- data.tk1 %>%
            tokens_tolower() %>%
            tokens_replace(pattern = hash_lemmas$token, 
                           replacement = hash_lemmas$lemma)

```

### Dictionary-Based 

The dictionary-based sentiment analysis matches tokens from each document to a reference dictionary with token values and then, look for word polarity. The sentiment is the average over token values of the document. 

The disadvantage of the dictionary-based sentiment analysis is that the negative forms are not taken into consideration. For example, in the sentence *I don't enjoy the show*, the sentence will be considered positive because it will not consider the contraction *don't* but only the word *enjoy*.

The following non-exhaustive table shows the number of terms matched with a **positive**, **negative**, **neg_positive** or **neg_negative** sentiment for each document (i.e. chapter). For example, in chapter one, 142 terms are matched with a negative sentiment and 167 are found matched with a positive sentiment. 

```{r dictionary-based analysis, message=FALSE, warning=FALSE, include=FALSE}

# Dictionary used
# data_dictionary_LSD2015 # un-comment to see the object

# Matches tokens with dictionary values
data.sent <- tokens_lookup(data.tk4, 
                           dictionary = data_dictionary_LSD2015) %>% 
             dfm() %>%     # creates a dtm matrix
             tidy() %>%
             pivot_wider(names_from = term, values_from = count) %>%
             select(!c(`neg_positive`,`neg_negative`))

kbl(data.sent,
    caption = "Sentiment Analysis (Dictionary-Based)") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) %>%
  scroll_box(height = "20%")

```

The below graph is a representation of the previous table. For each chapter of the book, we see the proportion of terms matched with a positive, negative, negative positive or negative negative sentiment. 

Overall, positive and negative sentiments are found in all 16 chapters of the book. Although, more terms are recognized as negative (3,596) than as positive (2,661) indicating that the frequency of negative terms is higher than the one of positive terms. 

```{r dictionary-based analysis plot2, echo=F, message=F, warning=F}

data.sent <- tokens_lookup(data.tk4, 
                           dictionary = data_dictionary_LSD2015) %>% 
             dfm() %>%     # creates a dtm matrix
             tidy() %>%
            filter(term != "neg_positive" & term != "neg_negative")

# Plots the above tibble
ggplotly(ggplot(data.sent,
       aes(x = reorder(document, count), 
           y = count, 
           fill = term, 
           text = paste('</br><b>Chapter:</b> ', document,
                              '</br><b>Count:</b> ', count))) + 
  geom_bar(stat = "identity") + 
  coord_flip() + 
  labs(title = "Dictionary-Based Sentiment Analysis",
       x = "Chapter (i.e. document)",
       y = "Count",
       fill = "Sentiment"),tooltip = "text")

neg <- data.sent %>%
       filter(term == "negative")

neg_count <- sum(neg$count) # 3,596

pos <- data.sent %>%
       filter(term == "positive")

pos_count <- sum(pos$count) # 2,661


```

### Valence Shifters {.tabset}

The valence shifters approach uses positive/negative sentiment scores (i.e. value-based) to extract the sentiment of a document. Here, we use two dictionaries, a polarized words dictionary where we find a list of terms communicating a positive or negative attitude and a valence-shifters dictionary which provides terms that alter or intensify the meaning of the polarized words. 

#### Polarized words dictionary

The next table shows the first ten words of the polarized words dictionary and their numerical score.

```{r polarized words dictionary, echo=F, message=F, warning=F}

# Polarized words dictionary
hash_sentiment_jockers_rinker %>% 
  rename(token = x, value = y) %>% 
  head(5) %>% 
  kbl(caption = "Polarized Words Dictionary") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

#### Valence-shifters dictionary

The next table shows the first ten words of the valence-shifters dictionary and their numerical score.

```{r valence-shifters dictionary, echo=F, message=F, warning=F}

# Valence-shifters dictionary
hash_valence_shifters %>% 
  rename(token = x, value = y) %>% 
  head(5) %>% 
  kbl(caption = "Valence-Shifters Dictionary") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

To proceed to the valance-shifters sentiment analysis, we extract the sentences from the text and we compute the sentiment value for each sentence. Because the column *word_count* had NAs, we remove rows that have no available information since no sentiment can be extracted. Moreover, we do not assign weights to certain types of sentences (e.g. questions) since we believe that the sentence type does not have a particular influence in our analysis.

The below output displays the sentiment value of the first ten sentences of the corpus and indicates the number of terms for each. Anything below -0.05 is considered negative and anything above 0.05 is considered positive. Anything in between is considered neutral. For example, the first sentence is negative (-0.408), the seventh sentence is neutral (0) and the eigth sentence is positive (0.332). 

```{r sentiment analysis, echo=F, message=F, warning=F}

# Extracts sentences from the text
text_sentences <- get_sentences(text)

# Computes sentiment by sentence
sent_bysent <- sentiment(text_sentences) %>%   # assigns a polarity scores
               filter(word_count != "NA") %>% 
               mutate(document = paste("Chapter", element_id, sep = " ")) %>% 
  relocate(document, .before = element_id) %>% 
  select(!(element_id))

kbl(head(sent_bysent, 5), 
    caption = "Sentiment Values by Sentence") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

After taking a look at the sentiment score for each sentence, we zoom out to look into the sentiment score by chapter of the book. The *ave_sentiment* column gives the average sentiment (i.e) score by chapter. 

The below table displays the average sentiment scores by chapter in a decreasing fashion. According to the table, chapter 4 *The Myth of Meritocracy* has the greatest positive average. In total, five chapters have a positive sentiment (i.e. scores above 0.05), four have a negative sentiment (i.e. scores below -0.05) and seven chapters have a neutral sentiment (i.e. scores between -0.05 and 0.05).  -> CORRECT ONCE PROBLEM FIXED 

[PROBLEM IL NY A PAS TOUS LES CHAPITRES ET LES CHAPITRES APPARAIISSENT PLUSIEURS FOIS]

[J'ai mis un tooltip avec toutes les infos dans le prochain plot pour pouvoir potentiellement enlever ce tableau]

```{r sentiment analysis by document, echo=F, message=FALSE, warning=FALSE}

# Computes the sentiment score for each chapter
sent_bychap <- sentiment_by(text) %>%
               arrange(element_id) %>%
               mutate(document = paste("Chapter", element_id, sep = " ")) %>% 
  relocate(document, .before = element_id) %>% 
  select(!(element_id))

sent_bychap %>% arrange(desc(ave_sentiment))  %>% 
    kbl(caption = "Sentiment Values by Chapter") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) 

```

Sentiment changes as sentences change. Thus, in order to better track the evolution of the sentiment, we plot the sentiment scores' evolution by chapter of the book (i.e. by document). 

The below plot shows the range of sentiment scores by chapter indicating how much sentiment evolves throughout the sentences.

PROBLEM NOT ALL CHAPTERS ARE ON THE PLOT 

```{r sentiment analysis by document plot, echo=F, message=F, warning=F}

# Plots the sentiment curve by document
#sent_bychap %>% 
ggplotly(
  ggplot(sent_bychap, 
         aes(x = reorder(document,ave_sentiment),
             y = ave_sentiment, 
             text = paste('</br><b>Document:</b> ', document,
                          '</br><b>Word count:</b> ', word_count,
                          '</br><b>Std. deviation:</b> ', sd)
             )) + 
  geom_bar(stat = "identity", fill = "dodgerblue3", alpha = 0.5) + 
  coord_flip() +
  labs(title = "Sentiment Score by Chapter",
                   x = "Document",
                   y = "Average sentiment",
                   caption = "source: Invisible Women.pdf") +
  theme_minimal(),
  tooltip = "text"
)

```

## Similarities and Clustering

Similarity is a proximity measure. Once can measure similarity between terms to see if they are used in the same context or similarity between documents and see whether a document uses the same tokens. 

### Similarities between Documents {.tabset}

#### Jaccard Index

To study similarities between chapters (i.e. documents) of the book, we first compute the Jaccard index matrix displaying the relative number of common words using the TF-IDF matrix. 

```{r Jaccard, echo=F, message=F, warning=F}

# Computes similarities and distances between documents
data.jac <- textstat_simil(data.tfidf,                  # uses the frequencies
                           method = "jaccard", 
                           margin = "documents")        # between documents

# Converts the object to matrix then to data frame
data.jac.mat <- melt(as.matrix(data.jac)) 

# Plots
ggplot(data = data.jac.mat, 
       aes(x = Var1, 
           y = Var2, 
           fill = value)) +
  scale_fill_gradient2(low = "blue", 
                       high = "red", 
                       mid = "white",
                       midpoint = 0.5, 
                       limit = c(0,1), 
                       name="Jaccard") +
  geom_tile() +
  labs(title = "Heatmap Representation Of Similarities Between Documents",
       x = "Chapter",
       y = "Chapter")+
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 22))
```

#### Cosine

Then, we compute the cosine similarity matrix. 

```{r Cosine, echo=F, message=F, warning=F}

# Cosine
data.cos <- textstat_simil(data.tfidf,                  # uses frequencies
                           method = "cosine",           # cosine
                           margin = "documents")   

# Converts the object to matrix then to data frame
data.cos.mat <- melt(as.matrix(data.cos))

# Plot
ggplot(data = data.cos.mat, 
       aes(x = Var1, 
           y = Var2, 
           fill = value)) +
  scale_fill_gradient2(low = "blue", 
                       high = "red", 
                       mid = "white",
                       midpoint = 0.5, 
                       limit = c(0,1), 
                       name="Cosine") +
  geom_tile() +
  labs(title = "Heatmap Representation Of Similarities Between Documents",
       x = "Chapter",
       y = "Chapter")+
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 22))

```

#### Euclidean similarity 

Finally, we compute the Euclidean-based similarity matrix. Is there a clearer separation between documents ? compare with above heatmaps ! Which documents are similar ? Are their groups ? 
 
```{r Euclidean, echo=F, message=F, warning=F}

# Euclidean
data.euc <- textstat_dist(data.tfidf,                   # uses the frequencies
                          method = "euclidean",         # Euclidean distances
                          margin = "documents")

# Converts the object to matrix then to data frame
data.euc.mat <- melt(as.matrix(data.euc))

# Maximum distance
M <- max(data.euc.mat$value) 

# Converts from distance to similarity in [0,1]
data.euc.mat$value.std <- (M - data.euc.mat$value)/M 

# Plots
ggplot(data = data.euc.mat, 
       aes(x = Var1, 
           y = Var2, 
           fill = value.std)) +
  scale_fill_gradient2(low = "blue", 
                       high = "red", 
                       mid = "white",
                       midpoint = 0.5, 
                       limit = c(0,1), 
                       name = "Euclidean") +
  geom_tile() +
  labs(title = "Heatmap Representation Of Similarities Between Documents",
       x = "Chapter",
       y = "Chapter")+
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 22))

```

### Clustering of Documents

To proceed to clustering documents, we need to build the dissimilarities and/or vector space model (VSM) on which we can apply the clustering methods. There are two approaches to clustering. One is based on distances (i.e. hierarchical clustering) and the other one is based on features co-occurrences? (i.e. partitioning). 

Clusters are difficult to interpret. This is why we can look at the most frequent terms in each cluster to understand better what they are grouping.

#### Hierarchical Clustering {.tabset}

Hierarchical clustering is based on distances and applied on the dissimilarities.

##### Inverted Jaccard Dissimilarity Matrix

The inverted Jaccard dissimilarity matrix seems to show that there are two clusters. One grouping chapter 15 *Who Will Rebuild* and the other one grouping the rest of the book. 

```{r Jaccard inverted, echo=F, message=F, warning=F}

# Hierarchical clustering
data.hc_jac <- hclust(as.dist(1 - data.jac)) # inverted similarity matrix

plot(data.hc_jac)

```



```{r Jaccard cutree, echo=F, message=F, warning=F}

# Creates three clusters
data.clust_jac <- cutree(data.hc_jac, 
                         k = 6)
data.clust_jac %>% 
  as.tibble() %>% 
  mutate(document = as.factor(names(data.clust_jac)), 
         cluster = value) %>% 
  select(!(value)) %>% 
  arrange(cluster) %>% 
  #pivot_wider(names_from = cluster, names_prefix = "cluster", values_from = document) %>% 
  kbl(caption = "Clusters using Jaccard") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) %>% 
  scroll_box(height = "10%")

```

From the clustering, we extract the ten words that are the most used. 

```{r Jaccard clust words, echo=F, message=F, warning=F}
# Extract most frequent terms from clusters
clust_jac <- data.frame(Clust.1 = names(sort(apply(data.tfidf[data.clust_jac == 1,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.2 = names(sort(apply(data.tfidf[data.clust_jac == 2,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.3 = names(sort(apply(data.tfidf[data.clust_jac == 3,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.4 = names(sort(apply(data.tfidf[data.clust_jac == 4,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.5 = names(sort(apply(data.tfidf[data.clust_jac == 5,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.6 = names(sort(apply(data.tfidf[data.clust_jac == 6,], 2, sum), 
                                            decreasing = TRUE)[1:10]))

kbl(clust_jac, 
    caption = "Ten Most Frequent Terms By Cluster") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) %>%
  scroll_box(height = "5%")

```
Using jaccard we still have two cluster that have terms the same previous previous clustering using euclidean distance : transport and medical, but the cluster 3 is different with top terms related to disaster or war, rather than top terms related to cars.  


##### Inverted Cosine Dissimilarity Matrix

The inverted cosine dissimilarity matrix seems to show that there are two clusters. One grouping chapters 4, 14, 5, 6, 10, 11 and 8, 9 and the other one grouping chapters 1, 2, 15, 16, 7, 13, 3 and 12.

```{r cosine inverted, echo=F, message=F, warning=F}

# Hierarchical clustering
data.hc_cos <- hclust(as.dist(1 - data.cos)) # inverted similarity matrix

plot(data.hc_cos)

```

 

```{r cosine cutree, echo=F, message=F, warning=F}

# Creates three clusters
data.clust_cos <- cutree(data.hc_cos, 
                         k = 6)
data.clust_cos %>% 
  as.tibble() %>% 
  mutate(document = as.factor(names(data.clust_jac)), 
         cluster = value) %>% 
  select(!(value)) %>% 
  arrange(cluster) %>% 
  #pivot_wider(names_from = cluster, names_prefix = "cluster", values_from = document) %>% 
  kbl(caption = "Clusters using Cosine") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) %>% 
  scroll_box(height = "10%")

```


From the clustering, we extract the ten words that are the most used.

```{r cosine clust words, echo=F, message=F, warning=F}
# Extract most frequent terms from clusters
clust_cos <- data.frame(Clust.1 = names(sort(apply(data.tfidf[data.clust_cos == 1,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.2 = names(sort(apply(data.tfidf[data.clust_cos == 2,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.3 = names(sort(apply(data.tfidf[data.clust_cos == 3,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.4 = names(sort(apply(data.tfidf[data.clust_cos == 4,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.5 = names(sort(apply(data.tfidf[data.clust_cos == 5,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.6 = names(sort(apply(data.tfidf[data.clust_cos == 6,], 2, sum), 
                                            decreasing = TRUE)[1:10]))
kbl(clust_cos, 
    caption = "Ten Most Frequent Terms By Cluster") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) %>% 
  scroll_box(height = "10%")  

```
Here we have cluster top terms similar than using euclidean distance.  

##### Euclidean-based Dissimilarity Matrix

The Euclidean dissimilarity matrix seems to show that there are three clusters. One grouping chapter 10 *The Drugs Don't Work*, one chapter 9 *A Sea Of Dudes* and the other one the rest of the book. 

```{r Euclidean-based, echo=F, message=F, warning=F}

# Hierarchical clustering
data.hc_eu <- hclust(as.dist(data.euc))

plot(data.hc_eu)

```

From the clustering, we extract the ten words that are the most used. 

```{r Euclidean-based cutree, echo=F, message=F, warning=F}

# Creates three clusters
data.clust_eu <- cutree(data.hc_eu, 
                        k = 6)
data.clust_eu %>% 
  as.tibble() %>% 
  mutate(document = as.factor(names(data.clust_jac)), 
         cluster = value) %>% 
  select(!(value)) %>% 
  arrange(cluster) %>% 
  #pivot_wider(names_from = cluster, names_prefix = "cluster", values_from = document) %>% 
  kbl(caption = "Clusters using Euclidiean distance") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) %>% 
  scroll_box(height = "10%")
```

From the clustering, we extract the ten words that are the most used. 

```{r Euclidean-based clust words, echo=F, message=F, warning=F}
# Extract most frequent terms from clusters
clust_eu <- data.frame(Clust.1 = names(sort(apply(data.tfidf[data.clust_eu == 1,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.2 = names(sort(apply(data.tfidf[data.clust_eu == 2,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.3 = names(sort(apply(data.tfidf[data.clust_eu == 3,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.4 = names(sort(apply(data.tfidf[data.clust_eu == 4,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.5 = names(sort(apply(data.tfidf[data.clust_eu == 5,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.6 = names(sort(apply(data.tfidf[data.clust_eu == 6,], 2, sum), 
                                            decreasing = TRUE)[1:10]))

kbl(clust_eu, 
    caption = "Ten Most Frequent Terms By Cluster") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) %>% 
  scroll_box(height = "10%") 

```
It is interesting to observe that cluster one seems more to have words about transport and work, cluster two about car, and cluster 3 about medical.  

#### K-Means

K-means is applied on the features (i.e. terms?) using TF-IDF. 


```{r Euclidean-based km, echo=F, message=F, warning=F}

# Creates three clusters
data.km <- kmeans(data.tfidf,
                     centers = 6)
data.km$cluster %>% 
  as.tibble() %>% 
  mutate(document = as.factor(names(data.clust_jac)), 
         cluster = value) %>% 
  select(!(value)) %>% 
  arrange(cluster) %>% 
  #pivot_wider(names_from = cluster, names_prefix = "cluster", values_from = document) %>% 
  kbl(caption = "Clusters using Euclidiean distance") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) %>% 
  scroll_box(height = "10%")
```

```{r km clust words, echo=F, message=F, warning=F}

# Extract most frequent terms from clusters
clust_km <- data.frame(   Clust.1 = names(sort(apply(data.tfidf[data.km$cluster == 1,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                          Clust.2 = names(sort(apply(data.tfidf[data.km$cluster == 2,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                          Clust.3 = names(sort(apply(data.tfidf[data.km$cluster == 3,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                          Clust.4 = names(sort(apply(data.tfidf[data.km$cluster == 4,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                          Clust.5 = names(sort(apply(data.tfidf[data.km$cluster == 5,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                          Clust.6 = names(sort(apply(data.tfidf[data.km$cluster == 6,], 2, sum), 
                                            decreasing = TRUE)[1:10]))

kbl(clust_km, 
    caption = "Ten Most Frequent Terms By Cluster") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) %>% 
  scroll_box(height = "10%")

```
Using kmean we have the cluster 2 that is different from other method, with top terms related to politics.

### Similarities Between Words

Now we analyze similarities between words through chapters (i.e. documents). Because of the large number of words, we focus on word frequency ranks smaller or equal to 40 (i.e. 40 most frequent words).

#### Cosine

What words are similar ? It means they are used in similar proportion through documents.

```{r euclidean similarities btwn words, echo=F, message=F, warning=F}

# Extracts the features
data.feat <- textstat_frequency(data.dfm) %>% 
  filter(rank <= 40)

#?????????? à enlever non ? data.feat$feature

data.cos <- textstat_simil(data.dfm[, data.feat$feature], 
                           method = "cosine", 
                           margin = "feature")

# Converts the object to matrix then to data frame
data.cos.mat <- melt(as.matrix(data.cos)) 

# Plots
ggplot(data = data.cos.mat, 
       aes(x = Var1, 
           y = Var2,  
           fill = value)) +
  scale_fill_gradient2(low = "blue", 
                       high = "red", 
                       mid = "white",
                       midpoint = 0.5, 
                       limit = c(0,1), 
                       name = "Cosine") +
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 45, 
                                   hjust = 1))


```


### Clustering With Co-occurrences [à revoir]

Using co-occurrences to cluster features requires transforming co-occurrences into dissimilarities. Object needs to be the tokens because co-occurrences needs the token order and thus cannot be computed on a BOW object. 

Are terms co-occurring ? if yes, a lot ? 

```{r clustering with co-occurrences, echo=F, message=F, warning=F}

# Makes a co-occurrence on a (half-)window of 3
data.fcm <- fcm(data.tk2, 
                window = 80, 
                tri = FALSE)

# Makes the co-occurrence matrix symmetrical
data.fcm <- (data.fcm + t(data.fcm))/2

data.feat <- textstat_frequency(dfm(data.tk2)) %>% filter(rank <= 40) 

# Makes a matrix
data.fcm.mat <- melt(as.matrix(data.fcm[data.feat$feature, 
                                        data.feat$feature]), 
                     varnames = c("Var1","Var2"))

# Makes a heatmap with most frequent features
ggplot(data = data.fcm.mat, 
       aes(x = Var1, 
           y = Var2, 
           fill = value)) +
  scale_fill_gradient2(low = "blue", 
                       high = "red", 
                       mid = "white",
                       midpoint = 140, 
                       limit = c(0,280), 
                       name = "Co-occurrence") +
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r clustering analysis, echo=F, message=F, warning=F}

data.inv_occ <- 280 - as.matrix(data.fcm[data.feat$feature, 
                                         data.feat$feature]) # 280 is the max co-occurrence here

data.hc <- hclust(as.dist(data.inv_occ))

plot(data.hc)

```

## Topic Modeling

Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. There are two approaches: Latent Semantic Analysis model and Latent Dirichlet Allocation model. 

Both models can be applied on DTM or TF-IDF matrices. 

### Latent Semantic Analysis (LSA)

#### LSA on DTM

The Latent Semantic Analysis (LSA) is a dimension reduction method that decomposes the DTM or TF-IDF matrices in three sub-matrices around a pre-determined number of topics. Here, we proceed to LSA on the document-term (i.e. document-feature) matrix and then extract the matrices of the LSA decomposition. 

The first dimension of LSA is associated with the document lengths (i.e. sum of lines of DTM) this is why it is often ignored. 

```{r LSA on, echo=F, message=F, warning=F}

# Latent Semantic Analysis on DTM matrix
data.lsa <- textmodel_lsa(data.tfidf, 
                          nd = 10)          # number of dimensions/topics included in the output

```

##### Topics and Documents

The below output shows the association between the first six chapters and the ten topics. For example, text1 is associated the most with dimension (i.e. topic) 4 and the least associated with dimension (i.e. topic) 6. 

```{r LSA topics with documents, echo=F, message=F, warning=F}

# Extract matrices from LSA decomposition for documents
data.lsa$docs

```

To verify if the first dimension is associated with the document length we look at the below scatter-plot. We see that dimension 1 is negatively correlated with the number ok tokens, which confirms that the first dimension is associated with the document lenght.

[Improve plot: add a straight line showing the correlation]

```{r first dimension, echo=F, message=F, warning=F}

# Number of tokens per text = row-sum of the DTM 
doc.freq <- ntoken(data.tk2) 

# Scatter plot
data.frame(doc.freq, 
           dim1 = data.lsa$docs[,1]) %>% 
  ggplot(aes(doc.freq, dim1)) +
  geom_point(fill = "dodgerblue3", alpha = 0.5) + 
  xlab("Number of tokens") + 
  ylab("LSA dim. 1") +
  theme_minimal()

```

##### Topics and Features

The below output shoes the association between the first six most frequent words and the ten topics. For example, the term *sexist* is associated the most with topic 10. 

```{r LSA topics with terms, echo=F, message=F, warning=F}

# Extract matrices from LSA decomposition for features
head(data.lsa$features, n = 10)

```

To actually be able to interpret the topics (i.e. dimensions) of the LSA, we look at the ten terms with the largest values and the ten terms with the lowest values. We take a look at dimension 4 and 5.

According to the below output, topic 4 is positively associated to *public*, *transport*, *sexual*, *girls*, *bus*, *spaces*, *data*, *toilets*, *harassment*, *toilet* and negatively associated to *maternity*, *drug*, *heart*, *pay*, *trials*, *paid*, *hours*, *sex*, *unpaid*, *leave*. Therefore, documents that have a large dimension 4 use more these first terms and less these last terms. 

```{r interpretaion topics-features dim 4, echo=F, message=F, warning=F}

# Number of terms to look at
n.terms <- 10

# For Dimension 4
w.order <- sort(data.lsa$features[,4], #
                decreasing = TRUE)

w.top4 <- c(w.order[1:n.terms], 
            rev(rev(w.order)[1:n.terms]))

w.top4 #?????? KABLE

```

According to the below output, topic 5 is positively associated to *female*, *public*, *trials*, *politicians*, *found*, *representation*, *party*, *government*, *political*, *study* and negatively associated to *boler*, *male*, *motion*, *seat*, *vr*, *dummy*, *car*, *body*, *tech*, *data*.Therefore, documents that have a large dimension 5 use more these first terms and less these last terms. 

```{r interpretaion topics-features dim 5, echo=F, message=F, warning=F}

## For Dimension 5
w.order <- sort(data.lsa$features[,5], 
                decreasing = TRUE)

w.top5 <- c(w.order[1:n.terms], 
            rev(rev(w.order)[1:n.terms]))

w.top5 #?????? KABLE

```

##### Topics and Documents and Features

The following biplot associates the positions of the terms (points) and the positions of the document (arrows) in the LSA space for topics 4 and 5 computed above.

Here, we see that topic (i.e. dimension) 4 is positively associated with *tech*, *body*, *data*, *politicians*, *female*, and anti-associated with *transport*, *leave*, *sexy*. 

```{r biplot2, echo=F, message=F, warning=F}

w.subset <- data.lsa$features[c(unique(c(names(w.top4), 
                                         names(w.top5)))),
                              4:5]

biplot(y = data.lsa$docs[,4:5],
       x = w.subset, 
       col = c("black","purple"),
       xlab = "Dim 4", 
       ylab="Dim 5")

```

### Latent Dirichlet Allocation (LDA)

The Latent Dirichlet Allocation is a generative model (i.e. Bayesian model) for topic modeling. LAD generates a Bag of Words model where the number of topics are pre-defined. 

```{r LDA model, echo=F, message=F, warning=F}

# Sets a seed for reproducibility
set.seed(1234)

# Semi-supervised LDA 
data.lda <- textmodel_lda(data.dfm, 
                          k = 10)    # sets the number of topics at 10

#?????? KABLE
```

The following output displays the ten most frequent terms per topic and the ten most frequent topics per document.

```{r top terms, echo=F, message=F, warning=F}

# Top terms per topic
terms(data.lda, 10)

```

```{r top topics, echo=F, message=F, warning=F}

# Top topics per document
topics(data.lda) #??? Nécessaire ??? 

```

#### Term-Topic Analysis

The term-topic analysis computes the conditional probability for a term to be found given that it is assigned to a given topic. Then, for a given topic, the largest conditional probabilities give the terms that are the most associated with this topic.The advantage of LDA is that we obtain the probabilities in addition to the term-topic assignment. 

The below output shows for each ten topics, the terms with the highest conditional probabilities. For example, this conditional probability for the term *women* is almost 1, indicating that *women* is very strongly associated with topic 5 and so that any document associated with topic 5 will have the term *women* in it. We also see that some topics are better defined than others. For example, topic 5 is the most well defined and topics 6, 7, 8 and 9 are not well defined. 

To sum up, the tables below give the probabilities to select a term w given that the term comes form topic k.

```{r term-topic, echo=F, message=F, warning=F}

# Creates the topic-term probabilities => to describe a topic we look at the largest phis
phi.long <- melt(data.lda$phi,             
                 varnames = c("Topic","Term"), 
                 value.name = "Phi")

# Transforms into a long data set and then we plot it
phi.long %>%                  
  group_by(Topic) %>% 
  top_n(10, Phi) %>%
  ggplot(aes(reorder_within(Term, Phi, 
                            Topic), Phi)) +
  geom_col(show.legend = FALSE, fill = "dodgerblue3", alpha = 0.5) +
  coord_flip()+
  facet_wrap(~ Topic, 
             scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 20))


```
#### Topic-Document Analysis

The topic-document analysis provided the probabilities for a topic to be found in a document. 

The below output shows that more than 60% of topics 2 is about text 13 and more than 60% of topic 3 is about text 14. 

To dive a little bit deeper, we then look at the ten six longest chapters (i.e. documents).

```{r longest document, echo=F, message=F, warning=F}

# Longest six documents
doc.list <- names(sort(ntoken(data.tk2), 
                       decreasing = TRUE)[1:6])
doc.list #???KABLE

```

[Improve layout: can we add chapter name instead of text ?]

Below we see that chapter 14 *Women's Rights Are Human Rights* mainly talks about topic 3. 

```{r longest document plot, echo=F, message=F, warning=F}

theta.long <- melt(data.lda$theta, 
                   varnames = c("Doc", "Topic"), 
                   value.name = "Theta")

theta.long %>% 
  filter(Doc %in% doc.list) %>%                # selects the selected documents above
  ggplot(aes(reorder_within(Topic, 
                            Theta, 
                            Doc,), 
             Theta, fill = "dodgerblue3", alpha = 0.5)) +
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ Doc, 
             scales = "free_y") +
  scale_x_reordered() + 
    labs(title = "Topic analysis by Chapter",
                   x = "Document",
                   caption = "source: Invisible Women.pdf") +
  theme_minimal()


```

#### LDA Diagnosis

The topic distribution (i.e. proportion) in a corpus is called the topic prevalence. 

The following output displays the prevalence scores for each topic using the thetas (i.e. topic-document probabilities). Here, there are 16 documents. We see that the most prevalent topic is topic 5 with 0.3722. 

```{r prevalence, echo=F, message=F, warning=F}

# Topic Prevalence
colSums(data.lda$theta)/sum(data.lda$theta)

# here, the document length is not taken into account (each document has the same length)
# sum(data.lda$theta) = number of documents

#??KABLE 

```

### Topic Modeling Measures

Topic modeling allows to organize, understand and summarize large corpora (plural of corpus). Yet, topic modeling has limitations especially regarding its interpretation of its outcomes. Therefore, some measures provide a way to extract more insight from it. 

#### Coherence

The measure of coherence allows to assess the quality of a topic (i.e. good versus bad). 

The below output gives the coherence for each ten topic. The most coherent topic is topic 5 with a coherence of 0.428 and the least coherent topic is topic 9 with a coherence of -8.934. 

```{r coherence LSA, echo=F, message=F, warning=F}

# Creates a feature co-occurrence matrix
data.codo <- fcm(data.dfm, 
                   context = "document", 
                   count = "boolean", 
                   tri = FALSE) # co-document frequencies

# Extracts terms object 
term.mat <- terms(data.lda, 5)

# Creates a loop computing the coherence measure 
Coh <- numeric(10) # creates a numeric vector of 10 zeros

# Loops over 1 to K = 10 topics
for (k in 1:10){
  
  D.mat <- as.matrix(data.codo[term.mat[,k], 
                               term.mat[,k]])
  
  D.vec <- data.dfm %>% 
    textstat_frequency %>% 
    filter(feature %in% term.mat[,k]) %>% 
    data.frame() %>%
    select(feature, docfreq)
  
  for (m in 2:5){
    
    for (l in 1:(m-1)){
      
      vm <- term.mat[m,k]
      
      vl <- term.mat[l,k]
      
      Coh[k] <- Coh[k] + log((D.mat[vm, vl] + 1) / filter(D.vec, feature == vl)$docfreq)
} }
}

Coh # ?!?!? KABLE 

```

To verify we take a look at the co-document frequencies. Comparing the two below term-frequency matrices for topic 9 and 4, it is obvious that the top five terms in topic 5 are co-occurring more often in the same document than the top five terms in topic 9.

```{r feature matrix for topic 5, echo=F, message=F, warning=F}

# Feature matrix for topic 5
as.matrix(data.codo[term.mat[,5], 
                    term.mat[,5]])

```

```{r feature matrix for topic 9, echo=F, message=F, warning=F}

# Feature matrix for topic 9
as.matrix(data.codo[term.mat[,9], 
                    term.mat[,9]])

```

#### Exclusivity 

A topic is exclusive if it is associated with terms that are not associated to another topic. 

The output below shows that the most exclusive topic (i.e. topic which as the more terms not associated with another topic) is topic 1 with an exclusivity measure of 0.65252, meaning that its five top terms are more specific to it, and the least exclusive one is topic 4 with 0.00280. 

```{r exclusivity LSA, echo=F, message=F, warning=F}

# Creates a numeric vector of 10 zeros
excl <- numeric(10)

# Loops over the 10 topics
for (k in 1:10){
  for (i in 1:length(term.mat[,k])){
    
    term.phi <- filter(phi.long, 
                       Term == term.mat[i,k])
    
    excl[k] <- excl[k] + filter(term.phi, Topic == "topic1")$Phi / sum(term.phi$Phi)
}
  excl[k] <- excl[k] / length(term.mat[,k])
}

excl # ?!?!? KABLE 

```

## Unsupervised learning: Embedding

Embedding refers to the representation of elements (documents or tokens) in a Vector Space Model (VSM).

### Word Embedding Based on Co-occurences and GloVe

The idea behind word embedding based on co-occurrences is to reflect co-occurrences and not only Bag of Words (BoW). 

To start with, we compute a feature co-occurrence symmetric matrix with a window of five. The matrix is very large (8,640x8,640) and displays the number of times terms (i.e. features) appear together in a window of five words. 

```{r fcm, echo=F, message=F, warning=F}

# Makes a co-occurrence on a (half-)window of 5
data.tk5 <- tokens(data.cp,
                   remove_punct = TRUE,
                   remove_symbols = TRUE,
                   remove_numbers = TRUE,
                   split_hyphens = TRUE) %>%
                   tokens_tolower() %>%
                   tokens_remove(stop_words$word) %>%
                   tokens_remove("chapter") %>% 
                   tokens_remove(min_nchar = 3) %>%
                   tokens_replace(pattern=hash_lemmas$token, 
                   replacement = hash_lemmas$lemma)


 
data.coo <- fcm(data.tk5, 
                context = "window", 
                window = 5, 
                tri=FALSE)

head(data.coo)

```

From the feature co-occurrences matrix we compute two vector representations for a given word (i.e. feature). One representation for a word being the central term and the other one  for a word being in the context. To then have a unique representation for a given word, we compute the average of the two representations.

```{r two representations, echo=F, message=F, warning=F}

# Sets a seed for reproducibility 
set.seed(123)

# Word embedding dimension
p <- 2 

# x_max is a needed technical option
data.glove <- GlobalVectors$new(rank = p,    # dimension of the vector space model
                                x_max = 10) 

# central vectors; speech.glove$components contains the context vectors
data.weC <- data.glove$fit_transform(data.coo) 

```

The following output displays the unique representations of the vectors?.

```{r vectors, echo=F, message=F, warning=F}

# unique representation
data.we <- t(data.glove$components) + data.weC 

head(data.we, 10)

```

We now plot the vectors of the 50 most used words (50 largest frequencies).

```{r unique reprensentaion, echo=F, message=F, warning=F}

# Words with the 50 largest frequencies
index2 <- textstat_frequency(dfm(data.tk5))[1:20,]$feature

index2 # ?? Nécessaire ? ?


```

```{r plot, echo=F, message=F, warning=F}

# Plots
plot(data.we[index2, ], 
     type = 'n',  
     xlab = "Dim 1", 
     ylab = "Dim 2")

text(x = data.we[index2, ], 
     labels = rownames(data.we[index2, ]))
title(main = "20 most used words", sub = "source: Invisible Women.pdf")
```

### Document Embedding {.tabset}

To build the document embedding we compute the centroids of the documents. First, we need to extract the words in each document. 

```{r words extraction, echo=F, message=F, warning=F}

# Words in Document 1
head(data.tk5[[1]]) # ??? Nécessaire on les voit juste après 

```

Then, for these words we extract the word vectors and make a matrix.

```{r word vectors, echo=F, message=F, warning=F}

# Word vectors
head(data.we[data.tk5[[1]],])

```

Finally, we average all these vectors.

```{r average, echo=F, message=F, warning=F}

# Averages all these vectors => Document 1 vector
apply(data.we[data.tk5[[1]],], 2, mean) 

#POtentiellement faire un élément pour mettre ça et le truc précédent dans un kable 

```

Now we make the loop to apply the previous steps on all documents.

```{r loop over documents, echo=F, message=F, warning=F}

# Number of documents
nd <- length(data.tk5) 

# Document embedding matrix (1 document per row)
data.de <- matrix(nr = nd, 
                  nc = p) 

# Loop over documents
for (i in 1:nd){
  
  # drop=FALSE is needed in case there is only one token
  words_in_i <- data.we[data.tk5[[i]], , drop = FALSE] 
  
  data.de[i,] <- apply(words_in_i, 2, mean)
  
}

row.names(data.de) <- names(data.tk5)

head(data.de) ## document vectors


```
 
#### Centroids using DTM 
```{r plot-texts, echo=F, message=F, warning=F}

# Plots
plot(data.de, type='n',  xlab="Dim 1", ylab="Dim 2", main="Centroids")
text(x=data.de, labels=rownames(data.de))
title(sub = "source: Invisible Women.pdf")
```

#### Centroids using TF-IDF
```{r centroid plot tfidf, echo=F, message=F, warning=F}

data_tfidf <- dfm_tfidf(dfm(data.tk5))

data.detfidf <- matrix(nr=nd, nc=p) # document embedding matrix (1 document per row)

for (i in 1:nd){
  
  words_in_i <- data.we[data.tk5[[i]],,drop=FALSE]
  
  weights.tfidf <- as.numeric(data_tfidf[i, data.tk5[[i]]])
  
  weights.tfidf <- weights.tfidf / sum(weights.tfidf)
  
  data.detfidf[i,] <- apply(weights.tfidf * words_in_i,2,sum)
  
}

plot(data.detfidf, type='n',  xlab="Dim 1", ylab="Dim 2", main="Centroids - TFIDF")

text(x=data.detfidf[,1], y=data.detfidf[,2], labels=rownames(data_tfidf))

```
## Supervised Analysis

The goal of the supervised analysis is to re-classify its documents (i.e. chapters) using the features (i.e. terms) analyzed previously [add where maybe] [is that a good reformulation of the aim?]. To do so, we proceed to a machine learning approach consisting of splitting the corpus (*data* object) into a training and a test sets. Then, we train the classifiers on the training set and finally the best classifiers are selected on the test set. 

To be able to proceed to the classification method described above, the corpus must be cleaned in order for the features (i.e. terms) to be usable. The cleaning process is achieved in the *Data Structuring and Cleaning* of the **Data** section and consists in sequential steps from the tokenization, removing useless words (i.e. stop words), lemmatization (i.e. tokens simplification) to stemming (i.e. reduce words to their stems). 

```{r test, echo=F, message=F, warning=F}
# create dfm object
supervied_dfm <- dfm(supervised_tk)

# dimension reduction with LSA 
# reduce feature to 30 dimensions
data.lsa <- textmodel_lsa(supervied_dfm, nd = 30) 

# set the response to predict (parts of the book as factor)
y <- as.factor(supervised_data$part)

# transform to data frame
df <- data.frame(Class=y, X=data.lsa$docs)  

set.seed(245)
# split training set 80% and test set 20%
index.tr <- sample(size=round(0.4*length(y)), x=c(1:length(y)), replace=FALSE)
df.tr <- df[index.tr,]
df.te <- df[-index.tr,]

# train the classifier (random forest)
data.fit <- ranger(Class ~ ., 
                     data = df.tr)
pred.te <- predict(data.fit, df.te)


confusionMatrix(data=pred.te$predictions, reference = df.te$Class)
```

```{r, echo=FALSE}
set.seed(999)
trctrl <- trainControl(method = "boot")

# trctrl <- trainControl(method = "cv", number = 20)

train(Class~., 
      data=df.tr, 
      method = 'ranger', 
      trControl=trctrl, 
      preProcess = c("center","scale"), 
      tuneLength = 10)

```






