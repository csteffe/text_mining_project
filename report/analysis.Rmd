# 4. Analysis

This section dives deeper into the analysis of the content of the corpus in which each part is supported by well-designed relevant charts and graphs using the **ggplot2**, **sentimentr**, **reshape2**, **quanteda.textmodels**, **seededlda** and **text2vec** packages.

The previous exploratory data analysis sheds light on overall and document-specific findings related to terms and vocabulary used throughout the book. Indeed, following a top-down approach, we examine terms specific to the general corpus which unsurprisingly reveal a gender-centric vocabulary and then look more closely at the focus of each chapter on the gender discrimination issue on which *Invisible Women* devotes all its attention. 

Looking at the complexity of the book, *Invisible Women* is built of 16 chapters divided in six parts and tells its story over 280 pages. For the purpose of the following analysis, we take care of talk about the complexity of the book: [not sure about the point of this sentence??] The lenght of the document without any cleaning is 97'382. With the cleaning process we keep less than 40% of the tokens present in the document with a lenght of 36'160. The vast majority of the words bringing little to no information, we can consider that the the book is not so complex [can we really say that ??] [DO YOU HAVE ANY OTHER IDEAS THAT COULD ASSESS THE COMPLEXITY OF THE TEXT ?]
-> talk about the uniqueness of the data : [NOT SURE WHAT TO MENTION HERE BUT MUST BE MENTIONNED]

The subsequent analysis is as follows. We first study the sentiment of the corpus by extracting for each chapter its average sentiment using a qualitative and a quantitative approach. Then, we focus our attention on the similarity between terms to study their context and between chapters to better understand their associated topics of interest. From clustering of term similarities, we continue through topic modelling using two approaches. Finally, we end with an unsupervised and supervised learning methods to represent terms and documents in dimensions. 

## Sentiment Analysis 

Now that the content of the corpus is cleaned and that we explored its content, we proceed to its further analysis with first its sentiment analysis (i.e. opinion mining) which qualifies or quantifies the sentiment emerging from one text. To proceed to the sentiment analysis, we use two approaches. The first one uses qualifiers (i.e. dictionary-based) and the second one uses numerical values (i.e. value-based).  

When analyzing the sentiment emerging from a document, we care to not remove stop words since they might be in the sentiment dictionary and provide useful insight. 

```{r data.tk4, echo=F, message=F, warning=F}

# Tokenize = create a tokens object
#data.tk1 <- tokens(data.cp,
                   #remove_numbers = FALSE,
                   #remove_punct = TRUE,
                   #remove_symbols = TRUE,
                   #remove_separators = TRUE)

# Not removing the stop words
data.tk4 <- data.tk1 %>%
            tokens_tolower() %>%
            tokens_replace(pattern = hash_lemmas$token, 
                           replacement = hash_lemmas$lemma)

```

### Dictionary-Based 

The dictionary-based sentiment analysis matches tokens from each document to a reference dictionary with token values and look for word polarity (i.e. association to a sentiment). The dictionary matches terms to a **positive**, **negative**, **neg_positive** or **neg_negative** sentiment. For simplicity purposes, we only consider the **positive** and **negative** sentiment in the rest of this analysis. Note that the sentiment is the average over token values of the document. 

The disadvantage of the dictionary-based sentiment analysis is that the negative forms of words are not taken into consideration. For example, in the sentence *I don't enjoy the show*, the sentence will be considered positive because it will not consider the contraction *don't* but only the word *enjoy*.

```{r dictionary-based analysis, message=FALSE, warning=FALSE, include=FALSE}

# Dictionary used
# data_dictionary_LSD2015 # un-comment to see the object

# Matches tokens with dictionary values
data.sent <- tokens_lookup(data.tk4, 
                           dictionary = data_dictionary_LSD2015) %>% 
             dfm() %>%     # creates a dtm matrix
             tidy() %>%
             pivot_wider(names_from = term, values_from = count) %>%
             select(!c(`neg_positive`,`neg_negative`))

kbl(data.sent,
    caption = "Sentiment Analysis (Dictionary-Based)") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) %>%
  scroll_box(height = "20%")

```

The below interactive graph shows for each chapter of the book the proportion of terms matched with a positive and negative sentiment. For example, chapter two *Gender Neutral With Urinals* is found to have 177 terms matched with a positive sentiment and 408 with a negative one. 

Overall, positive and negative sentiments are found in all 16 chapters of the book although more terms are recognized as negative (3,596) than as positive (2,661) indicating that the frequency of negative terms is higher than the one of positive terms. 
```{r dictionary-based analysis plot2, echo=F, message=F, warning=F, fig.height = 3, fig.width = 3, fig.align = "center"}

data.sent <- tokens_lookup(data.tk4, 
                           dictionary = data_dictionary_LSD2015) %>% 
             dfm() %>%     # creates a dtm matrix
             tidy() %>%
             filter(term != "neg_positive" & term != "neg_negative")

# Plots the above tibble
ggplotly(ggplot(data.sent,
                aes(x = reorder(document, count), 
                    y = count, 
                    fill = term, 
                    text = paste('</br><b>Chapter:</b> ', document,
                                 '</br><b>Count:</b> ', count))) + 
         geom_bar(stat = "identity", position = "fill", alpha = 0.5) + 
         coord_flip() + 
         theme_minimal() +
         scale_fill_manual(values = c("firebrick3", "forestgreen")) +
         labs(title = "Dictionary-Based Sentiment Analysis",
              x = "Chapter",
              y = "Count",
              fill = "Sentiment"),
         tooltip = "text") 


data.sent <- tokens_lookup(data.tk4, 
                           dictionary = data_dictionary_LSD2015) %>% 
  dfm() %>%     # creates a dtm matrix
  tidy() %>%
  filter(term != "neg_positive" & term != "neg_negative") %>% group_by(document) %>%summarise (relfreq = count/sum(count), term=term)%>%ungroup()



# Plots the above tibble
ggplotly(ggplot(data.sent,
                aes(x = reorder(document, term), 
                    y = relfreq, 
                    fill = term, 
                    text = paste('</br><b>Chapter:</b> ', document,
                                 '</br><b>Relative Frequency:</b> ', relfreq))) + 
           geom_bar(stat = "identity", alpha = 0.5) + 
           coord_flip() + 
           theme_minimal() +
           scale_fill_manual(values = c("firebrick3", "forestgreen")) +
           labs(title = "Dictionary-Based Sentiment Analysis",
                x = "Chapter",
                y = "Count",
                fill = "Sentiment"),
         tooltip = "text") 

neg <- data.sent %>%
       filter(term == "negative")

neg_count <- sum(neg$count) # 3,596

pos <- data.sent %>%
       filter(term == "positive")

pos_count <- sum(pos$count) # 2,661

```

### Valence Shifters {.tabset}

The valence shifters approach uses positive and negative sentiment scores (i.e. value-based) to extract the sentiment of a document. Here, we use two dictionaries, a polarized words dictionary where we find a list of terms communicating a positive or negative attitude and a valence-shifters dictionary which provides terms that alter or intensify the meaning of the polarized words. 

#### Polarized words dictionary

The next table shows the first five words of the polarized words dictionary and their respective numerical scores.

```{r polarized words dictionary, echo=F, message=F, warning=F}

# Polarized words dictionary
hash_sentiment_jockers_rinker %>% 
  rename(token = x, value = y) %>% 
  filter(token !="acute" & token!="acutely") %>% 
  head(5) %>% 
  kbl(caption = "Polarized Words Dictionary") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

#### Valence-shifters dictionary

The next table shows the first five words of the valence-shifters dictionary and their respective numerical scores.

```{r valence-shifters dictionary, echo=F, message=F, warning=F}

# Valence-shifters dictionary
hash_valence_shifters %>% 
  rename(token = x, value = y) %>% 
  head(5) %>% 
  kbl(caption = "Valence-Shifters Dictionary") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

To proceed to the valance-shifters sentiment analysis, we first extract the sentences from the text and compute their sentiment value. Here, we do not assign weights to certain types of sentences (e.g. questions) since we believe that the sentence type does not have a particular influence on our analysis. 

The below output displays the sentiment values of the first five sentences of the corpus, respectively of chapter one, and indicates for each the sentiment emerging from the terms of the sentences. Anything numerical value below 0.05 is considered negative and any value above 0.05 is considered positive. Anything in between is considered neutral. Thus, the first, fourth and fifth sentences are negative, respectively -0.408, -0.296 and -0.535, and the second and third ones are positive, respectively 0.245 and 0.096. Note that because the column *word_count* had NAs, we remove rows that have no available information since no sentiment can be extracted.

```{r sentiment analysis, echo=F, message=F, warning=F}

# Extracts sentences from the text
text_sentences <- get_sentences(text)

# Computes sentiment by sentence
sent_bysent <- sentiment(text_sentences) %>%   # assigns a polarity scores
               filter(word_count != "NA") %>% 
               mutate(document = paste("Chapter", element_id, sep = " ")) %>% 
               relocate(document, .before = element_id) %>% 
               select(!(element_id))

kbl(head(sent_bysent, 5), 
    caption = "Sentiment Values by Sentence") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)  

```

Since sentiment changes as sentences change, we zoom out to look at the sentiment score by chapter of the book. The *ave_sentiment* column gives the average sentiment score by chapter. 

```{r sentiment analysis by document, echo=F, message=FALSE, warning=FALSE, include=F}

# Computes the sentiment score for each chapter
sent_bychap <- sentiment_by(text) %>%
               arrange(element_id) %>%
               mutate(document = paste("Chapter", element_id, sep = " ")) %>% 
               relocate(document, .before = element_id) %>% 
               select(!(element_id))

sent_bychap %>% 
  arrange(desc(ave_sentiment))  %>% 
  kbl(caption = "Sentiment Values by Chapter") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) 

```

The next interactive graph displays the average sentiment scores by chapter in a decreasing fashion. According to it, chapter 4 *The Myth of Meritocracy* has the greatest positive average (0.323) and chapter 16 *It's Not The Disaster That Kills You* has the the most negative average sentiment. In total, five chapters have a positive sentiment (i.e. above 0.05), four have a negative sentiment (i.e. below -0.05) and seven chapters have a neutral sentiment (i.e. between -0.05 and 0.05). 

```{r sentiment analysis by document plot, echo=F, message=F, warning=F,  fig.height = 3, fig.width = 3, fig.align = "center"}

# Plots the sentiment curve by document
#sent_bychap %>% 
ggplotly(
  ggplot(sent_bychap, 
         aes(x = reorder(document,ave_sentiment),
             y = ave_sentiment, 
             text = paste('</br><b>Document:</b> ', document,
                          '</br><b>Word count:</b> ', word_count,
                          '</br><b>Std. deviation:</b> ', sd)
             )) + 
  geom_bar(stat = "identity", fill = "dodgerblue3", alpha = 0.5) + 
  geom_hline(yintercept = 0.05, linetype="dotted", 
             color = "darkred", size=0.5) +
  geom_hline(yintercept = - 0.05, linetype="dotted", 
               color = "darkred", size=0.5) +
  coord_flip() +
  labs(title = "Sentiment Score by Chapter",
                   x = "Document",
                   y = "Average sentiment",
                   caption = "source: Invisible Women.pdf") +
  theme_minimal(),
  tooltip = "text"
)

```

## Similarities and Clustering

Similarity is a numerical value used to measure proximity between terms, to see if they are used in the same context or between documents, to see if they use the same terms. Note that similarity is dependent on the types of tokens found in the corpus. If it uses mostly one vocabulary, it might be possible that most terms are similar. 

To compute similarities, we use three different measures namely the *Jaccard Index*, *Cosine Similarity* and *Euclidean Distance* all three using the term-frequency inverse-document frequency.

For all the different clusters, we decided to cut tree at 6 clusters as there is 6 part in the book.

### Similarities between Documents {.tabset}

We first compute the similarity between chapters of the book to investigate whether they use the same token types. 

#### Jaccard Index

We first compute the Jaccard index matrix displaying the relative number of common words using the TF-IDF matrix. Note that the Jaccard coefficient considers only once each token type (i.e. set model). 

From the Jaccard similarity matrix, where similarities are based on the Jaccard coefficient (i.e. relative number of common words) by document, we get the below heatmap in which chapters are likely to use similar terms. A red square indicates a strong similarity (e.g. 1) whereas a dark blue square indicates no similarity. Therefore, the heatmap shows that chapter 15 *Who Will Rebuild* is the text with the least similarity with other chapters (score closer to 0). Also, we observe that chapters 10 *The Drugs Don't Work* and 11 *Yentl Syndrome* seem to use a bit more similar terms.  

```{r Jaccard, echo=F, message=F, warning=F}

# Computes similarities and distances between documents
data.jac <- textstat_simil(data.tfidf,                  # uses the frequencies
                           method = "jaccard", 
                           margin = "documents")        # between documents

# Converts the object to matrix then to data frame
data.jac.mat <- melt(as.matrix(data.jac)) 

# Plots
ggplot(data = data.jac.mat, 
       aes(x = Var1, 
           y = Var2, 
           fill = value)) +
  scale_fill_gradient2(low = "blue", 
                       high = "red", 
                       mid = "white",
                       midpoint = 0.5, 
                       limit = c(0,1), 
                       name="Jaccard") +
  geom_tile() +
  labs(title = "Heatmap Representation Of Similarities Between Documents",
       x = "Chapter",
       y = "Chapter")+
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 22))

```

#### Cosine Similarity

Then, we compute the cosine similarity matrix which computes the similarity between two vectors of an inner product space. Note here that the similarity is independent of the vector length and that only the cosine angle between two weighted term-frequency vectors is determinant.

Compared to the heat map generated using the Jaccard index, the next heatmap displays
much more no similarity (darker blue squares) between terms used in different chapters than the previous one. However, chapter 10 and 11 are still shown as sharing less similarity.

```{r Cosine, echo=F, message=F, warning=F}

# Cosine
data.cos <- textstat_simil(data.tfidf,                  # uses frequencies
                           method = "cosine",           # cosine
                           margin = "documents")   

# Converts the object to matrix then to data frame
data.cos.mat <- melt(as.matrix(data.cos))

# Plot
ggplot(data = data.cos.mat, 
       aes(x = Var1, 
           y = Var2, 
           fill = value)) +
  scale_fill_gradient2(low = "blue", 
                       high = "red", 
                       mid = "white",
                       midpoint = 0.5, 
                       limit = c(0,1), 
                       name="Cosine") +
  geom_tile() +
  labs(title = "Heatmap Representation Of Similarities Between Documents",
       x = "Chapter",
       y = "Chapter")+
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 22))

```

#### Euclidean Distance

Finally, we compute the Euclidean-based similarity matrix using the Euclidean distance. The below heatmap shows again a different output. Indeed, less documents are shown as having no similarity and more chapter similarities stand in a middle in which we cannot infere on similarity. For example, chapter 15 *Who Will Rebuild* and 16 *It's Not The Disaster That Kills You* do not seem to either show similarity or dissimilarity. On the contrary, chapters 12 *A Costless Resource To Exploit* and 15 *Who Will Rebuild* are slightly more using similar terms.

```{r Euclidean, echo=F, message=F, warning=F}

# Euclidean
data.euc <- textstat_dist(data.tfidf,                   # uses the frequencies
                          method = "euclidean",         # Euclidean distances
                          margin = "documents")

# Converts the object to matrix then to data frame
data.euc.mat <- melt(as.matrix(data.euc))

# Maximum distance
M <- max(data.euc.mat$value) 

# Converts from distance to similarity in [0,1]
data.euc.mat$value.std <- (M - data.euc.mat$value)/M 

# Plots
ggplot(data = data.euc.mat, 
       aes(x = Var1, 
           y = Var2, 
           fill = value.std)) +
  scale_fill_gradient2(low = "blue", 
                       high = "red", 
                       mid = "white",
                       midpoint = 0.5, 
                       limit = c(0,1), 
                       name = "Euclidean") +
  geom_tile() +
  labs(title = "Heatmap Representation Of Similarities Between Documents",
       x = "Chapter",
       y = "Chapter")+
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 22))

```

### Clustering of Documents {.tabset}

To proceed to clustering documents, we need to build the dissimilarities and/or Vector Space Model (VSM) on which we can apply the clustering methods, hierarchical clustering based on distances and K-mean partitioning based on features. 

Clusters are difficult to interpret. This is why we will look at the largest term frequencies of clusters to better understand what is the common denominator for the grouping.

#### Hierarchical Clustering {.tabset}

Hierarchical clustering is based on distances and applied on the dissimilarities using the function **hclust()**. The hierarchical approach assigns each document to its own cluster and then at each iteration the two most similar chapters are grouped together in one cluster. The iteration continues until all chapters belong to a cluster. 

##### Inverted Jaccard Dissimilarity Matrix

The inverted Jaccard dissimilarity matrix shows that there are two main clusters. One cluster groups chapters 15 *Who Will Rebuild* and 16 *It's Not The Disaster That Kills You* and the other one groups the rest of the book. Moreover, inside the second larger cluster, we see that there are two sub-clusters. From the dendogram, we understand that chapters 10 *The Drugs Don't Work* and 11 *Yentl Syndrom* are the most similar ones as their distance is the smallest (~0.76). On the contrary, chapters [CAN WE ADD WHICH CHAPTERS ARE THE MOST DISSIMILAR?]

```{r Jaccard inverted, echo=F, message=F, warning=F}

# Hierarchical clustering
data.hc_jac <- hclust(as.dist(1 - data.jac)) # inverted similarity matrix

plot(data.hc_jac)
abline(h=0.84, lty = 2)

```

The following table indicates to which cluster a chapter belongs.

```{r Jaccard cutree, echo=F, message=F, warning=F}

# Creates three clusters
data.clust_jac <- cutree(data.hc_jac, 
                         k = 6)
data.clust_jac %>% 
  as.tibble() %>% 
  mutate(document = names(data.clust_jac), 
         cluster = value) %>% 
  select(!(value)) %>% 
  pivot_wider(names_from = cluster, names_prefix = "cluster", values_from = document) %>% 
  kbl(caption = "Clusters using Jaccard") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) 

```

For interpretation purposes, we extract the ten words that are the most frequent in each cluster to identify a common denominator between chapters. According to the most used terms in each cluster, cluster 1 groups terms about public transports or spaces, cluster 2 about medical or technological trials and clusters three to six are composed of only one chapter and therefore group terms according to their respective chapters' vocabulary. 

```{r Jaccard clust words, echo=F, message=F, warning=F}

# Extract most frequent terms from clusters
clust_jac <- data.frame(Clust.1 = names(sort(apply(data.tfidf[data.clust_jac == 1,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.2 = names(sort(apply(data.tfidf[data.clust_jac == 2,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.3 = names(sort(apply(data.tfidf[data.clust_jac == 3,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.4 = names(sort(apply(data.tfidf[data.clust_jac == 4,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.5 = names(sort(apply(data.tfidf[data.clust_jac == 5,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.6 = names(sort(apply(data.tfidf[data.clust_jac == 6,], 2, sum), 
                                            decreasing = TRUE)[1:10]))

kbl(clust_jac, 
    caption = "Ten Most Frequent Terms By Cluster") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) %>%
  scroll_box(height = "300px")

```

##### Inverted Cosine Dissimilarity Matrix

The inverted cosine dissimilarity matrix seems to show that there are again two main clusters and for each we observe two smaller sub-clusters. One main cluster groups chapters 10, 11, 5, 6, 8 and 9 and the other one groups chapters 7, 11, 12, 15, 16, 13, 3, 2, 4 and 14. The dendogram generated from the inverted cosine dissimilarity matrix indicates that again chapter 10 *The Drugs Don't Work* and 11 *Yentl Syndrom* are the most similar ones as their distance is the smallest (~0.70). On the contrary, chapters [CAN WE ADD WHICH CHAPTERS ARE THE MOST DISSIMILAR?]

```{r cosine inverted, echo=F, message=F, warning=F}

# Hierarchical clustering
data.hc_cos <- hclust(as.dist(1 - data.cos)) # inverted similarity matrix

plot(data.hc_cos)

```

```{r cosine cutree, echo=F, message=F, warning=F}

# Creates three clusters
data.clust_cos <- cutree(data.hc_cos, 
                         k = 6)
data.clust_cos %>% 
  as.tibble() %>% 
  mutate(document = names(data.clust_cos), 
         cluster = value) %>% 
  select(!(value)) %>% 
  pivot_wider(names_from = cluster, names_prefix = "cluster", values_from = document) %>% 
  kbl(caption = "Clusters using Cosine") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) #%>% 
  #scroll_box(height = "300px")

```

For interpretation purposes, we extract the ten most frequent terms of each cluster to identify a common denominator between chapters. According to the most used terms in each cluster, cluster 1's chapters share terms specific to public transport or spaces and violence, cluster 2's chapters use terms regarding households, families and economics, cluster 3 reveals a political vocabulary, cluster 4 shows terms related to technology, cluster 6 displays terms about agriculture and finally, cluster 6 indicates a medical vocabulary.

```{r cosine clust words, echo=F, message=F, warning=F}

# Extract most frequent terms from clusters
clust_cos <- data.frame(Clust.1 = names(sort(apply(data.tfidf[data.clust_cos == 1,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.2 = names(sort(apply(data.tfidf[data.clust_cos == 2,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.3 = names(sort(apply(data.tfidf[data.clust_cos == 3,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.4 = names(sort(apply(data.tfidf[data.clust_cos == 4,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.5 = names(sort(apply(data.tfidf[data.clust_cos == 5,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.6 = names(sort(apply(data.tfidf[data.clust_cos == 6,], 2, sum), 
                                            decreasing = TRUE)[1:10]))
kbl(clust_cos, 
    caption = "Ten Most Frequent Terms By Cluster") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) %>% 
  scroll_box(height = "300px")  

```

##### Euclidean-based Dissimilarity Matrix

The Euclidean dissimilarity matrix seems to show that there is sequence of clusters which is a completely different clustering pattern compared to the other two distance measures. This pattern seems to indicate that each chapter is rather independent from the others and do not share a lot of similarities across chapters of the book. According to Euclidean-based dissimilarity matrix, we see that chapters 12 *A Costless Resource to Exploit* and 15 *Who Will Rebuild?* share the smallest distance indicating that they are the most similar. This finding is different than the ones using the two other distance measures. On the contrary, chapters [CAN WE ADD WHICH CHAPTERS ARE THE MOST DISSIMILAR?]

```{r Euclidean-based, echo=F, message=F, warning=F}

# Hierarchical clustering
data.hc_eu <- hclust(as.dist(data.euc))

plot(data.hc_eu)

```


```{r Euclidean-based cutree, echo=F, message=F, warning=F}

# Creates three clusters
data.clust_eu <- cutree(data.hc_eu, 
                        k = 6)
data.clust_eu %>% 
  as.tibble() %>% 
  mutate(document = names(data.clust_eu), 
         cluster = value) %>% 
  select(!(value)) %>% 
  pivot_wider(names_from = cluster, names_prefix = "cluster", values_from = document) %>% 
  kbl(caption = "Clusters using Euclidiean distance") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) 

```

For interpretation purposes, we extract the ten most frequent terms of each cluster to identify a common denominator between chapters. According to the most used terms in each cluster, cluster 1 shows terms specific to transport and travel, cluster 2's chapters use terms that are not necessarily specific to one topic, cluster 3 reveals terms related to technology, cluster 4 indicates a medical vocabulary, cluster 5 displays terms related to households and economics and finally and finally, cluster 6 shows politics-related terms. 

```{r Euclidean-based clust words, echo=F, message=F, warning=F}

# Extract most frequent terms from clusters
clust_eu <- data.frame(Clust.1 = names(sort(apply(data.tfidf[data.clust_eu == 1,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.2 = names(sort(apply(data.tfidf[data.clust_eu == 2,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.3 = names(sort(apply(data.tfidf[data.clust_eu == 3,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.4 = names(sort(apply(data.tfidf[data.clust_eu == 4,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.5 = names(sort(apply(data.tfidf[data.clust_eu == 5,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                       Clust.6 = names(sort(apply(data.tfidf[data.clust_eu == 6,], 2, sum), 
                                            decreasing = TRUE)[1:10]))

kbl(clust_eu, 
    caption = "Ten Most Frequent Terms By Cluster") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) %>% 
  scroll_box(height = "300px") 

```

#### K-Means Clustering 

K-means is a non-hierarchical partitioning method for clustering. In text analysis, K-means is based on feature frequencies. Here, we set the pre-defined number of clusters (i.e. respectively number of centroids) to six since there are six parts in the book. By doing so, we expect to see chapters being grouped following the parts division logic of the author. 

```{r K-means, echo=F, message=F, warning=F}

set.seed(253)

# Creates six clusters
data.km <- kmeans(data.tfidf,
                  centers = 6)

data.km$cluster %>% 
  as.tibble() %>% 
  mutate(document = names(data.km$cluster), 
         cluster = value) %>% 
  select(!(value)) %>% 
  arrange(cluster) %>% 
  pivot_wider(names_from = cluster, 
              names_prefix = "cluster", 
              values_from = document) %>% 
  kbl(caption = "Clusters using Euclidiean distance") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL)

```

For interpretation purposes, we extract the ten most frequent terms of each cluster to identify a common denominator between chapters. According to the most used terms in each cluster, cluster 1 shows terms specific to politics, cluster 2's chapters use terms that are not necessarily specific to one topic, cluster 3 reveals terms related to transportation and travel, cluster 4 reveals terms about public spaces and violence, cluster 5 displays terms related to technology and finally, cluster 6 indicates a medical vocabulary. 

```{r km clust words, echo=F, message=F, warning=F}

# Extract most frequent terms from clusters
clust_km <- data.frame(   Clust.1 = names(sort(apply(data.tfidf[data.km$cluster == 1,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                          Clust.2 = names(sort(apply(data.tfidf[data.km$cluster == 2,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                          Clust.3 = names(sort(apply(data.tfidf[data.km$cluster == 3,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                          Clust.4 = names(sort(apply(data.tfidf[data.km$cluster == 4,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                          Clust.5 = names(sort(apply(data.tfidf[data.km$cluster == 5,], 2, sum), 
                                            decreasing = TRUE)[1:10]),
                          Clust.6 = names(sort(apply(data.tfidf[data.km$cluster == 6,], 2, sum), 
                                            decreasing = TRUE)[1:10]))

kbl(clust_km, 
    caption = "Ten Most Frequent Terms By Cluster") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = NULL) %>% 
  scroll_box(height = "300px")

```

### Similarities Between Words {.tabset}

Now, we analyze similarities between words through chapters (i.e. documents). A word is embedded in a Vector Space Model using its document frequencies (or weighted frequencies) this is why, we proceed to computing similarities between terms using the DTM matrix. To compute the similarities between words, we use the same three different measures as when computing the similarities between documents. Note that because of the large number of tokens, we focus on the highest word-frequencies respectively, the lowest word ranks (i.e. smaller or equal to 40). 

```{r smaller ranks, echo=F, message=F, warning=F}

# Extracts the features in the DTM
data.feat <- textstat_frequency(data.dfm) %>% 
             filter(rank <= 40)

```

#### Jaccard Index

The below heatmap shows that according to the Jaccard index most words among the 40 most frequent ones are very similar or identical. Although, the Jaccard similarity indices of *tax* and all other 39 words are represented by a dark blue squares, indicating that the these words in each pair with *tax* are not used in similar proportion through documents and consequently are not considered similar through documents. Moreover, *worker* and *pay* are not considered similar nor dissimilar according to the Jaccard index since their indices are represented by white squares (similarity of 0.5).

```{r Jaccard index similarities btwn words, echo=F, message=F, warning=F, out.height=  "70%", out.width = "80%", fig.align = "center"}

data.jac2 <- textstat_simil(data.dfm[, data.feat$feature], 
                            method = "jaccard", 
                            margin = "feature")

# Converts the object to matrix then to data frame
data.jac.mat2 <- melt(as.matrix(data.jac2)) 

# Plots
ggplot(data = data.jac.mat2, 
       aes(x = Var1, 
           y = Var2,  
           fill = value)) +
  scale_fill_gradient2(low = "blue", 
                       high = "red", 
                       mid = "white",
                       midpoint = 0.5, 
                       limit = c(0,1), 
                       name = "Cosine") +
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 90, 
                                   hjust = 1))

```

#### Cosine Distance

The below heatmap shows that according to the Cosine similarity measure some words are very similar (but not exactly identical). For example, the Cosine similarity estimate of *explain* and *mean* or *find* and *study* are represented by a dark orange square, indicating that the two words in each pair are used in similar proportion through documents. On the contrary, *design* and *tax* are found to not be used in similar proportion through documents (dark blue square) which indicates that these two terms are not considered similar through documents.

```{r cosine similarities btwn words, echo=F, message=F, warning=F, out.height=  "70%", out.width = "80%", fig.align = "center"}

data.cos2 <- textstat_simil(data.dfm[, data.feat$feature], 
                            method = "cosine", 
                            margin = "feature")

# Converts the object to matrix then to data frame
data.cos.mat2 <- melt(as.matrix(data.cos2)) 

# Plots
ggplot(data = data.cos.mat2, 
       aes(x = Var1, 
           y = Var2,  
           fill = value)) +
  scale_fill_gradient2(low = "blue", 
                       high = "red", 
                       mid = "white",
                       midpoint = 0.5, 
                       limit = c(0,1), 
                       name = "Cosine") +
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 90, 
                                   hjust = 1))

```

#### Euclidean Distance 

[TO DO !!]

### Clustering of Words {.tabset}

To avoid redundancy, we decide not to use the same similarity measures for the clustering methods (Jaccard index, Cosine distance, Euclidean distance) as in the *Clustering of Documents* and to use instead the co-occurrence similarity measure to cluster the 40 most frequent terms in the corpus that we studied above in **Similarities Between Words**. 

#### Clustering With Co-occurrence (Hierarchical Clustering)

Co-occurrence is also a similarity measure for which two words are considered similar if they are often used closely in a similar context, in any part of the corpus. Therefore, co-occurrence similarity is dependent on the token's context (i.e. document or window). Here, we consider the context as being a window of terms around a target (i.e. central word or center). The subtlety of the co-occurrence clustering is that by considering the context of a word, we are computing similarities beyond literal resemblance and, consider as well the meaning of words. Thus, this measure is different than the Jaccard, Cosine or Euclidean similarity measures which do not depend on the context of a word. Note that since the context is important, the tokens ordered must kept when computing the co-occurrence matrix (i.e. Bag of Word object cannot be used here). 

To proceed to the clustering, we first compute a symmetrical co-occurrence matrix on a window of 80 terms around a target used as a similarity matrix. The below heatmap shows the similarities of the most frequent terms [ASK PROFESSOR BOLDI WHY WE HAVE GREY SQUARES: IS IT BECAUSE TERMS DON'T CO OCCUR ?]

```{r clustering with co-occurrences, echo=F, message=F, warning=F, out.height=  "70%", out.width = "80%", fig.align = "center"}

# Makes a co-occurrence on a window of 80
data.fcm <- fcm(data.tk2, 
                window = 80, 
                tri = FALSE)

# Makes the co-occurrence matrix symmetrical
data.fcm <- (data.fcm + t(data.fcm))/2

# Makes a matrix
data.fcm.mat <- melt(as.matrix(data.fcm[data.feat$feature, 
                                        data.feat$feature]), 
                     varnames = c("Var1","Var2"))

# Makes a heatmap with most frequent features
ggplot(data = data.fcm.mat, 
       aes(x = Var1, 
           y = Var2, 
           fill = value)) +
  scale_fill_gradient2(low = "blue", 
                       high = "red", 
                       mid = "white",
                       midpoint = 140, 
                       limit = c(0,280), 
                       name = "Co-occurrence") +
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```
Then, we transform the co-occurrence matrix into a dissimilarity matrix (*maximum co-occurrence - similarity matrix*) from which we derive the co-occurrence distance-based clustering (i.e. hierarchical clustering). 

[ADD INTERPRETAION: KEEP IN MIND THAT THE CO-OCCURRENCE MATRIX IS TURNED INTO A DISSIMILARITY MATRIX -> PROVIDE EXAMPLE WITH FEMAL-WOMAN]

```{r clustering analysis, echo=F, message=F, warning=F}

# Dissimilarity matrix 
data.inv_occ <- 280 - as.matrix(data.fcm[data.feat$feature, 
                                         data.feat$feature]) # 280 is the max co-occurrence here

# Cluster
data.hc <- hclust(as.dist(data.inv_occ))

plot(data.hc)

```

## Topic Modeling {.tabset}

From the above clustering of documents, we understand that clusters are formed by grouping chapters with *similar* terms or vocabularies and that by looking at the most frequent terms in each cluster we can get an idea of what groups of chapters talk about. This is referred at the **topic** of a document. Note that one document can treat several topics. Topics are associated to both documents and terms. Indeed, a topic is associated to documents that use similar terms related to this topic and a topic is associated to terms that appear in documents treating this topic. 

Topic modeling is a type of statistical modeling for discovering the abstract *topics* that occur in a collection of documents. There are two approaches to the statistical modeling of topics, the Latent Semantic Analysis (LSA) model and the Latent Dirichlet Allocation (LDA) model. 

Both models can be applied on DTM or TF-IDF matrix and here we decide to proceed to the topic modeling on the DTM.

### Latent Semantic Analysis (LSA) 

LSA is a dimension reduction method that decomposes the DTM in three sub-matrices around a pre-determined number of topics. $\Sigma$ represents the strength of each topic, $U$ expresses the documents-topic similarity (i.e. links between documents and each topic) and $V$ expresses the terms-topic similarity (i.e. the links between the terms and each topic). 

Here, the advantages or goals of LSA is to find and interpret topics within chapters of *Invisible Women* and to reduce the dimension of the DTM object by removing sparsity which does not allow us to infer on terms association with chapters of the book. On the other hand, its disadvantage is the difficulty to interpret its results. 

To build the LSA object, we use the **textmodel_lsa** function of the **quanteda.textmodels** package and we specify the number of dimensions to be included to be ten. Then, we proceed to the LSA decomposition into the three sub-matrices extracted from the DTM. The below output shows the documents-topic matrix $U$ for the first six chapters, the terms-topic matrix $V$ for the ten most frequent terms [IS THAT CORRECT?????] and finally the topics' strength $\Sigma$ in decreasing order. 

The first matrix $U$ shows that chapter 1 *Can Snow-Clearing be Sexist?* is the most positively associated to topic 8 and the most negatively associated to topic 6. The second matrix $V$ indicates that *sexist* is the most positively associated to topic 8 and the most negatively associated to topic 3.

```{r LSA on DTM, echo=F, message=F, warning=F}

# Latent Semantic Analysis on DTM matrix
data.lsa <- textmodel_lsa(data.dfm, 
                          nd = 10) # number of dimensions/topics included in the output

head(data.lsa$docs)
head(data.lsa$features, n = 10)
data.lsa$sk

```

Before delving into the topic modeling analysis, we start by looking at the first dimension of the LSA since it is often correlated with the length of the document. The following plot shows that there is a negative linear relationship between the first component and the document length (i.e. total number of tokens in a document) indicating that the lengthier is a document (i.e. chapter), the less it will be associated to dimension 1. For this specific reason, we ignore the first component of LSA i the further topic analysis.

```{r first component LSA, echo=F, warning=F, message=F}

# Number of tokens per text = row-sum of the DTM 
doc.freq <- ntoken(data.tk2) # counts the number of tokens

# Scatter plot
data.frame(doc.freq, 
           dim1 = data.lsa$docs[,1]) %>% 
  ggplot(aes(doc.freq, dim1)) +
  geom_point(color = "dodgerblue3", 
             alpha = 0.5) + 
  theme_minimal() +
  labs(title = "LSA: dimension 1",
       subtitle = "Negative linear correlation",
       caption = "source: Invisible Women.pdf",
       x = "Number of tokens",
       y = "Dimension 1")
  
```

Then, to interpret the topics (i.e. dimensions) of the LSA, we look at the ten terms with the largest values and the ten terms with the lowest values. We decide randomly to take a look at dimension (i.e. topic) 4 and 5.

According to the below output, topic 4 is positively associated to *public*, *transport*, *toilet*, *woman*, *space*, *bus*, *travel*, *sexual*, *report*, *girl* and negatively associated to *politician*, *party*, *government*, *country*, *candidate*, *leave*, *pay*, *bias*, *male*, *female*. Therefore, chapters that are associated with topic 4 use more the first ten terms and less the last ten terms. Consequently, chapters strongly associated with component 4 are likely to talk about female experience in public spaces. 

```{r interpretaion topics-features dim 4, echo=F, message=F, warning=F}

# Number of terms to look at
n.terms <- 10

# For Dimension 4
w.order <- sort(data.lsa$features[,4], #
                decreasing = TRUE)

w.top4 <- c(w.order[1:n.terms], 
            rev(rev(w.order)[1:n.terms]))

w.top4 

```

According to the below output, topic 5 is positively associated to *test*, *dummy*, *body*, *car*, *crash*, *tech*, *datum*, *seat*, *vr*, *design* and negatively associated to *tax*, *government*, *transport*, *trial*, *find*, *female*, *drug*, *sex*, *gender*, *public*.Therefore, documents that are associated with topic 4 use more the first ten terms and less the last ten terms. Consequently, chapters strongly associated with component 5 are likely to talk about technology. 

```{r interpretaion topics-features dim 5, echo=F, message=F, warning=F}

## For Dimension 5
w.order <- sort(data.lsa$features[,5], 
                decreasing = TRUE)

w.top5 <- c(w.order[1:n.terms], 
            rev(rev(w.order)[1:n.terms]))

w.top5 

```

Now, to connect the topics and the documents and the topics and the terms, we generate a biplot that associates the position of the words and the position of the chapters in the LSA space, for dimensions 4 and 5 detailed above. To avoid readability issues due to the large number of terms, we display only words that are the most associated to these dimensions.

Here, we see that topic 4 is associated with chapters 1, 2, 11, 15, 16 and terms *woman*, *report*, *sexual*, *travel*, *bus*, *space*, *toilet*, *transport*, *public* and anti-associated with chapters 3, 4, 12, 13, 14 and terms *male*, *pay*, *leave*, *politicians*, *bias*, *country*, *politicians* etc... Topic 5 is associated with chapters 5, 6, 7, 8, 15, 19 and terms *test*, *dummy*, *body*, *car*, *crash*, *tech*, *datum* and anti-associated with chapter 10 and terms *trial*, *drug*, *sex*. 

```{r biplot2, echo=F, message=F, warning=F, fig.height = 12, fig.width = 12, fig.align = "center"}

w.subset <- data.lsa$features[c(unique(c(names(w.top4), names(w.top5)))), 4:5]

biplot(y = data.lsa$docs[,4:5],
       x = w.subset, 
       col = c("black","purple"),
       main = "Biplot: dimensions 4 and 5",
       xlab = "Dim 4", 
       ylab="Dim 5")

```

### Latent Dirichlet Allocation (LDA) {.tabset}

The Latent Dirichlet Allocation is a generative model (i.e. Bayesian/probabilistic model) meaning that it generates documents (i.e. BoW) where the number of topics are pre-defined, as with LSA. The model works as follows. Random proportions of topics are drawn in a document and for each word of the document, a topic is selected at random. Then, given this randomly selected topic, a word in its vocabulary is also selected at random. Note that the main disadvantage of LDA is the difficulty to interpret its results and that its advantage is that we obtain probabilities in addition to the term-topic assignment. To be able to interpret its results, the model is represented as being a set of conditional probabilities defined by a set of parameters fitted to the DTM using a maximum likelihood approach. 

To build the LDA object, we use the **textmodel_lda** function of the **seededlda** package and we specify the number of dimensions to be included to be ten.

```{r LDA model, echo=F, message=F, warning=F}

# Sets a seed for reproducibility
set.seed(1234)

# Semi-supervised LDA 
data.lda <- textmodel_lda(data.dfm, 
                          k = 10)   # sets the number of topics at 10

```

After building the LDA object, we look at the top three terms per topic and the top three topics per document. The following output shows that *public*, *sexual* and *toilet* are the top three words of topic 1 and that [CAN YOU HELP ME INTERPRET ???]. 

```{r top terms and topics, message=FALSE, warning=FALSE}

# Top terms per topic
terms(data.lda, 3)

# Top topics per document
topics(data.lda) %>% table()

```

#### Term-topic Analysis

The term-topic analysis computes the conditional probability $\phi$ to find a term knowing that it is assigned to a given topic. Then, for a given topic, the largest conditional probabilities give the terms that are the most associated with this topic. 

The below output shows for each topic the terms with the highest conditional probabilities $\phi$. For example, the conditional probability of finding *woman* knowing that this term is assigned to topic 10 is 1 which indicates that *woman* is very strongly associated with topic 10 and consequently, that any document associated with topic 10 will have the term *woman* in it. Moreover, we also see that some topics are better defined than others and that some topics can overlap. For example, topic 10 is the best defined topic whereas and topics 2 is not well defined at all. The reason for overlapping is probably linked to the cleaning process of the corpus. 

```{r term-topic analysis, echo=F, message=F, warning=F, out.height = "100%", out.width = "100%", fig.align = "center"}

# Creates the topic-term probabilities => to describe a topic we look at the largest phis
phi.long <- melt(data.lda$phi,             
                 varnames = c("Topic","Term"), 
                 value.name = "Phi")

# Transforms into a long data set and then we plot it
phi.long %>%                  
  group_by(Topic) %>% 
  top_n(10, Phi) %>%
  ggplot(aes(reorder_within(Term, Phi, 
                            Topic), Phi)) +
  geom_col(show.legend = FALSE, fill = "dodgerblue3", alpha = 0.5) +
  coord_flip()+
  facet_wrap(~ Topic, 
             scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90),
        axis.text=element_text(size=9))


```

#### Topic-document Analysis

The topic-document analysis computes the conditional probability $\theta$ to find a word in a topic knowing that it is assigned to a specific document. Then, for a given document, the largest conditional probabilities give the terms that are the most associated with this document. 

The below output shows, for the six longest chapters, the proportion (i.e. the highest conditional probabilities $\theta$ for each topic) of each topic in the document. For example, chapter 2 *Gender Neutral With Urinals* mainly talks about topic 1 and 10 (not in the same proportion though).

```{r topic-document analysis, message=FALSE, warning=FALSE, include=F}

# Longest six documents
doc.list <- names(sort(ntoken(data.tk2), 
                       decreasing = TRUE)[1:6])

```

```{r longest document plot, echo=F, message=F, warning=F}

theta.long <- melt(data.lda$theta, 
                   varnames = c("Doc", "Topic"), 
                   value.name = "Theta")

theta.long %>% 
  filter(Doc %in% doc.list) %>%                # selects the selected documents above
  ggplot(aes(reorder_within(Topic, 
                            Theta, 
                            Doc,),
                            Theta)) +
  geom_col(show.legend = FALSE, 
           fill = "dodgerblue3", alpha = 0.5) +
  coord_flip()+
  facet_wrap(~ Doc, 
             scales = "free_y") +
  scale_x_reordered() + 
  labs(title = "Topic analysis by Chapter",
       x = "Document",
       caption = "source: Invisible Women.pdf") +
  theme_minimal()


```

#### LDA diagnostics {.tabset}

##### Prevalence

The prevalence is the the topic distribution (proportion) in a corpus: 

$Prev(topic\ k) =  (\frac{1}{M})\Sigma_{m=1}^M \theta (k,m)$

The following output displays the prevalence scores for each topic. We see that topic 10 is the most prevalent topic in the corpus (6.264) and that topic 8 is the least prevalent (0.616).

```{r prevalence, echo=F, message=F, warning=F}

# Topic Prevalence
colSums(data.lda$theta)/sum(data.lda$theta) %>% table()

# here, the document length is not taken into account (each document has the same length)
# sum(data.lda$theta) = number of documents

```

##### Coherence

Topic modeling allows to organize, understand and summarize large corpora. Yet, topic modeling has limitations especially regarding the interpretation of its outcomes. In fact, other measures provide a way to extract further insight. 

The measure of coherence allows to assess the quality of a topic (i.e. good versus bad):

$C = \Sigma_{t=2}^t \Sigma_{t'=1}^{t-1}log(\frac{D(v_t,v'_t) + 1}{D(v'_t)})$

The below output gives the coherence for each ten topic. We see that the most coherent topic is topic 10 (0.557) and the least coherent topic is topic 8 (-9.174). Therefore, the words associated with topic 10 (large $\theta$) are used in the same documents.

```{r coherence LSA, echo=F, message=F, warning=F}

# Creates a feature co-occurrence matrix
data.codo <- fcm(data.dfm, 
                   context = "document", 
                   count = "boolean", 
                   tri = FALSE) # co-document frequencies

# Extracts terms object 
term.mat <- terms(data.lda, 5)

# Creates a loop computing the coherence measure 
Coh <- numeric(10) # creates a numeric vector of 10 zeros

# Loops over 1 to K = 10 topics
for (k in 1:10){
  
  D.mat <- as.matrix(data.codo[term.mat[,k], 
                               term.mat[,k]])
  
  D.vec <- data.dfm %>% 
    textstat_frequency %>% 
    filter(feature %in% term.mat[,k]) %>% 
    data.frame() %>%
    select(feature, docfreq)
  
  for (m in 2:5){
    
    for (l in 1:(m-1)){
      
      vm <- term.mat[m,k]
      
      vl <- term.mat[l,k]
      
      Coh[k] <- Coh[k] + log((D.mat[vm, vl] + 1) / filter(D.vec, feature == vl)$docfreq)
} }
}
names(Coh) <- c("topic1", "topic2", "topic3", "topic4", "topic5","topic6", "topic7", "topic8", "topic9", "topic10")

Coh 

```

To verify the above statements, we take a look at the co-document frequencies. Below, we compare the term-frequency matrices of topics 8 (least coherent) and 10 (most coherent). The matrices for topic 8 (top) and 10 (bottom) show that it is obvious that the top five terms in topic 10 are co-occurring more often in the same document than the top five terms in topic 8.

```{r feature matrix for topics 8 and 10, echo=F, message=F, warning=F}

# Feature matrix for topic 8
as.matrix(data.codo[term.mat[,8], 
                    term.mat[,8]])

# Feature matrix for topic 10
as.matrix(data.codo[term.mat[,10], 
                    term.mat[,10]])

```

##### Exclusivity 

A topic is considered exclusive if it is associated with terms that are not associated to another topic:

$Excl(topic\ k) = \Sigma_{t=1}^T(\frac{\theta(v,k)}{\Sigma_{k'=1}^K \theta(v,k')})$

The next output shows that the most exclusive topic, which has the more terms not associated with another topic, is topic 10 (0.693663) meaning that its five top terms are more specific to it. The least exclusive topic is topic 1 (0.000309). 

```{r exclusivity LSA, echo=F, message=F, warning=F}

# Creates a numeric vector of 10 zeros
excl <- numeric(10)

# Loops over the 10 topics
for (k in 1:10){
  for (i in 1:length(term.mat[,k])){
    
    term.phi <- filter(phi.long, 
                       Term == term.mat[i,k])
    
    excl[k] <- excl[k] + filter(term.phi, Topic == "topic10")$Phi / sum(term.phi$Phi)
}
  excl[k] <- excl[k] / length(term.mat[,k])
}

names(excl) <- c("topic1", "topic2", "topic3", "topic4", "topic5", "topic6", "topic7", "topic8", "topic9", "topic10")

excl 

```

## Unsupervised Learning: Embedding {.tabset}

Embedding refers to the representation of elements (documents or tokens) in a Vector Space Model (VSM). From previous sections, we saw that tokens and documents are embedded in DT and TF-IDF matrices. The difference here is that we aim at representing document and term embedding and apply it to non-BoW models and instead use terms co-occurrences (i.e. order is important).

### Word Embedding (GloVe)

This section proceed to embed words based on co-occurrences using the GloVe model. The idea is to reflect co-occurrences and not only documents (BoW). Here, each term is assigned to a vector and we look if the word-vectors similarity matches their words co-occurrence. Two terms are considered close if their co-occurrence is large. 

The GloVe model defines two vectorial representation for one word. One representation is for the contextualized word and the other one for the centralized term. Then, the representations are combined to form only one. To build the GloVe model, we use the **GlobalVectors** function of the **text2vec** package.

To start with, we compute a feature co-occurrence symmetric matrix with a window of five (i.e. context window). The matrix is very large (8,640x8,640) and displays the terms co-occurrences in the pre-specified window. 

```{r fcm 5, echo= FALSE, message=FALSE, warning=FALSE}

# Makes a co-occurrence on a (half-)window of 5
data.tk5 <- tokens(data.cp,
                   remove_punct = TRUE,
                   remove_symbols = TRUE,
                   remove_numbers = TRUE,
                   split_hyphens = TRUE) %>%
                   tokens_tolower() %>%
                   tokens_remove(stop_words$word) %>%
                   tokens_remove("chapter") %>% 
                   tokens_remove(min_nchar = 3) %>%
                   tokens_replace(pattern=hash_lemmas$token, 
                   replacement = hash_lemmas$lemma)

data.coo <- fcm(data.tk5, 
                context = "window", 
                window = 5, 
                tri=FALSE)


```

From the feature co-occurrences matrix we compute two vector representations for a given word, one being for the central term and the other one for its contextualization. To then have a unique representation for a given word, we compute the average of the two representations. Here we set a two-dimensional representation (rank=2) for readability and we plot the vectors of the 20 most frequent terms. The plot shows that dimension 2 is strongly associated to *woman* and anti-associated to *include* and dimension 1 associated to *pay*, *increase*, *design*, *country* and anti-associated to *datum*.

```{r two representations, message=FALSE, warning=FALSE, include=FALSE}

# Sets a seed for reproducibility 
set.seed(123)

# Word embedding dimension
p <- 2 

# x_max is a needed technical option
data.glove <- GlobalVectors$new(rank = p,    # dimension of the vector space model
                                x_max = 10) 

# central vectors; speech.glove$components contains the context vectors
data.weC <- data.glove$fit_transform(data.coo) 

```

```{r vectors, message=FALSE, warning=FALSE, include=FALSE}

# unique representation
data.we <- t(data.glove$components) + data.weC 

head(data.we, 10)

```

```{r unique reprensentaion, message=FALSE, warning=FALSE, include=FALSE}

# Words with the 50 largest frequencies
index2 <- textstat_frequency(dfm(data.tk5))[1:20,]$feature

index2 

```

```{r plot, echo=F, message=F, warning=F, fig.height = 12, fig.width = 12, fig.align = "center"}

# Plots
plot(data.we[index2, ], 
     type = 'n',  
     xlab = "Dim 1", 
     ylab = "Dim 2")

text(x = data.we[index2, ], 
     labels = rownames(data.we[index2, ]))
title(main = "Word Embedding: GloVe model", 
      sub = "source: Invisible Women.pdf")

```

### Document Embedding {.tabset}

Document embedding uses word embedding by translating the word representations into document representations. To build the document embedding, we compute the centroids of the documents. To do so, we use two methods, the centroids from averaging using DTM and weighted centroids using TF-IDF.

Before generating the centroids, we extract words in each document and for these words, we extract the word vectors and make a matrix. Then, we average all these vectors.

```{r words extraction, message=FALSE, warning=FALSE, include=FALSE}

# Words in Document 1
head(data.tk5[[1]]) 

```

```{r word vectors, message=FALSE, warning=FALSE, include=FALSE}

# Word vectors
head(data.we[data.tk5[[1]],])

```

```{r average, message=FALSE, warning=FALSE, include=FALSE}

# Averages all these vectors => Document 1 vector
apply(data.we[data.tk5[[1]],], 2, mean) 

#POtentiellement faire un élément pour mettre ça et le truc précédent dans un kable 

```

```{r loop over documents, message=FALSE, warning=FALSE, include=FALSE}

# Number of documents
nd <- length(data.tk5) 

# Document embedding matrix (1 document per row)
data.de <- matrix(nr = nd, 
                  nc = p) 

# Loop over documents
for (i in 1:nd){
  
  # drop=FALSE is needed in case there is only one token
  words_in_i <- data.we[data.tk5[[i]], , drop = FALSE] 
  
  data.de[i,] <- apply(words_in_i, 2, mean)
  
}

row.names(data.de) <- names(data.tk5)

head(data.de) ## document vectors

```

#### Centroids (DTM)

The centroid method averages the word representations over the number of documents:

$vec(d) = \frac{1}{|d|} \Sigma vec(w)$

[INTERPRET]

```{r plot-texts, echo=F, message=F, warning=F, fig.height = 12, fig.width = 12, fig.align = "center"}

# Plots
plot(data.de, type='n',  xlab="Dim 1", ylab="Dim 2", main="Document Embedding: Centroids (averaging using DTM)")
text(x=data.de, labels=rownames(data.de))
title(sub = "source: Invisible Women.pdf")

```

#### Weighted Centroids (TF-IDF) 

The weighted centroid method uses TF-IDF: 

$vec(d) = \frac{\Sigma_{w \in d}tfidf(w,d)vec}{\Sigma_{w \in d}tfidf(w,d)}$

[ADD INTERPRETATION]

```{r centroid plot tfidf, echo=F, message=F, warning=F, fig.height = 12, fig.width = 12, fig.align = "center"}

data_tfidf <- dfm_tfidf(dfm(data.tk5))

data.detfidf <- matrix(nr=nd, nc=p) # document embedding matrix (1 document per row)

for (i in 1:nd){
  
  words_in_i <- data.we[data.tk5[[i]],,drop=FALSE]
  
  weights.tfidf <- as.numeric(data_tfidf[i, data.tk5[[i]]])
  
  weights.tfidf <- weights.tfidf / sum(weights.tfidf)
  
  data.detfidf[i,] <- apply(weights.tfidf * words_in_i,2,sum)
  
}

plot(data.detfidf, type='n',  xlab="Dim 1", ylab="Dim 2", main="Centroids - TFIDF")

text(x=data.detfidf[,1], y=data.detfidf[,2], labels=rownames(data_tfidf))

```

### Relaxed Word Mover’s Distance (RWMD) {.tabset}

The document embedding can be used for clustering (using K-Means) or for computing the similarity/dissimilarity (Jaccard, Cosine, Euclidean) matrices before clustering (hierarchical). An alternative approach is to use the Relaxed Word Mover’s Distance to compute document similarity using word embedding. RWMD can use DTM or TF-IDF. 

#### Distances

Distances (i.e. similarities) between documents in the RWMD approach estimate how hard it is to transform words from one document into words from another document and vice versa. 

To compute the RWMD, we use the **RelaxedWordMoversDistance** function from the **text2vec** package using DTM (word and document embedding). Note that the function provides similarities and distances which allows us to plot a dendogram displaying the clustering of chapters. 

The following dendogram shows that chapter 10 *The Drugs Don’t Work* and chapter 11 *Yentl Syndrome* from part IV *Going to the Doctor* are the most similar since the height at which they are joined (~0.14) is the smallest. Considering the RWMD, this means that the cost from converting the words of chapter 10 into the words of chapter 11 is the smallest because the distance of their word embeddings distances is the smallest. 
```{r RWMD distances, echo=F, message=F, warning=F}
set.seed(432)
# Preparation
data.glove <- GlobalVectors$new(rank = 30, x_max = 10)

data.weC <- data.glove$fit_transform(data.coo)

data.we <- t(data.glove$components) + data.weC

data.dtm <- dfm(data.tk5)

# Build the model
data.rwmd.model <- RelaxedWordMoversDistance$new(data.dtm, data.we)
data.rwms <- data.rwmd.model$sim2(data.dtm)
data.rwmd <- data.rwmd.model$dist2(data.dtm)

# Plot the dendogram
data.hc <- hclust(as.dist(data.rwmd))
plot(data.hc, cex = 0.8)

```

#### Document Embedding using RWMD

Since RWMD generates dissimilarities between documents, we can easily obtain a clustering of the RWMD document embedding. The positions of vectors here mirror the Relaxed Word Mover’s Distances. 

[ADD INTERPRETAION]

```{r clustering from RWMD, echo=F, message=F, warning=F}

# Preparation
data.glove <- GlobalVectors$new(rank = 2, x_max = 10)
data.deC <- data.glove$fit_transform(data.rwms)
data.derwms <- t(data.glove$components) + data.deC

# Plot clustering
plot(data.derwms, 
     type = 'n',  
     xlab = "Dim 1", 
     ylab = "Dim 2", 
     main = "Document embedding (RWMD)")

text(x = data.derwms, labels = rownames(data.derwms))

```

## Supervised Learning {.tabset}

The goal of the supervised analysis here is to re-classify the 256 pages of the book in the six parts. To do so, we proceed to a machine learning approach consisting of splitting the corpus into a training (40%) and a test (60%) sets. Then, we train the classifiers on the training set and finally, we select the best classifiers from the results on the test set.

To be able to proceed to the classification method, the corpus must be cleaned in order for the features (i.e. terms) to be usable. Note that since terms must be usable (i.e. meaningful), we do not apply stemming on features. The appropriate cleaning process consists in sequential steps from the tokenization to removing useless words (i.e. stop words) and lemmatization.

```{r cleaning for supervised analysis, echo=F, message=F, warning=F}

# Data cleaning for the supervised learning analysis 

# count the total number of pages additioning all the pages
totalpages <- 0

for ( i in 1 :nrow(data)){
  # split the paragraph by pages
  paragraphdata <- strsplit(data$text[[i]],"\n\n")
  # count the number of pages for the chapter
  NbPages <- length(paragraphdata[[1]])
  # add the number of pages of the chapter to the total number of pages 
  totalpages <- totalpages + NbPages
  
}

# remove change the nb of pages because the chapter number and the name of the chapter are counted as a two pages
totalpages_nochap <- totalpages - (16*2)

# create emptydata frame with all the pages
supervised_data <- data.frame(matrix(NA, ncol = 4, nrow = totalpages_nochap))
colnames(supervised_data)<-c("document","part","page_nb","pagetext")

rowindex <- 1

for (i in 1:nrow(data)){
  paragraphdata <- strsplit(data$text[[i]],"\n\n") # split the text to pages
  paragraphdata <- paragraphdata[[1]][-c(1,2)] # remove first two elements ( chapter and chapter name )
  
  NbPages <- length(paragraphdata)# compute the number of pages
  
  for (j in 1:NbPages){
    supervised_data$document[[rowindex]] <- data$document[[i]]
    supervised_data$part[[rowindex]] <- data$part[[i]]
    supervised_data$page_nb[[rowindex]] <- paste(i,j,sep=".")
    supervised_data$pagetext[[rowindex]] <- paragraphdata[j]
    rowindex <- rowindex+1
  }
}

# remove small text ( like name part)
supervised_data <- supervised_data %>% filter(nchar(pagetext)>40)

# Cleaning 

# transform data frame into copus object
supervised_cp <- corpus(supervised_data,text_field = "pagetext")

# data cleaning ans lemmatization
supervised_tk <- supervised_cp %>% 
                   tokens(remove_numbers = TRUE,
                          remove_punct = TRUE,
                          remove_symbols = TRUE,
                          remove_separators = TRUE) %>%
                   tokens_split(separator = "'") %>%
                   tokens_split(separator = "-") %>%
                   tokens_remove(stop_words$word) %>% 
                   tokens_tolower() %>%  
                   tokens_replace(pattern = hash_lemmas$token, 
                                  replacement = hash_lemmas$lemma) %>%
                   tokens_remove(c("chapter")) 

```

### Classification with Random Forest {.tabset}

Random Forest is a supervised leanring algorithm used in regression and classification problem. It consists of building many decisions trees that will be averaged to make a final prediction. Here we will do classification for the 6 parts of the book.
Part I: *Daily Life*
Part II: *The Workplace*
Part III: *Design*
Part IV: *Going to the Doctor*
Part V: *Public Life*
Part VI: *When it Goes Wrong*

#### Features: DTM and LSA

First, to obtain a set of structured features, we generate a DTM of dimensions 256 by 5,738 with term frequencies. Since the DTM is relatively large, we reduce its dimension using the Latent Semantic Analysis (LSA) by targeting 30 dimensions. Note that we already remove the stop words in the cleaning and that we previously make sure that they are unimportant to our analysis.

In this case, LSA has the advantage of decreasing the number of features to be used while keeping relevant information for the analysis. 

```{r DTM supervised, echo=F, message=F, warning=F}

# create dfm object
supervised_dfm <- dfm(supervised_tk)

#dim(supervied_dfm)

```

```{r dimension reduction with LSA, echo=F, message=F, warning=F}

# Dimension reduction with LSA 
data.lsa <- textmodel_lsa(supervised_dfm, nd = 30) # reduce feature to 30 dimensions

```

From the results of LSA, we proceed to fir the classifiers on the training set and predict the results on the test set. The below results show first the overall quality of the predictions (on the test set) of the random forest classifier on the confusion matrix. 

The diagonal indicates the correct predictions of pages in their actual parts of the book and around it we see the false predictions. For example, for part IV *Going to the Doctor*, 21 pages are correctly classified and 3 pages are incorrectly predicted.

The overall statistics demonstrates an overall accuracy (i.e. proportion of correct predictions) of 83.1% and balanced accuracy (i.e. average between sensitivity and specificity) for the classes over 76.7% as well, indicating that our model is good. The interesting fact here is that even after reducing the dimension to 30 of the DTM, we still obtain a high accuracy. 

```{r random forest, echo=F, message=F, warning=F}
set.seed(235) # for reproducibility of the results
# Set the response to predict: chapters (as factor)
y <- as.factor(supervised_data$part)

# Transform into data frame
df <- data.frame(Class = y, 
                 X = data.lsa$docs)  



# Split training set 40% and test set 60%
index.tr <- sample(size = round(0.4*length(y)), 
                   x = c(1:length(y)), 
                   replace = FALSE)

# Assign features to training and test sets
df.tr <- df[index.tr,] 
df.te <- df[-index.tr,]

# Fit on the training set using random forest
data.fit <- ranger(Class ~ ., data = df.tr)

# Make predictions on the test set
pred.te <- predict(data.fit, df.te)

# Prediction results in confusion matrix
confusionMatrix(data = pred.te$predictions, 
                reference = df.te$Class) 

```

#### Improving the features

Now, we can consider to tune the elements that compose the features. For this propose we reconsider the previous LSA dimensions and select this number based on the best accuracy.
[revoir]

```{r feature improvement,echo=F, message=F, warning=F}

set.seed(2345) # for reproducibility of the results

## May take a long time
nd.vec <- c(2, 5, 25, 30, 50, 100, 1000)

acc.vec <- numeric(length(nd.vec))

for (j in 1:length(nd.vec)){
  
  data.lsa <- textmodel_lsa(supervised_dfm, nd = nd.vec[j])
  df <- data.frame(Class = y, X = data.lsa$docs) 
  df.tr <- df[index.tr,]
  df.te <- df[-index.tr,]
  
  data.fit <- ranger(Class ~ .,
                     data = df.tr)
  pred.te <- predict(data.fit, df.te)
  
  acc.vec[j] <- confusionMatrix(data = pred.te$predictions, reference = df.te$Class)$overall[1]
}

## Growing trees.. Progress: 82%. Estimated remaining time: 6 seconds.
acc.vec

## [1] 0.7942547 0.8625776 0.8819876 0.8827640 0.8804348 0.8812112 0.8796584
plot(acc.vec ~ nd.vec, type = 'b')

```

The below plot shows us that the greatest accuracy is when we set 25 dimensions for the LSA.

Another method to verify whether the accuracy can be improved is to fit the model using the TF-IDF instead of the DTM. Here we reduce the dimension of the TF-IDF using the Latent Semantic Analysis (LSA) by now targeting 25 dimensions.



```{r random forest using TF-IDF, echo=F, message=F, warning=F}
set.seed(255) # for reproducibility of the results
# Random forest using TF-IDF
data.tfidf <- dfm_tfidf(supervised_dfm)
data.lsa <- textmodel_lsa(data.tfidf, nd = 25)
df <- data.frame(Class = y, X = data.lsa$docs)
df.tr <- df[index.tr,]
df.te <- df[-index.tr,]
data.fit <- ranger(Class ~ .,
                     data = df.tr)
pred.te <- predict(data.fit, df.te)
confusionMatrix(data=pred.te$predictions, reference = df.te$Class)
```

As expected using the TF-IDF and an optimized number of dimension, we increase the accuracy from 81.2% to 83.1%. Additionally, the balance accuracy also increase from 76.7% to 79.1% so the tuning slightly improved the classification.

#### Adding Variable

Here, we try to improve the classification of the model by adding a feature that are the lengths of the tokens, however the accuracy didn't improve, therefore as we have two model with same accuracy, the previous one with less variable is preferred.  

```{r classification improvement, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
set.seed(245) # for reproducibility of the results
# Transform dtm into tfidf
data.tfidf <- dfm_tfidf(supervised_dfm)

# LSA dimension reduction with 30 dimensions
data.lsa <- textmodel_lsa(data.tfidf, nd = 25)

df <- data.frame(Class = y, X = data.lsa$docs)

# Adding features
df <- cbind(df, 
            length = log(sapply(supervised_tk, length)))
            
df.tr <- df[index.tr,]
df.te <- df[-index.tr,]

# Random forest when adding more features
data.fit <- ranger(Class ~ .,
                   data = df.tr, 
                   importance = "impurity")

# Make predictions
pred.te <- predict(data.fit, df.te)

# Confusion matrix
confusionMatrix(data = pred.te$predictions, 
                reference = df.te$Class)

```

### Classification with SVM 

The SVM method is a machine learning method, that is very different compared random forest. SVM tries to classify classes by separating data points by the largest vector as possible, in our class the SVM is used to predict multiple classes and not only two.   

To assess the prediction quality of the random forest model, we proceed to the classification of the pages into parts of the book using the Support Vector Machine learner. To do so, we use the **svm** function of the **e1071** package with radial kernel and default parameter. 

The following results show that the SVM model has a much worse overall accuracy and so does not improve our classification of the pages into the six parts. We are better off with the random forest model. 

```{r svm classifier, echo=F, message=F, warning=F}
#set.seed(245) # for reproducibility of the results
data.tfidf <- dfm_tfidf(supervised_dfm)
data.lsa <- textmodel_lsa(data.tfidf, nd = 25)

df <- data.frame(Class = y, X = data.lsa$docs)

df.tr <- df[index.tr,]
df.te <- df[-index.tr,]

# SVM model
svm.model <- svm(Class ~ ., data = df.tr, 
                 kernel = "radial")

# SVM predictions
svm.pred  <- predict(svm.model, df.te)

# Results
confusionMatrix(data = svm.pred, 
                reference = df.te$Class)

```

